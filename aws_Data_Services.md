# AWS
	- We're gonna start by talking about Amazon Web Services in general. They have been in the commercial market for the longest of any commercial vendor and their products are the most mature because of that. Their products are also priced at a very good value. They often lead the market in terms of the best pricing, or the cheapest pricing, for their particular services. And in fact, price cuts are common in this industry and they're usually lead by Amazon and then followed by other cloud providers. As a working big data and cloud architect, I most often use Amazon Web Services in my production workloads.

	- Amazon has a frequent product release cycle and update cycle. It's not uncommon to have more than 10 updates or releases in the period of one week's time. A tip that I have right from the beginning of this course is to subscribe to the Amazon Web Services' main blog so that you can get notifications when their products release or update. Another consideration as we're working with Amazon Web Services is to understand where their physical data center locations are.

	- They're shown here in two aspects. The first is their primary centers and then the second is the zones within their centers. So I'm recording this course from North america. If you are working with some of the aspects of the course, be sure to select the appropriate region and zone from where you are watching the course. As we drill in to Amazon Services, let's think about the categories those services are grouped into. As a cloud architect, I categorize cloud based services as compute, which is now virtual machines, docker containers and in the case of Amazon, something called a Lambda which is a service that has some applicability to the architecture's and services we're going to talk about in this course.

	- The core of this course, though is not compute, it's storage. We're gonna be talking about Amazon Web Services around file storage and databases. And there are so many choices and so much information, this is the core of this entire course. There are, of course, other services that are available through the Amazon set of services in addition to compute and storage and they're very often used in production workloads, as well. So for most of this course, we'll be working with the Amazon services as they're available through the Amazon Web Services console, which is a website.

	- So what I've done here is I've set up a new account for demonstration and logged in. The way it works with Amazon is you have access to try out some, but not all, of the services that we're gonna show for free. So I'll try to point out as we're going through learning how the services work and how you might test them, which are available for free tier and which aren't. But I'm going to give you a tip when you are setting this up if you click on your name and then you click on Billing & Cost Management what you can do is you can set up a budget.

	- Now, what I've done is I've set up a budget associated to any resources that have a tag that is called Lynda associated to this. So if you click Create Budget, you put in the name, and then you include costs related to, and I created a Tag. And I'll show you how to do that in just a minute. And then I set a time period and a monthly amount and then I set a billing alarm. This is a real world tip that when you're trying out services you'll really want to do because in case you forget to turn them off, you don't want to be billed accidentally. So let me show you how to create a tag.

	- So you create a group and I'll call this one demo and then I'll put a key called Name and I'll call it demo and then I'll just save it. And so now if I click, I have two resource groups. I had created this Lynda one in advance. If I want to edit it then I can go into my Tag Editor and then I can look for a tag key and I can look for a value and edit it. So it's a little bit of setup, but this is a tip from the real world because I don't want you to get charges that you didn't intend when you're trying out some of these services.

	


	- Now you might be wondering why I decided to create a course just around AWS Data Services. There's so much more to the AWS ecosystem. Well, there are a couple of reasons. I find the data service options on a particular vendor cloud often drive not only the choice of those services, but actually the choice of the vendor. This is a very, very active development area for the various vendors: Amazon, Google, Azure, and other vendors as well, and I find that as I'm building solutions with my customers, the data services that are available are really critically important in the selection of the cloud itself.

	- So I really wanted to take the knowledge that I'd gained from real world implementations and share that with you. Another aspect of this is that data today is partitioned not within a set of databases on a server, rather between data services on a cloud. And this might not make any sense to you at this point in the course, but we're going to talk about this and see examples of architectures where data is split amongst, for example, NoSQL databases and relational databases and other types of data storage systems on the Amazon cloud because it makes the most sense given the product line available.

	- So this is definitely a new direction that I'm seeing more and more in my work as an architect, and I want to show you how I've implemented these types of solutions on the Amazon cloud using their data service offerings. The other thing about the Amazon data service offerings is the strength of the partner ecosystem. It's critically important that what is built is usable by the team you have in-house. So again, I'm going to draw from my real world experience and highlight third-party partners, partners who provide service around key aspects of the use of data, such as importing data, cleaning data, processing data, visualizing data, being able to process complex queries, including machine learning.

	- The partner ecosystem that exists around the AWS Services in general, but the Data Services in particular, is unmatched, and it's important that you consider that when you're looking at building solutions on the Amazon cloud. Now, just to make this a bit more real, I'm back on the Amazon Web Services console. And I just want to call out the sections we're going to focus on in this course. We're going to focus on the products that are offered in Storage and Content Delivery, that would be files in my vernacular. Databases, that would be relational and non-relational databases.

	- And if I scroll down here, Analytics, which would be huge data solutions, streaming, processing, and pipelines. Now, peripherally we'll talk about some of the other services that either are new and have an impact on data architectures, services like Container Service and Compute and Lambdas. We'll also talk about some of the other services that I've used with some of my customers that, again, are either new or enhanced. Things like API Gateway and some of the monitoring and administration tools over here as well.

	- But the core focus for this course will be storage, database, and analytics.
	## Exploring File and HDFS Storage
		- So the first aspect of data that we're going to talk about is file storage. Now you might think, gosh, this is simple and straightforward, but there's more and more complexity to understanding all the switches or parameters or options around file storage. The first consideration is warm or cold. That means files that you access day in, day out, or files that are archived. Other considerations are should you replace your local area network file system, or your LAN? Do you have a need for Hadoop type of files or HDFS? Also, do you have a need for SSDs? So there's quite a lot around file storage.

		- We'll have an entire section looking at the different offerings. Also you need to consider how are you going to transfer your files either one time or on an ongoing basis from your network up into the AWS cloud? So quite a lot of things to look at here. And again, in the real world, I found lots of ways to optimize the process and save money, and I'm going to share that with you when we look at these services from the AWS cloud.

	## Exploring AWS data storage
		- Now, when we talk about big data solutions, it's trendy to talk about what are called the V's. Volume, velocity, variety, voracity, so on and so forth. I find that to be a bit overused, so the way I like to talk about data selection is a little bit more simple. It's almost like buying a soda. So, when you think of the data that you're going to work with, are you going to go with a small or medium set of data? Or a large or huge? Then what is the relationship between this data and also, what is the complexity of both the data and the queries? These are the kinds of questions that I ask to start to understand what kinds and which of the AWS data services are going to be the best fit for the solutions that I build.

		- So, I'll be using these terms throughout the course. Is the data that we're looking at small or medium? I'll define what those sizes are as we go along. Large or huge? What is the complexity of the data itself and the complexity of the queries? More complex data and more complex queries require different kind of AWS data solutions. So, if I were to overlay my thinking around small, medium, large, and huge, to the sets of data service choices available on the AWS Cloud, would there be a match? There is.

		- So, you can see that I have listed down here on the left, relational databases, NoSQL, Hadoop ecosystem, and something that's called NewSQL. Some of these terms might be unfamiliar to you, but as we go through this course, I'll not only define the terms, but I'll associate this to the services that are available on the Amazon Cloud. I've laid out on the right my vernacular of size. So, you can see that in the world of AWS data services, relational databases are used for small data workloads. NoSQL for medium, the Hadoop ecosystem for large or huge, and NewSQL for complex.

		- Again, I'll come back to these terms. I think that they're simpler way to understand the complex set of services than the more popular V's if you will. So, I'll use them throughout the course. Another aspect of data services that I want to call out early in the course, that I'll be emphasizing through the lens of the various services is what is it that you're paying for when you're renting a data service on the Amazon Cloud? What are the different aspects or the things you can buy? So, you're always going to be paying for storage capacity and the redundancy.

		- In addition to that, you'll be paying for the access for these data services. The write speed, the read speed, the query complexity. You may also be paying for the... Sending the data in or pulling the data out in addition to the query complexity and the other components here. So, we'll be looking at the pricing calculators, so you can get a sense of what the true price comparison for the different data services is. Cuz that's a really important aspect of deciding which data services are right for your particular solution. Then there are other aspects to some of these data services.

		- I call them premium services. Aspects like encrypting the data, advanced logging, so on and so forth. So, very often when my customers think about managed data services, they just think about read write and they don't think about some of these other charges. I want to be comprehensive in our coverage of what it is you're actually buying when you're working with manage data services in the Amazon Cloud. So, we'll be covering this in detail as we review each of the services available on the Amazon Cloud.

	## Understanding Why Cloud Tools Matter
		- Another thing that I've found when working with cloud services and when they're new to my customers is that the tools matter. Understanding these new service capabilities is much eased by tools that are simple to use and familiar if possible. So for the Amazon tools, we're going to focus mostly on the console here. You certainly can use the command line interface, the CLI tool, and there are very rich SDKs. Personally, I use the SDKs that work with C Sharp, that go with Visual Studio, or with Java and Eclipse.

		- But the focus of this course is architectural more than programmatic, and so I will give references to the scripts in the SDK, but we'll be focusing on the console. Now there will be certain situations where there are aspects that are not available there, and then we'll go to some of the other tools. Now in addition to the Amazon tools, there are vendor tools, and I'm actually going to be featuring some of my favorite vendor tools because the Amazon ecosystem in general and the data ecosystem in particular has a really rich set of partner tools that I use constantly.

		- And I just listed a tool there that is a open source tool, actually. It's the MySQL Workbench as an example. Because we are working with so many different data stores, there are a number of data tools out there. Some of them are free, some of them are freemium, some of them are premium. But that's just one that I happen to use constantly, so we'll be working with that along with some of my other favorite tools in addition to the Amazon console and some of the Amazon tools throughout this course.

# Storage

	## Core File Storage Choices
		- So as we get started taking a look at file storage, let's look at the categories that we're going to be working with. In core file storage, we have some choices. The first one is what's called warm storage, and this is for files that you read and write to frequently. Amazon's offering here is a service called S3. For cold storage, this is the type of files that are archived or that you put information into the archive and you very, very rarely will read it out. In Amazon, this service is called Glacier.

		- Now in addition to these core storage offerings, AWS also has offerings around their virtual machines or EC2 instances, and there's two types that we'll be looking at in this section. The first type is single storage, which is Elastic Block Storage or EBS, and multiple storage, or EFS, Elastic File Storage, which is a relatively new offering at the time of this recording. And these types of storage have to be associated to EC2 virtual machines, so I'll be showing all of these different types of storage in this section.

	## S3
		- In this movie, we are going to take a look at AWS S3 or simple storage service. This is their core file system. So its file storage on their cloud using buckets, which are kind of analogous to drives locally and folders, which are the same thing, and then, of course, files. Its commodity priced and we'll look at pricing near the end of the section when we compare file storage option pricing, and it's used by almost all of the other data services. So this is probably the most used data product in the Amazon cloud services.

		- And nearly every customer that I work with, whether or not they deploy the majority of their solution on the Amazon cloud uses S3 for redundant or duplicate storage across vendors. So because it's so cheap and easy to use, it's just the core of really all of the data services on the Amazon cloud, so we are going to start with it. So here we are in the Amazon Web Services management console and under Storage and Content Delivery, we are going to click on S3 for Scalable Storage in the Cloud.

		- And inside of the console because we don't have any objects created, it gives us a little information about the object and it tells us that we can read, write and delete objects ranging from one byte to five terabytes each. This is also called a BLOB or a file. Blob for binary large objects. So the idea is that you can store files that are very large, such as sound files, movies, so on and so forth. So as it says, get started by creating a bucket and uploading a test object. Now folders are optional here and we'll work with buckets and files, and then we'll add folders as well.

		- Notice also, create, add and then manage, there is a lifecycle management capability built-in that allows you to move as it saves some of your files to archival storage, which is an even lower cost. But let's click on the blue button to create a bucket, and let's call it, First Bucket Sunday. Notice that I have different regions I can create this bucket in, and I'm recording in California, so I'll select Northern California. I have the option to set up logging, which would be more detailed capture of the activities.

		- We don't need that at this point, so I'm just going to click Create. So one of the aspects of working with S3 to get the most out of it is to understand the bucket properties. So you can see this is the metadata around the bucket, and if I expand the permissions you can see that being the owner I have these permissions associated. And if I wanted to, what I could do is add more permissions here, and I could say, to whom this was granted. So let's say it was authenticated users and they could in this case list the information inside of the bucket.

		- They could see what's there. And then I can go ahead and say Save. Now as with files that are stored locally, you can have more granular permissions assigned to particular objects inside of a bucket. We are just starting with the top level permissions. If you want to add a bucket level policy, what you get is a policy generator. Now this is a core concept in working with data objects in Amazon where you want to determine what security policies you need to set up and then you want to apply them via policies on the highest level of granularity.

		- So lock things down, if you will. So you can see that we've got sample bucket policies here, and it's going to show you just the examples here of the various bucket policies. So, for example, granting read-only permission to anonymous user, that's a common bucket policy. And they show you the text of that that you could just copy and paste in. So I'm actually going to go back over here, and I'm going to also click on the AWS policy generator because that's another tool that I've used in setting up security policies. So what this allows you to do is fill in this form and then the policy that you are needing will be generated, if one of the default ones doesn't fit for you.

		- So notice S3 bucket policy, and then inside of here I can say Allow or Deny, and then I can say, who it is, and I'm just going to put something a little bit fakey just for the purposes of demonstration. And then the AWS service, and then here is where I can put the actions. So I could put any of the actions that I would want. So let's say create bucket, delete bucket, just again just to show you an example. Then I want to associate this to an Amazon resource name, an ARN. So this is the syntax, ARN, Amazon S3, and then the bucket name and then the key name, and then you could add conditions as well.

		- Conditions could be associated to properties of the bucket. And the reason I'm showing you all this is that it's very, very common as you become more sophisticated in your storage of files up in the cloud to have a more granular set of permission. So this is a common area that I work with in my enterprise customers, and this is the path that I'll take where we'll start with the bucket policy examples and see if those work, and just kind of get everybody familiar with that. And then we'll use the policy generator, which I won't take the time to complete, but I think you can see how it works.

		- You would fill in the additional ARN information and then set the conditions and it would generate something that looks kind of like that. And then you would paste that back in there, and that would be a very specialty bucket policy or set of permissions. It's an intermediate technique when you are storing more and more files that I wanted to include in our discussion of S3 because we've got a lot of questions around how do you set the permissions. So I'm going to click Delete and just go with the permissions that I have and say Save. And then the next section here is Static Website Hosting.

		- Again, this surprises a lot of customers that I show that you can actually host a static website just inside of S3. So all you do is you enable website hosting and then you specify what is the index document. Of course, we don't have one in here yet, and that would be the start document. And then when you click on this URL, you'd have an automatic website. It's a capability that is incredibly simple and useful for static hosting. And I just wanted to make sure that people who are watching were aware of it. In addition to this, we have logging capabilities, so if we enable logging, we can say which bucket, so this bucket.

		- And then the destination for logs, which would be written out to a different bucket. This will log either all activities by default or anything you've to find via policies. So you might want to say, I want to have a log of whenever there is a delete of files in this bucket, I want to have a log of whenever there is a change to folder structure, so on and so forth, so all these activities can be easily logged. In addition we have events. So what events do is they allow notifications to be sent through another set of AWS services, so that you can then receive notifications about activities that are performed on the bucket.

		- As with logging, if something, for example, a file is deleted; if you enable events, then somebody could, for example, get an email notification on their phone or through an application when a file change was performed. Versioning allows you to store the different versions of objects. So notice, preserve, retrieve, restore every version, okay, additional level of protection. And you can use lifecycle rules, which comes up next to manage all versions as well as to put older versions into the archive.

		- And notice, once enabled versioning cannot be disabled, only suspended, so it's disabled by default. And it's a toggle, so you just click it to turn it on. Lifecycle allows you to move some of the objects from this warm storage into cold storage. So we'll come back to that when we take a look at cold storage. Cross-Region Replication replicates every future upload of every object to another bucket. This is often used in conjunction with versioning.

		- You are required to enable versioning on this bucket and the target bucket. So again, the idea here is redundancy, and as I work with enterprise customers and they move more and more of their critical information up into the cloud, these kind of processes are enabled more frequently as they move into production. Tags allow you to tag their resources, and you might remember from the introduction. So if I say, add more tags and I say name and I say Lynda, I've created a group, and I say Save.

		- Once I go look at that group, which I'll do in a minute, then you'll see that this bucket will show up. Really this is great in general, but I often use it with test projects because then you can easily see which resources are associated and remove them all when you're done with the project. It's just a good practice in general to tag your objects, so that you can find them and associate them to groups. And then the last property at the bucket level is Requester Pays. This is when a requester will pay for the data transfer and anonymous access is then disabled.

		- All right, so this is our bucket, again lots of things you can do in terms of the bucket properties, click Save here, say okay. And I encourage you to utilize the ones that are going to make sense for you. Now if I click Actions, I can create, delete, empty, refresh or go to properties. Now if I go inside of this bucket, I can upload a file directly or I can create a folder. And you might wonder, do I need to create folders? And really this is dependent on your particular set of circumstances. I find that most often I will want to create folders because there is different sets of functionality, and in the address it just makes sense to differentiate.

		- So I'm going to go ahead and create a folder called One. And then if I look at the properties of the folder, I have a similar set of properties. Let me click on this, but it's a subset. So the properties that are available at the folder level are the type of storage, so I either going to have Standard or Reduced Redundancy. And this affects pricing of S3 storage. What this means is, Standard is replicated across different locations, physical locations; Reduced Redundancy is within the particular location and it's cheaper; Server Side Encryption, none or AES-256.

		- Again, these impact the cost. Now I could create a folder in a folder if I wanted to, but the whole point of S3 is uploading files. So let's work with uploading some files. Let me go inside the folder and click Upload and then click Add Files. I'm going to click test one and say Open and click Set Details, and here I could use Reduced Redundancy or Server Side Encryption, Set Permissions. I could have individual permissions here. So here I'm going to start the upload.

		- And my upload is done, and now the actions that I can do on this particular file are, I have a direct link to it, here are my details, here are my permissions and here is associated metadata. And if I click on Actions, I can Open, Download, Create a Folder, Make Public, and that will make this link publicly accessible; or Cut, Copy or Delete. Now I'll show you how resource tagging works in the Amazon console.

		- You might remember that this bucket we tagged with the name Lynda, so let's go back to the main page. And if we click on Resource Groups Lynda, you'll see that because we tagged it with the name Lynda and that we matched this resource group, the group name is Lynda and the tag name is Lynda. Now I can go ahead and see the resources associated. So like I said, this is just a usability tip that I use as an architect when I'm creating samples and demos, I just create a set of tags, and that way I can see all the resources that are associated and easily delete them or make sure that they are turned off.

		- So the way that this works is, you just create a tag, and if you wanted to edit it, you associate it then to a group over here. And remember that everything is case-sensitive. They tell you that but it's easy to forget, and you won't have matches if you don't follow the case-sensitive. You can filter by regions and resource types, and it's just a really simple thing that will help you when you are starting working with your AWS data services.

	## Glacier
		- The next AWS file service we're going to look at is used for archival file storage and that's called Glacier. It allows for file archiving using vaults and archives. It is commodity priced, at the time of this recording, it's a penny a gigabyte a month. You didn't hear me incorrectly, it's that cheap. And it's called cold file storage because as opposed to S3 that we just looked at, the idea is that you'll place your files in here as a backup or as a holding area and you won't be accessing them unless there's some kind of unusual situation.

		- So the access times are substantially different than S3, this is really designed for archival storage. Let's take a look. On the main Amazon console, under Storage & Content Delivery, I'll click on Glacier. Then I'll get some information about getting started, so creating a vault, setting data retrieval and setting notifications if I want to be notified of activity in my vault. So I'm going to click Get started. And I'm going to create a vault in us-west-2.

		- Notice, I can have up to 40 terabytes in my archive and I can store any kind of information, I can also use a compressed or ZIP files, TAR or ZIP, and upload them as archives. I can setup event notifications that use the Amazon Simple Notification Cloud Service and then it tells me if I'm going to use that, I need a topic that people can subscribe to or applications can subscribe to. For this, I'm just going to not enable notifications and I'm going to click Submit.

		- So here's my first vault. Inside of the Web Console, really this is a view-only kind of interaction with the vault because really I can't do any uploads, this is not enabled like S3 where I can just upload files quickly through the Web Console. I click Settings. This is talking about data retrieval which is an important aspect of pricing Glacier. Free Tier Only means only retrieve data within the free tier. Data retrieval requests that exceed this won't even be accepted.

		- Normally in production, you're going to set either Max Retrieval Rate or No Retrieval Limit. Max Retrieval Rate, data retrieval requests that would exceed the specified maximum will not be accepted. 1 gig an hour, retrieval costs 7.20 a month or less. The point of this is that, while it's nearly free to put information into Glacier, you need to consider, and this is especially around disaster recovery scenarios where it's often used, how much it would cost to take it out and how long it would cost to take it out because it's not instantaneous, this is truly a vault.

		- So I'm going to click on Cancel. Now, to show the interaction between services, I'm going to go back to S3 and go back to our bucket. Do you remember how to see the properties? We're going to click on Properties and we're going to go into Lifecycle. This is one of many ways that you can setup interaction between the files in warm storage and cold storage. I'm going to add a rule and I'm going to apply this to the whole bucket or a prefix, configure rule, and then I'm going to say what it is.

		- I'm going to say Archive, Permanent Delete or Archive and then Delete. So Archive Only. Archive to the Glacier Storage Class, and I'll put this 30 days after the object's creation date, and then Review. I'll call it thirtydayarchiverule then I will create and activate the rule. It's pretty darn simple to integrate archival storage with regular storage even through the console.

		- Now, what you might wonder is if you don't use this process, how do you get files into archival storage? In the next movie, I'm going to highlight a partner solution that I use with many of the costumers.

	## CloudBerry to Backup Data
		- As I mentioned in the previous movie, in addition to working with the Amazon console, I use partner tools in working with core file storage and archival services and one of my favorites is something called CloudBerry Lab. So they have free versions of similar tools and commercial versions, but they're relatively inexpensive and provide great value because they have a very easy, gooey interface. So they have a set of storage management tools and the ones that you use are going to depend on which operating system you're working with; Mac, Linux, or Windows. And, like I said, very easy to use, looks like the native file system, so gooey based.

		- And includes capabilities to support sophisticated synchronization, backup, and so on. So let's take a look. So here we are on the main site, cloudberrylab.com and I'm working with a Mac here. So you can see I'll just give you a look at the products. So we've got backup, and this is integration with the core services based in the OS and then we have the CloudBerry Explorer and notice they work with, basically, all the file services on all the major clouds. Drive, and backup service.

		- So, this is really what they specialize in. So, here we are in the CloudBerry online backup gooey window and we're going to go ahead and click on Backup Files. We need to specify our bucket and our access key and our secret key. Our bucket is Firstbucketsunday. Now to find our access key and our secret key we need to go to the credentials section of the console. So we'll go there next.

		- We'll click on "Continue to Security Credentials". Here we're using just a single account as an aside for demo purposes. This is, of course, not best practice for production and that's the warning that you're getting here where Amazon's talking about IAM, or Identity and Access Management individual users. But just for this course we're going to use a single user which is really only good for demonstration. You want to have users with lesser than full admin permission on your entire Amazon account for production, obviously. So we're going to click on Continue to Security Credentials and click the plus by Access Keys and click Create New Access Key.

		- And click Show Access Key. Copy and paste the access key and the secret key into the add account dialogue box and click OK. And click OK again. Now, we're going to put the backup plan name and we'll call it firsttest and Cloud Storage, this is the name for the connection; demo. And click Next. And then we're going to specify files and folders that we want to backup.

		- This is from our system going up to the cloud. So, I'm gonna backup my files and click Next. And I have a couple switches; Backup empty folders or Exclude system folders. Notification on arrow only, on or off, click Next. Then I can enable compression and, or encryption with a password. I can use Reduced Redundancy for cheaper prices or Server Side Encryption and click Next, I'll go with the defaults here.

		- And then I can set a schedule, or not, and click Next, and click Next again. And I'm going to turn on the Run plan now and Next.

	## EBS and EFS

		- So the next set of file services are the Amazon Elastic Block Store. Now, this is a specialty set of file services that is associated with Amazon virtual machines or EC2 instances. I'm going to actually show you what this looks like. It provides additional capabilities for the information used for those virtual machines, like snapshots. In addition to this, these are associated to individual EC2 machines. There's a new type of storage for EC2 instances that's called Elastic File Store.

		- So just to contrast these, we have EFS and EBS. It's a little bit confusing with all the acronyms. So, EBS is for one file set that's associated to one virtual machine, and EFS can be associated to multiple virtual machines. It's probably easiest to just go to the console and take a look at some of these concepts. So let's do that. So the first thing that we're going to do is we're going to go to storage and content delivery, and we're going to go to the Elastic File System, which is in Preview or Beta at the time of this recording.

		- We're going to create a file system. Then notice, we're going to access that file system. Write files to and read files from Amazon EFS via NFS version four protocol. This is the key difference here. Any number of these C2 instances can work with your file system. The idea with this is it could be a replacement for what we had on premise as a local area storage network, a LAN or a SAN. And you can manage this with EFS, the command line interface, or the SDK. So let's create a file system.

		- This requires that we have a VPC. So instances can connect it to a file system via network interface called a mount target. Each mount target has an IP address that is assigned automatically, so this is very much like LAN or SAN technology. Here's our VPC and here's our mount targets, they're in our particular zone. We're going to associate this with our tag, so we can find it later. Do you remember this from the S3 video? This just allows us to quickly search on these resources.

		- Then we'll say Next Step and create the file system. Great, so now we have our file system and we can mount it from an EC2 instance with an NFS version four client installed. We'd have to go over here for the instructions. So using the EC2 console, we're going to associate our EC2 instance with the VPC that enables access to our mount target. So for example, if we use the default security group, we would assign the default security group to the EC2 instance.

		- So we would open in SSH client, and connect to EC2, and install the NFS client on the EC2 instance. If we were using Amazon Linux, Red Hat Enterprise Linux, or SuSE Linux instance, we would use this command. On Ubuntu, we would use this command. That's to install the client. And then to mount the file system, we would connect, and then we would just issue this sudo mkdir efs command, and then we would mount it using the DFS name.

		- Let's contrast this with the standard storage under EC2. If I go inside of EC2, this is to create a virtual machine, and I say launch instance. I take the first one and I say select. I just use the micro instance and I can figure instance details. Just take all the defaults, this is really what we're interested in, an add storage. So we have our root storage here and if we wanted to add a new font volume, notice we'd have EBS by default.

		- We do not have the other type of storage available through the console because, as I just showed you, you have to create them mount point, and then you have to install the client. Then you have to access the mount point. If you wanted to add EBS storage to an individual instance, you just specify the storage, and you specify the size, and the volume type. You can use SSD's or Provisioned IOPS, those are SSD's that have a higher throughput, or regular discs. Of course, these are going to cost the most. So we just say this and then we can specify the encryption behavior, or not.

		- We could, of course, tag our instance. We're getting used to this, it's just the best practice that I want to reinforce throughout this course because it will save you lots of time. Then we can review and launch. Then we can launch. Then we have to associate a key pair if we want to connect to it. We need to create a new key pair, and download it, and then launch our instance. If we go inside of EC2, and refresh, go to the EC2 dashboard and instances.

		- You can see our instance is initializing. If we take a look at this you can see that we have EBS backed instance. We have the lynda tag. Just to reinforce while this is spinning up, if we go over to the lynda resource group, you can see there's our instance and we can manage it through here. There's our bucket. So you can kind of get a sense of how this is very, very useful when you've got a lot of objects in the console.

		- So to summarize, the Elastic Block Storage is the file storage that's by default associated with EC2. And the Elastic File System is a replacement for a SAN or a LAN, that can be associated to multiple instances, and in order to do that you have to run a command line and connection via SSH. There's no way to do that yet through the console in this Preview implementation.

	## Summarizing raw storage choices and prices
		- Now let's summarize what we've learned about raw storage on AWS. So the storage service that you select is going to be dependent on your particular use case. We learned that S3 is the workhorse and nearly every customer, I think actually every customer I've ever worked with, uses warm file storage. You do want to understand all the different properties. Particularly at the bucket level, and then use the appropriate type of S3 storage to get the best value. By default you get the S3 storage, but you can go with reduced redundancy for a cheaper cost.

		- Glacier is the archival storage and it's nearly free to put information in. Remember it's a penny a gigabyte a month. But also remember that it does cost to retrieve both in money and in time. Another type of storage that we looked at was associated to virtual machines or EC2 instances, and that's the individual EBS, Elastic Block Storage, or the newer EFS, which allows you in the latter case to associate your files with more than one EC2 instance. It's emulating what you have on premise as a LAN or a SAN.

		- As we'll see when we progress through this course there are other types of storage, particularly around SSD or super fast discs. And these are used with various data services. You really want to make sure you understand the cost of what you're buying when you look at raw storage. Because as you're implementing various types of data projects, especially large or huge projects, the cost can be significant, so to that end I'm going to show you a tool that I use to help me figure out costs. This tool is the Amazon Web Service's Simple Monthly Calculator. And it's at calculator.s3.amazonaws.com/index.html.

		- And you can see on the left that we have a reset button. So I'm going to click that and make sure we're cleaned out here. And then we have our various services. So let's start with S3. There's a lot of switches and you want to use all those switches. So the first thing is you want to use the appropriate region and for the purposes of this recording, I'm in Northern California. Also you want to read everything on the calculator page. So notice inbound data transfer is free and outbound is 1GB free per region per month. So I'm going to put in 1TB of storage here.

		- Then I'm going to put in 1TB of reduced redundancy storage. And I'm going to just leave off the requests and the data transfer, which is a little bit inaccurate because of course you need to get the data in and out. But you can see our monthly bill is $59.24 for 2TB of storage using 1TB in regular and one in reduced redundancy. Now of course obviously we could just change that. And see that the difference in pricing is about $6 a month by putting 1TB in reduced redundancy.

		- So core storage is really very, very cheap. Now in addition to this, we have some other services that we want to take a look at here. Let's clear out this S3. And let's go down to Glacier. And let's put 1TB of storage into Glacier. And you can see $11. So it's all small money at this point, but when I'm working with huge scenarios where we're in the petabytes, core storage costs do matter and some of these switches can help my customers to save money if they don't have a need for keeping everything in warm storage.

		- Of course again, I'm oversimplifying here. I'm not putting anything in "Requests" and "Data Transfer". And you'll need to fill this out in terms of your particular usage scenarios. You might remember in an earlier movie I talked about load testing not only for testing performance, which frankly is rather predictable, but also for testing for costs. Because it's complicated to figure out how many requests, how much data transfer is going on if you have multiple applications working with these services.

		- Now another aspect of this that I want to call out is AWS Import Export. I didn't talk very much yet about the initial data load, and that is something that my customers encounter. So I still have the $11 from Glacier on here. Notice "import to S3," number of TBs, let's just say 10TB. "Type of Interface" and then this is when we are actually working with physical drives here. And this is "Return Shipping" and notice this is 335.

		- Eleven of that is for the Glacier storage. This is 10TB to get the initial data load up. So this calculator is very useful in terms of some of the core storage. You'll notice not everything we talked about is on here. Of course if you wanted to work with EBS, you'd have to go over to EC2 and I'm not going to bother to do that because you can kind of do that on your own. We're focusing more on bulk storage, which is S3 and Glacier, but this storage calculator is really useful in terms of getting some sort of estimate. Particularly when you're starting a new project.

	## Other AWS File Services
		- Rounding out our discussion of AWS File Services, there are a couple other services that I just want to touch on. Storage Gateway, which allows you to integrate on-premises storage with AWS Storage Services and CloudFront, which is a CDN or allows you to stream files where your customers live. So let's take a look at these two. So first under Storage and Content Delivery on the Amazon Portal is Storage Gateway. So it's pretty self explanatory here. It's and on-premise, virtual appliance that provides seamless and secure integration between your on-premise applications and AWS storage infrastructure.

		- You can securely upload your data to the AWS Cloud for scalable and cost-efficient storage. AWS Storage Gateway allows you to create ISCSI storage volumes, storing all of just your recently accessed data on-premises for low-latency access, while asynchronously uploading this data to S3, minimizing the need to scale your local storage infrastructure. So this is used in ... My customer situations, for example, I worked with a geo mixed group and they needed to upload processed files up into the Amazon Cloud for subsequent processing.

		- So they did some of the processing locally. These were test results and just different types of test information, so whether it was photos or detailed text information, they did some pre-processing and then they had an ongoing need to have this open connection. So the steps here are to Provision a Host, Deploy and Download of the VM, and Provision Local Disk Storage, and then Activate Your Gateway. So it's not something that lends itself to a quick demo here, but I just wanted to point out the process.

		- It's not that difficult. It takes maybe like half and hour to an hour and then you're set up with the Storage Gateway. The other service I wanted to point out is CloudFront. This is commonly called a CDN. What this does is it allows you to create distributions. So a distribution allows you to distribute content on a worldwide network or edge location that provides low latency and high data transfer. Click to Create Distribution. So you have two kinds of distributions. Your web distributions, so this is for static or dynamic content, graphic files and this is what I'm usually using it for, media files, and add, update, or delete objects and submit data from web forms and then you can and then you can add live streaming.

		- You store your files in an origin, either an S3 bucket, that's the most common in my case, or a web-server. Or you can use RTMP to speed up distribution of streaming media using Flash. And this is becoming less and less of a thing as Flash kind of becomes older technology. Again, this goes a little bit beyond the scope of what I'm going to take the time to show in this short movie, but I just want to make you aware that Amazon does have a native CDN and I do have customers that only use, particularly the web type of CDN. And you just click through the steps here to set that up and then associate it to your S3 bucket.

# RDS

	## AWS RDS
		- Now, we're going to take a look at relational database storage on Amazon. So, you might be reminded from our introduction when we're considering which database services or which data services on Amazon to use, we have some different ways to think about our data workloads. We can think of them as small or medium, large or huge and the level of complexity of the data and query. Now, how does this relate to what Amazon offers around relational databases. Amazon has this set up services called RDS that offer up partially managed relational database systems and I associate these with small or medium workloads, very specifically the upper size limit for a particular instance is three terabytes.

		- Going beyond three terabytes, you would need to partition your data workload and that's usually done geographically or over time periods. So, an example would be to put data that's associated with North America in US locations and with Europe in European locations. Now, there's something new on the scene that we are also going to talk about that supports large relational data workloads. It's a new service, we're going to cover this well. It's called Amazon Aurora. So in general, what is the Amazon Relational Database Service? We can think of it as kind of a PasS or platform as a service and sometimes even call it DBaaS or Database-as-a-Service.

		- It's different than having a database or a database system setup on our virtual machine or an EC2 instance that you manage and it's priced differently because of the management aspect. How does this work? So, it's partially managed and you're going to use it when you don't have a large amount of database administrator resources on your team so, I commonly use it in customer scenarios that mostly have developers and/or data analysts but maybe don't have database administrators or database administrators are overworked with the on-premise resources.

		- What you can do using the implementation of RDS instances is you can allow Amazon to take care of routine database maintenance task like backup and even to have performance guarantees. This is implemented via SSD or solid-state drives and you can also set provisioned IOPS or throughput. These are performance guarantees. This is a fantastic set of services that many of my customers find very useful. Basically, you're allowing Amazon to manage not only the maintenance but also the tuning and performance of your database.

		- So, you can just focus on getting the data in and out of your application. And the Amazon RDS service includes, most popular relational database management systems. Everything from open source systems like MySQL to commercial systems such as Miscrosoft SQL Server and Oracle. So which ones are they? There are categorized in two buckets. The first is open source, so My S-Q-L or MySQL and PostgreSQL, those relational database systems, do not require commercial licenses, cause they're open source and that's important when you're considering the cost of partially managed database systems.

		- So, we'll look at that at the end of this section as we do with all the services. And then we have commercial database systems, Microsoft SQL Server and Oracle, also available and these do require licenses. So, it's important to accurately estimate cost when you're looking at using the relational database service on AWS.

	## Using MySQL Workbench
		- The first service in AWS Relational Database Service I'm going to look at is the one I use the most, and that's with MySQL. So, this is a fantastic, simple service to use that allows you to have relational storage that's partially managed. It's MySQL-compatible so you use MySQL tools. I generally use the MySQL Workbench. You can use any tool that you like but do be aware that you do not directly connect to the underlying Operating Systems, the instance, using SSH or something. You have to connect to the database instance itself.

		- So first we'll go into the Amazon console, set up the instance and then, in a subsequent movie, we'll work with the SQL Workbench so you can see how to work with a little bit more granularity with your instance in RDS. So, here we are in the Console and we are now in the Database section. And we're going to click on RDS. And again, it gives us our familiar information that I really advise you read when you're new to this. It really does help you get started and also the Getting Started Guide. But we're going to go ahead and click Get Started Now.

		- And you'll notice we have a selection of our DB Engine. By default it is My SQL, so we'll go ahead and we'll Select that. And then it asks us, are we going to use this in production or are we going to just test this? And we're just going to test it so we're going to say, it's outside of production use. This is one of the aspects of management and this is one of the reasons that you would use RDS rather than, as I was saying in an earlier movie, just installing MySQL on an EC2 Virtual Machine and managing it yourself. You have this ability to have Multi availabilities on Deployment and/or Provisioned IOPS.

		- So what that does is that takes care of high availability and also performance guarantees without you having to perform the tasks to set those aspects up manually. So, you're paying for this partial management. But in this case, we're just going to go through kind of the mechanics for the first time here and just say, no, we're just going to have a simple instance so that we can look at it and see how you work with it. So, we're going to click, Next. And notice, we have a License Model. In this case, it's just a public-license. And then we select the Engine Version.

		- And then we select the Instance Class and this is the size of the machine. We'll just pick something pretty small for this. Again, I'll just show you the variety. We go all the way up to the 8xlarge, which has 32 CPUs and 244 Gigs of RAM. That's why I said really, medium, in the initial discussion of the data workloads here, because I will sometimes use these larger-sized instances with relational workloads. But we're going to just take something kind of small. Let's just do this one, the medium.

		- Multi-AZ Deployment, we're not doing that. And if we were, we'd have to select the Availability Zone. And notice what you're doing there. It will create a synchronous standby replica in a different Availability Zone and will automatically fail over to the standby in case of planned or unplanned outage. And then, the Storage Type, by default, it defaults to SSDs, and if I pass my mouse over this it shows that I have three choices here. And SSDs are what are recommended. So, it tells you what the IOPS or the performances by default here.

		- And then, it says that you can select a higher level of IOPS. This is something I alluded to in an earlier movie but it becomes more important at this point in the product selection, which is, you want to do some kind of load-testing when you're moving workloads to the Cloud, to figure out if you need to pay for the Premium level of services. In this case, that's Provisioned IOPS. And Magnetic Storage is the cheapest basically. It's going to be the slowest and the cheapest. Again, this is great for testing and can really impact your pricing. So, we're just going to go with General Purpose (SSD) and then it gives a warning here.

		- Provisioning less than 100 Gigs of General Purpose (SSD) could result in higher latency. Yeah, that's fine, but we're just testing, okay. So now we're going to put in a Database Identifier. And again, it gives us some information here that it's case insensitive but it's got to be all lower-case. So, I'm going to call it, mycooldatabase, and see if that's going to be unique. And then I'm going to start with my Username and I'm just going to call it, awsuser. And this is the user for this particular instance, to log in.

		- I'm going to use a really weak Password here. And now, I'm going to click, Next. And I get a warning that I have to say that I do not want Multi Availabilities on Deployment, so I'll click, Next. And now, I need to fill out the information about Network & Security. So, by default, I'm in VPC and then I can select a subnet group if I want to. So this has to do with where the traffic can communicate. Again, I'm going to come skip through these settings 'cause this is just a test.

		- If you were going in production, you would very well want to, I call it, ringfence or limit, where the traffic from the database is permitted to go, for security purposes. And the Database, we'll call it, mycooldata. And the Default Port is 3306, when we connect. And then we have the Parameter Group, that's the configuration settings. And then we can say, if we want to Copy any Tags we have To Snapshots and Enable Encryption.

		- This is a great feature that's only available with certain types of databases. So, see it tells you here, this doesn't support encryption. So Backup, we retain our backups for seven days and we can set a Backup Window if we want to, and that would be a time period, but we'll just say, No Preference. Again, you wouldn't do this in production. And Maintenance, Auto Minor Version Upgrade, Yes or No. And Maintenance Window, we can set that again. So again, this is the partial management. So, let's click, Launch DB Instance.

		- And then we're going to go ahead and View the Instance. And that will take a couple of minutes to create.

	## Logging data to SQL Server RDS
		- All right, well it's been about five minutes and our AWS RDS MySQL instance is nearly created. You can see that it's got a backing-up status and then we have an Endpoint. This is going to be important because when we use our client tool to connect to it, we're going to need to connect to this Endpoint. So let me refresh and see if it's fully created. While it's completing, let's just tour through what we see in this interface here. So first of all, we see information about the MySQL instance itself.

		- We see that there are no connections to it. We see the Class. We see the Alarms. We see the Events. We see Monitoring. We see the Read and Write IOPS and the Swap Usage. And if we click inside of here, we can see configuration details. So, this is basically going to be the implementation that we got based on what we put in through the wizard. We can also see information about Maintenance, when the maintenance is being performed, what was the last date of maintenance. Of course we see this in the console but you can also programmatically dump this data out to logs.

		- You can also send yourself alerts and notifications using some of the other Amazon services, like we saw in the file sections using SNS. Looks like this thing probably is available. Let's just make sure, and I'll just click right here, and we don't have any replication. So this would be a replication tree and this would be if we were using multi-availability zone. Notice over here we have a slider, viewing one of many database instances, and I'm going to go ahead and just refresh one more time. It's almost ready. While it's completing, we're going to start preparing our client because we really just see top level metrics here and this isn't designed to be a client tool to interact with the database.

		- So, what we're going to do is go over to the website where we would get the client tool for MySQL and that would be www.mysql.com/products/workbench. What I've already done is I've created a free account and logged in. So when you initially go to this, it says, "Create Account." So, you have to do that with some email address. It's free, but you have to log in. And then you would want to get MySQL Workbench. So if we click Download Now, and then you want to scroll down, and it will detect the OS that you're running on.

		- I'm demonstrating on a Mac, but you can see that MySQL Workbench runs on basically almost any modern operating system and then I'm going to go ahead and click Download. And I have to fill this survey out, so I'll do that really quickly. So now I'm going to click Download Now. I will run the installation. This is installed. I'll switch back and see if my instance is created. And I see now that my instance is available. So, let's look at some of the actions that you can perform in the console before we connect to this.

		- We can Show Monitoring for Multi-Monitoring or Single. Oftentimes you're going to have replicas when you use RDS because you want that high availability. Of course we only have the one instance, so it's not going to be any difference in the view. But in the Instance Actions, we can Modify, Reboot, Delete, Create Read Replicas. We could promote a Read Replica to a Read/Write Instance, Take Snapshot, Restore to Point in Time, Migrate the Database or See Details.

		- So, it gives you a quick menu for the most common activities around a managed database. Read Replicas, in particular, is something that I've used with professional clients because then you can reduce the amount of read workload on OLTP or working online transactional database. In other words, you're separating the write activity from the read activity. Another feature that I've used in production is taking a snapshot. What that will allow you to do is to have a history. So, for some of my clients for regulatory purposes, they need to have snapshots at point in time.

		- So, we will use this feature so that they can meet their compliance need. Now we're going to need the endpoint for use in the client, so let's copy that. I'm going to double click on MySQL Workbench to open it and I'm going to make a new connection by clicking the Plus. The Connection Name will be AWSDemo. It will be TCP/IP and the Hostname will be that long name that we copied.

		- The Username will be the username that we associated to the instance and the Password will be the Password that we've set up for the instance. Now you'll notice we have the port number appended twice. So a tip, when you work with this, is that because you have the port setting over here to the right, even though when you copy it, it shows the port setting, we need to just take that off. Default Schema can be left blank at this point.

		- Now let's test our connection. We've connected successfully, and let's click OK. Now if we double click, the SQL editor is open and we can work with our database. Do you remember mycooldata? We can create tables, work with views, as if our instance were local. It's really great to leverage all the knowledge that you have around working with relational databases by using technologies and clients with instances that are hosted on the Amazon Cloud.
		
		- Now, in addition to My SQL on the Amazon relational database services, you also have Microsoft SQL Server and this is something I've used frequently with customers who have familiarity of SQL Server and have a need for the more advanced features that you get in this relational database system. We're going to take a look at how the setup goes. You'll find that on the Amazon site it's very, very similar and then we will talk about some of the production situations that I've used this in. Now, normally you would connect to this using SQL Server Management Studio or SQL Server Profiler, but I'm demonstrating on a Mac so you would have to have a Windows machine to use these tools.

		- Of course, a license is required when you work with commercial software. So let's go over to the console and take a look at how this works. So here we are in the AWS console and we're under a database section, under RDS so I'll click that. And it's gonna be very familiar to what you saw in the last movie if you watched the one on My SQL and RDS. And we're gonna go ahead and click Instances. And then we're gonna click Launch DB Instance. You se we've got the MySQL instance that we created in the previous movie there. And then we're gonna click on SQL Server.

		- Now, we then need to select which addition and then we'll select the version as we go in through this. So we have everything from Express, which is great for testing on maximum of 10 gigs, but it's cheaper in terms of licensing. And you can actually run really small commercial workloads on this. There's Web Edition, there's Standard Edition, and Enterprise Edition. And again, just for the purposes of this quick demonstration I'm going to select Express. So in Express, we've got license included. Now, in the more feature rich versions of SQL Server, you have license included which is Amazon's reselling the license, basically, or bring your own license.

		- So you're going to want to read your documentation and do some cost estimations. Because it does impact the cost, it's very important. Here's the engine instance version. So you have a 10.5 or 11 and then we have the Class and I'm just going to pick a medium again. Then we have the Storage Type which, SSD, you'll remember from the demonstration, the previous movie on My SQL that the options are SSD, Provisions iOPS, or Magnetic which is the older, sort of, cheaper version.

		- I'm just going to show you iOPS just to be inclusive here. If you use iOPS then you have Allocated Storage and then you have Provisioned iOPS and that is your through put. So it says over here, i/O-intensive database workloads. And you really can only determine that if you do some sort of testing on your workload. This is a tip from the real world; if you're putting production workloads up onto the RDS, which is usually what you're doing, you're not starting out something that's brand new in greenfield. You're usually moving production workloads. You want to do some load testing so that you can set up provisioned iOPS.

		- In general, you're going to want to choose to pay for that. So I'm going to actually just set this back to SSD and it just warns us that we should use more storage. Which is fine, we don't need storage for the demo. And then for the DB Identifier, I'm gonna call it mycoolsqlserver. And then next step. Okay, so, what we have in here is the VPC information again, as we saw in the previous demonstration. And this time we'll be working with Port 1433 and we have the same options around Encryption and Snapshots because our instance class doesn't support Encryption.

		- Same options around backup and maintenance. Another aspect of this that I've used in production is to put some of the workload into SQL Server, and some of the workload into My SQL. So this is what I talked about earlier in this course about partitioning the data by service rather than by database. For some business situations, it makes sense to use a database system that has more features for certain applications or certain parts of certain applications. And this is something that we'll talk about later in the course when we talk about architectural patterns.

		- But I wanted to just introduce this because I've actually done this in production a few times. I'm going to click Launch DB Instance. And then I'm going to click View. And as with the MySQL instance, this is going to take a few minutes so I'll come back in the new movie and I'll summarize what we see once this is created.
		- We can see that our RDS SQL Server instance is nearly ready to go, it's going through a state of backing up as our MySQL instance did and our endpoint is available. It should become available in just a minute now. While we're waiting for this I'm going to go ahead and show you another feature that I've used with some of my customers who choose RDS. Some have regulatory requirements around logging who has access to the database. Some of the databases have logs but one of the services on Amazon that I've used in conjunction with RDS databases is under Administration and Security.

		- That is 'Cloud Trail'. User activity and change tracking. Let me go ahead and go in here, basically what it does is it creates log files on activities around creative find, AWS Service API calls. Log files will be stored in S3 and you can then work with the log files manually or there are third party management tools that will read the log files. I'm going to just go ahead and turn this on and I'm going to create a new bucket and I'm going to call this 'Demo Log for RDS'.

		- Turn this on. Now I can look up API activity related to creation, modification, deletion of resources in my AWS account in the last seven days. Let me now go over to the RDS here. Let's see if this is done, first of all. This is still cooking. I'm going to go over to S3. It's kind of interesting to see how the services interplay. Here is my demo log for RDS and I'm going to go ahead and create a folder inside of my other bucket, something that I'll need later in this course.

		- Then I'm going to go back to my buckets and go to Demo Log for RDS in AWS logs. This folder is designed to hold a log of all activities which in our case is going to be RDS activity. So it would be activities around the database server on the database. Let me go back and see if this is ready yet Still not ready. Since we don't have a client I'm not going to take the time to wait for it to be available. What you would see, I'll just go into the MySQL as a point of comparison here, is you would see an endpoint and then with your SQL server client you would connect to that endpoint as we did in the MySQL demonstration with MySQL Workbench and supply the credentials and then just use your client.

		- It's a similar type of an interaction, which is a great aspect of working with this. You get similar type of services as well in terms of monitoring and automatic backups and so on and so forth. I'm just going to go back one more time and see if it's available. Now it is available. I'm going to go ahead and click on it, you can see the instance actions are pretty much the same thing as what we saw in MySQL and do note that we have the "Restore to Point in Time' which is a highly requested feature for SQL Server that is managed on the cloud.

		- It's just something that is really nice to see in the RDS implementation because it's something that is often a part of disaster recovery and high availability plans. If I click this you can see 'Creating a New DB Instance from a Source DB Instance at a Specified Time'. I literally can restore to a date and time and then I'll be creating a new instance is done through a wizard. Of course when you're just testing it out it's good to see what capability is there but for your DR plan you're going to script this with the Amazon CLI tool or some other management type of tool.

		- I just like to point it out because I find that a lot of my customers like this feature and use it really frequently. The other thing that you can do, of course, is you can take a snapshot, which I'm going to go ahead and say 'Take a snapshot'. I'm going to say 'mycoolsqlserversnapshot'. Something again a lot of my customers use on a regular basis Take a snapshot. You might wonder, why are there already snapshots here? Snapshots are taken on creation. As was mentioned, you can restore from snapshot, you can migrate a database.You can copy or delete snapshots. Lot's of great functionality in RDS instances.

	## Aurora
		- Okay, we're going to talk about an RDS service that I'm very, very excited about, it's called Aurora. It has just gone to general availability shortly before this recording, so it is a service that you can use and I think a lot of you are going to find it to be super useful and super cost-effective. So what is it? It's basically massive partially managed MySQL. You might remember from earlier movies that the current limit of an instance is three terabytes, which is quite a lot and is sufficient for many small and medium workloads.

		- However, for the large and huge workloads, although there are many types of database services, many of my customers just do not have familiarity with those technologies. And although the cost might seem to be cheaper, the actual cost really isn't cheap because the amount of time it takes for the people on staff to learn how to use these new technologies and to make them work properly and effectively is not counted up accurately. So that being said, I often get asked the question, "Isn't there just a very, very large relational system that I can work with?" and now there is, so thanks to Amazon.

		- You can go up to 64 terabytes. Yeah, that's actually true. 64 terabytes in Aurora instance and it is well priced. So we'll look at the pricing of all these services at the end, but I can see that this service is really going to be a game changer. A service like this is simply not available from any other cloud vendor and it reflects Amazon's close attention to what customers actually want. I am very, very excited to start working with my customers on this service. So you can think of it as a massive OLTP or read-write production database that's partially managed.

		- It has built-in scalability as well. Now I don't normally do this but because this is a new service we're actually going to take a look at the documentation so we can understand some of the differences. So I recommend, in general, that if you're going to put relational workload on the Amazon cloud using RDS, you actually read the entire set of documentation because I have many, many times gone out to a client, and found a configuration setting or an implementation problem that was directly addressed in documentation and the client simply hadn't taken the time because of the familiarity.

		- They had said, "Well, I'm an expert in MySQL" or "I'm an expert in Oracle" or so on and so forth. That may be the case in many of the features and configurations that you've used on-premise might transfer directly, but it is really important to consult the documentation for those small differences between the on-prim and the managed version. Now in the case of Aurora, if you're going to take a look at that, I recommend, it's only a page or two, you just read this page and here's the URL at the top. And if you scroll down, it talks first about how Aurora is MySQL compatible and then it has a very useful diagram here that talks about how an Aurora cluster is setup, that you have a primary instance, which supports read-write workloads and then it has a replica.

		- It supports read operations. Each cluster can have up to 15 replicas. So, if you're going to be putting data workloads of these very large sizes, you're going to want to think about where you're going to want to put these replicas. And again if this is new to you, you're probably going to want to work with a professional cloud architect to help you to partition your data so that it has the appropriate setup. I also have found that if you can read this section, the Comparison of Amazon RDS for Aurora and Amazon RDS for MySQL that can clear up a lot of questions that I commonly answered about Aurora.

		- You can see features such as read scaling, as I mentioned, Aurora supports up to 15 replicas and RDS MySQL supports five replicas. Aurora only works with the latest version of MySQL, so if you have some legacy applications that might drive your decision to using plain vanilla RDS for MySQL. Aurora is available in a subset of all the regions. And Aurora only supports this version of the database. MySQL supports both of these versions.

		- So it's very important to thoroughly read the documentation because as Amazon is aiming for MySQL compatibility, it's very important to understand how they have designed this massively scalable SQL database. Now the next movie we'll actually work with it and you'll see that it's deceptively simple. When you work in the console it will be very, very familiar to the previous movie on setting up MySQL and that's by design. So the core is very consistent with the experience, but it's always in the details that the top-level performance and that more consistent workloads are implemented, so we'll switch over to that in the next movie.

	## Exploring Aurora
		- So here we are at the Amazon console in the database section under RDS. And we're going to go through their process to launch an Aurora Instance. You can see Amazon's really excited about this too. They have a big button right here. And we will click select, and we'll select the Instance Class. Notice there only large and very large. Multi-AZ, we won't do that for the demo here. We'll call this mycoolaurora. Use our same user name.

		- Click Next Step. VPC information, we'll just go with the default. The Cluster Identifier, we'll call it aurorademo. Database Name, we'll call it auroradatabase. And then we'll go with the defaults. Here's the Backup Retention, the Maintenance window. All this should be pretty familiar by now. And we'll click Launch DB Instance. And then View.

		- And we'll click on Instances and RDS Dashboard. And Instances and you can see that our instances that are available MYSQL and SQL Server Express are listed down here and Aurora is listed at the top. And if I click on Instance Actions there are very similar to what we saw in the other RDS Instances. I can create an Aurora Replica, which is a different type of replica.

		- I can Failover, Take a Snapshot, Restore to Point in Time, Migrate the Database. So again, Amazon has worked really hard to make this simple to use and massive relational storage that is designed for both reading and writing. It's an impressive product and I'll be really excited to try this out with my customers and to hear how you move your production workloads up into Amazon's cloud using the Aurora service. While we're waiting for our Aurora Instance to finish creating, I wanted to circle back and show you some of the logging from CloudTrail.

		- It takes five minutes to initiate. So here we are at the console and you'll remember in a previous movie when we were creating a RDS SQL server instance, we turned on CloudTrail logging, which is something that commonly do with the enterprise customers for our regulatory compliance. You might remember that at the end of that movie, we created a snapshot and so we can click in here and we can see all the information about how that snapshot was created. Now this can be a quick and dirty way to just look up something that occurred but more often than not, what you'll do is programmatically, you'll work with CloudTrail and you will export these logs and then either use them for monitoring in compliance or just keep them externally in addition to having them available on the Amazon console.

		- So this is a service that commonly use and I just wanted to show this when we were talking about RDS because I just use it with several of my customers. So, let's go back over to the RDS console and check on the status of our Aurora Instance. And we can see we've got three out of 40 possible, DB Instances. And it is available, which is exciting. And so here is our end point and this is going to be just like MYSQL so we are going to copy this and remember we're going to take off the end.

		- And then we are going to switch over to the client. And we are going to create a new connection. And we're going to call this AWSAurora and paste this information in taking off the port number at the end. And then use that same user so I'm not following good naming conventions here and put in the password, say OK. And then I'm going to test my connection.

		- I was able to connect and say OK. Double click. There is my auroradatabase. So, to me this is so exciting that you can have a 64 terabyte, highly performant, highly available relational database on the Amazon cloud and integrate with the other services. Again this is something I'm going to be working with a lot of my customers on so I'll certainly share performance tips and just real world scenarios and I'll be really, really interested to hear how all of you listening use Aurora as well.

		- Also, I'll be interested to hear what the other cloud vendors, if they respond to this, because right now Amazon has this market to themselves and it's a very, very interesting market. So this this a massively scalable OLTP or read write database that is partially managed and that's Amazon Aurora.

	## Other RDS Options
		- Now just for completeness RDS also offers hosted Oracle and PostgreS. Oracle of course is a commercial product and requires licenses and it supports Oracle 11 or 12. PostgreS is open source and I find that I'm often using that with startups where the developer talent in-house is familiar with the PostgreS implementation. If we take a look at these offerings they're going to really be identical. So we'll quickly jump over and just look at some of the implementations on the portal.

		- For RDS Instances Launch Instance, we click on Oracle. You can see we have the Enterprise Edition, Standard or Standard Edition One. If I click Select and Next Step, the licenses I have available are 11 or 12 as of this recording. I'm actually just going to go back and go to Instances and Launch and PostgreS and Select and Next Step and show that the engine versions, the latest is 9.4.4 9.4.1 is recommended as of this recording.

		- One tip from the real world is if you're moving production workloads you want to of course match the versions and editions so that you have reduced application compatibility problems when you are moving these workloads on over. Because the process to set these up is identical I'm not going to go through the steps. And I do really recommend that if you're going to move either SQL Server or Oracle workloads you take a look at the new Aurora service. Because in some cases you're going to be able to move these workloads at a much smaller cost and actually get more features.

	## Summariing RDS and prices
		- Okay, we've covered a lot of ground in this section, so let's summarize. So we want to look at what Amazon offers in terms of managed relational database management systems. So, we have a set of services that really revolve around Use Cases. So in the real world, when I've got small, relational Use Cases, I'll use RDS MySQL. And of course it's Open source, so it really saves a lot of money not having to buy any commercial licenses. For medium workloads, or workloads that need feature sets or queriability that exists in the developer and DBA teams on premise, I'll use SQL Server, more commonly than Oracle frankly, on RDS.

		- But I have run in two couple Oracle shops. You will have license cost, and I generally only recommend this if you have existing talent on premises that knows how to use the advance features of these databases. PostgreSQL I often use with start-ups. I put Medium in terms of Use Case, but it's small to medium, same thing with MySQL Open source. And then of course, I could hardly contain my excitement about Aurora, large to huge, up to 64 terabytes and it's a new service. Now speaking of pricing, pricing for managed relational databases on the Amazon Cloud is tricky.

		- So, I'm going to show you a couple of my tips when I do price estimates with my customers. The first thing is I'm going to show the pricing calculator that you've already seen when we talk about file storage cost comparison. And here we are in Amazon RDS and the URL is calculator.s3.amazonaws.com/ index.html. Now one of the situations with this calculator is Aurora is not listed here yet, but I'll show you how to find Aurora pricing in just a minute. So, we're going to work with this and then we'll talk about some of the aspects of it.

		- First you have to Choose your region, of course, and then you're going to add a new row. Now, the very first thing it says up here, I don't want you to skip over this, this is On-Demand DB Instances. When you're purchasing Amazon compute and manage services, there's On-Demand, Reserved and Spot. So, Reserved means you buy in advance for a one year or a three year period of time and you can get substantial discounts. And I find a lot of customers do not understand Reserved, don't use it and don't save the money that they could be saving when they know fully-well they're going to be using these services for long periods of time.

		- Most customers choose the one year Reserved because there are price drops. And so that's the best sort of middle ground. We'll come back to Spot when we talk about some different kinds of work loads later in this course. So, On-Demand is how we'll start, and I'll just call this one a demo. And we're gong to have one instance, 100% utilized. We're going to use MySQL and we're going to use a medium, and Storage is going to be General Purpose, we don't have any IOPS going on here. Then you can see, that'll be $84 a month.

		- Now we might want Backup Storage and we might want Reserved Instances. So let's do Reserved Instance here. And let's call this one reserved. And 100% Utilized, and this is going to be medium again. So, we have to set our Utilization, Heavy Utilization. And our regular Storage. Then we're going to get rid of this one. And this one is $19 a month. Do you see the difference? I mean these are some really small sized servers.

		- You want to have a mix of both of these types of Instances, generally, in production environment. Sometimes you go all Reserved, but I find that the most common situation is some On-Demand and some Reserved. So, we also want to consider for a Data Transfer In and Out, and Data Transfer around the Region. Again, this comes from a low testing. I'll do this one as a Reserved, because that's more normally how you would do it. SQL Server, 1 Instance, and it's going to be SQL... We had $20 before, and we're going to have a Standard License and we'll have a medium, Heavily Utilized, and General Purpose.

		- And then we are $319... Look at the price difference. And this is coming from the licensing. So, again, I'm using really, really small examples here. Much smaller than I usually see in production workloads. But it is really important that you look at the cost of these services and you only buy what you need to pay for. And don't just go with the defaults. Because when you're moving into production you can save a lot of money if you take the time to read the documentation and get exactly what you need. Now, to understand Aurora, because if I click here, we don't see Aurora listed at all.

		- I'm going to go over to the RDS Pricing page. I think this page is a little bit confusing, but I'll kind of guide you through it. aws.amazon.com/rds/pricing. So the first thing that we talk about here is the Instance Types, so these are the types of machines. This is yet another consideration from the real world depending on the workload. For example, if you have some kind of application that uses a lot of memory, you're going to want to use a Memory-optimized Instance. The pricing is different, and the performance is different. You're going to want to select the appropriate Instance for your workload.

		- So, if I scroll down here... And these are the different types of Instances. Generally it's going to be a memory-enhanced Instance, although there are other types. There are Instances that have support for processing on the GPU, and for high IOPS. So you're going to want to research the Instances as well. Once you select your Instance type, and here they're listed for example, then we have On-Demand Instance Pricing. And you can see how this is set up for Aurora, MySQL, Oracle, SQL Server and PostgreSQL. And then by region, and then Reserved Instance Pricing.

		- So let's just do a little bit of comparison here. So, Aurora... Let's look at an extra large, so this is about $0.58 an hour. And if it's Reserved, we have a 1-YEAR and a 3-YEAR, so it is $0.32 an hour. Basically it's half price. Which is important to understand because if you're moving a bunch of production workloads there, you would like to have the price be half with the On-Demand prices. So again, I recommend that when you're tryng to forecast pricing, you go to this page if you're working with Aurora.

		- And also the pricing calculator, because that will allow you to calculate some of these other costs like Data Transfer. And you can also combine with other services, because you normally have storage for example, with S3 and possibly with Glacier, so this combination of the two can start to give you a truer picture of what your costs will be when you're working with Relational Database Services on Amazon.

# NoSQL

	## Exploring NoSQL options
		
		- As we continue in our tour of AWS data services, we're going to, again, think about our workloads to consider the services that we might use in the next category. So you may remember, from previous movies, that the way I look at data workloads in the Cloud is the size, small or medium, large or huge, and the complexity. And I look at the interrelationship between them. So, our next category is called NoSQL. Now, in addition to that, I have a category that is emergent called NewSQL, that we're going to talk about.

		- And, I find most often that these service offerings are most related to small or medium workloads, and in some cases, complex workloads. What I mean by that is complex source data and/or complex query types. So, what offerings are available on the Amazon Cloud in the NoSQL and NewSQL buckets? We're going to, again, use some categorization to help us, because in the NoSQL world, in general, there are over a hundred and fifty different currently available NoSQL databases and it's really difficult to get a grasp on it, if you look at that level.

		- So, I bucket these into different types and these align with the Amazon offers. So there's key/value databases, which basically, you can think of as really large dictionaries or hash tables and they're often contained in memory for speed. A simple example would be, I worked with a healthcare system and we had the customer name and the customer ID number in a really large cached system. These are key/value databases. The next type of NoSQL database is a column or wide column database.

		- And these are commonly used in situations where you're going to have data that is irregular, in that it will have an identifying key and it'll have one to many associated attributes. So, a common business example of this is social media type of solutions that you will have many, many optional fields. A very specific example is I worked with an Instagram-like company, a start-up in California, and they had the need to have an identifier, but they really had no idea how many hashtags would be associated with their particular version of the photo.

		- So they chose to use a wide column database and again, this has to do with the complexity of the data. The next type is a document database and I call this the new XML, in that you have a semi-structured type of data. Now, document databases usually don't use XML these days, they use JSON or DSN data, and these are just object notations that are commonly passed via files on the web and you see them with curly braces. The most common of this is MongoDB and we'll take a look at Amazon's implementation of this as well.

		- And then the last type of NoSQL database that is out there in the wild is something called a graph database. And I call this the noun-verb database because, in addition to persisting the entities, in other words, the objects and their properties, a big difference between graph and the other types of NoSQL databases is they also persist the verbs or the relationships. So, customers drive cars, customers eat food. The verb, or the relationship would be persisted and stored in the graph itself.

		- These are very, very commonly used in situations where queries will be done using these relationships because, of course, if the verb is persisted, then the result is faster than generating that at the time of query. So, we'll overlay these general categories of NoSQL databases onto AWS services and talk about business use cases, because one of the complexities of working with NoSQL is picking the right product for the right workload.

	## Understanding Elasticache
		- The first NoSQL AWS service that we're going to take a look at is called ElastiCache. Like the name, it is a service that provides caching for your applications. It's implemented with two underlying NoSQL databases and you get to pick which one is going to work best for you, Memcached or Redis. It's often the cheapest and fastest cache for small solutions. Now, this is in-memory only so if you need to persist, then you'll need to create some sort of pipeline and persist the data to a key-value store that accepts persistence.

		- Let's take a look at what this looks like in the console. Here we are in the AWS console and we're in the Database section. We're going to click on ElastiCache. As with the other services, when you don't have any instances, it gives you a little explanation which I recommend that you review before you implement it, but we're going to go ahead and just get started. You can see, this is similar in the look and feel to the section on Relational Database Services in the previous movies where you just select your engine and then you specify the details, specify the advanced settings and then launch.

		- For the purposes of this presentation, we'll work with Redis. Let's click Next. You can see we select the Engine Version, the Port, the Parameter Group. Replication and Multi Availability Zone are turned on by default for the cluster. We'll put in the Replication Group Name, the Group Description, we select the Node Type. We have a lot of different choices in the Node Type. Similar to the discussion that we had in the movie at the end of the Relational Database section, selecting the appropriate instance type for your workload is critical.

		- After you get through your initial testing, a tip from the real world is to do testing at scale or load testing to determine your workload type and to select the appropriately-sized instance. And it's not only the size, it's also the optimization. You may remember that rtype instances are optimized for memory utilization whereas m and t are optimized for mixed workloads. We're going to go with the default. We have the option to have Read Replicas and we can select the number of Read Replicas and that goes up to 5 for this particular engine.

		- When we've gone through the basic settings, we scroll down and we click Next. In the Advanced Settings, we can configure Cache Subnet Group for networking. We setup the Availability Zones, the Primary, and these are Amazon zones, the Read Replicas and a VPC Security Group which is the default, unless we change it otherwise. We can schedule Automatic Backups, those are turned off by default, and we can schedule a Maintenance Window, we can also enable SNS Notifications if that's part of our business requirement.

		- We're going to accept the defaults and click Next. Review the settings and click Launch Replication Group. Then we're going to click Close. And you can see that we get three nodes in a Redis cluster. As with Relational Database Service, this is partially managed so you do not have root access to these machines, you would not SSH into them to work with it, you would use a Redis client, whether it was command-line client or a GUI client, in order to work with this. You would just connect to the endpoints once they were launched.

		- Now, as with Relational Database Service, notice, you have some other capabilities and you can see it listed over to the left here. You can take snapshots, you can cache events, you can adjust your configuration settings. Also notice across the top, you can backup, reboot, delete, modify, or manage your tags inside of here. Now, just for fun, while this is launching, let's go back to the main console and let's go over to CloudTrail, and you may remember this from previous movies, this is our audit log, and you can see that there are a number of activities that are occurring with the launch of this cluster.

		- Not only is the use of CloudTrail great for compliance, it's also good when you're learning to understand exactly what is going on when you use these wizards and quick setups in the console. So that when you move from a POC to production, you can actually script these activities and modify the scripts as that meets your business needs. As you can tell, I'm a huge fan of CloudTrail, I use it with a lot of my different projects. Let me just go back over here to ElastiCache and you can see we have Cache Cluster being created.

		- Let's see if it's spun-up yet, it's still there. So I'm going to pause this movie and then I'll show you what it looks like once it's created. Now, we see that our cluster is available and we see that we have three nodes. If we click into a node, we can see detailed information about our node and we can work with the node using the menu items to the left and across the top. Notice, if we click the drop-down by All Cache Clusters, we could filter by database engine if we wanted to. This interface is mostly view so, as I said, you would probably work with some kind of client if you wanted to do anything different than what you can see in the menu options here.

		- In addition to working with your cluster by reviewing it inside of this console view, there is another utility that I commonly use with clusters that I want to show you. I'm going to click on the home button in the Amazon Web console and then I'm going to click on the CloudWatch service. Now, this is under the Administration & Security section and as we had seen in previous movies, CloudTrail logs all activities once you turn it on and it's useful for compliance but also for training. CloudWatch allows you to work with metrics around certain services and ElastiCache is one of the monitored services.

		- Let me go ahead and click that and you can see that you have some standard metrics here. Again, this is great for monitoring when you are running this live inside of your application. Of course, we don't have any traffic coming in and out of it because we just turned it on but this will help you to properly size your cluster in terms of the instances and the number of nodes. So I wanted to call your attention to this companion service in AWS.

		- You'll note also that you can monitor EC2, your virtual machines, EBS, the file store that backs them, the Relational Database Service and S3. So these are all part of this service which is called CloudWatch on Amazon Web Services.

	## Understanding DynamoDB
		- The next AWS NoSQL database software that we're going to talk about is called DynamoDB. DynamoDB took the market by storm when it was introduced many years ago and it still is one of the core NoSQL services. And Amazon, to their credit, keeps adding functionality to make it even more compelling. So how does it work, and what are the important aspects of DynamoDB? It's a key-value store at its core, however Amazon, interestingly, has added support for JSON data which makes it also a document database, so it's a combination of both.

		- And this is a unique offer in the marketplace. I find that a lot of my customers don't actually understand what it does, so one of the tips I would give you in working with it is to read the documentation so you can understand all the capabilities. It uses SSD drives, so it's really fast, it's designed for read/write workloads, and it creates tables with keys, values, and indexes, and you set the throughput, or the IOPS, on the tables. And it's used in situations when you want to have large to huge, scalable lookups.

		- An example is, there was a website used for the royal wedding in England. And as it got closer to the royal wedding, and as there were rumors about what the bride would wear, and so on, so forth, this website experienced very huge, and very unpredictable spikes in traffic. And backing it was DynamoDB, so that when these spikes occurred, they would be seamless and there would be no delay for all of those royal-watchers. So let me show you how this works, and I'm also going to talk about another new capability in the AWS stack around compute, it's called AWS Lambda, and how it can work in combination with DynamoDB.

		- So let's go over the console and take a look at this. So to find DynamoDB, we're on the main console, and we'll go to Database and DynamoDB. And notice it's Predictable and Scalable NoSQL Data Store. And Amazon's approach to the NoSQL problem, if you will, is very interesting. In the world of 150 different NoSQL commercial databases, commercial and open-source, I should say, they basically just keep adding features to Dynamo so that it can be a one-stop shop, so that if you have a workload that NoSQL is a better fit for than relational, for example, you can just use this database and it can work in several different ways.

		- So the core concept is you just, as it says, pick a primary key, set your throughput, and create a table with alarms so that you can be notified if the table needs to scale up or down. So we'll go ahead and run through this and create our table. And we'll call this mycooltable. It says where it'll be created, and then we say the primary key type. Now, again, we're going to go with the defaults, and for testing out, that works just fine, but as with some of the other services that I've shown you throughout this course, when you're moving to production, these choices are really important and you're going to want to read up on Amazon's recommendations in terms of what your source data is, and how to properly set up your primary key, your hash attribute, which does the scaling and the partitioning, and your range attribute.

		- So by default, we're going to just go with a string here, and we're going to call it ID, and then the range attribute we're going to call it location. And here they give you an example, "Customer ID is a good hash key, "while Game ID would be a bad choice "if most of your traffic relates to a few games." And again, you really want to read this when you go to production. So we're going to go ahead and say Continue. And one of the interesting capabilities that Amazon continues to enhance with Dynamo is the ability to add both primary and secondary types of indexes.

		- An index is a data structure that maintains an alternate hash and range key. You can use it to query an item, same way you use the table hash. So you have a global secondary index here, or a local secondary index. And if you're unsure as to what that means, you can always look over here. So global has an alternate hash and range separate from the table and requires its own throughput capacity. So you're paying for it. Local has an alternate range and shares a hash key with the table. So these nuances affect performance and cost, and important as you move from your testing into your production that you investigate these nuances and you set them correctly so that you're only paying for what you need in getting the performance you need.

		- So global secondary index, we're not going to set that for this, but we could if we had additional columns. And then projected attributes, what might this be? Projected attributes are attributes stored in the index and can be returned. It's similar in relational to optional values in a secondary index. So, it's interesting how index and concepts that come over from relational are entering the NoSQL world, and what I find is my DBA background helps me to set these types of solutions up correctly.

		- I often find when I work developers who don't have a relational database background, they just go with the defaults and then they wonder why their queries are slow. So again, tip from the real world, either find a relational database person to help you, or read the docs, or both. So we're not going to add any more indexes. Notice the warning, too. Local secondaries can only be defined at table creation time versus global. We're going to go ahead and click Continue, and then we're going to specify the read and writes. Another tip from the real world, this is one of the services that even when you're working with a new account, you get a lot of free credit on Amazon.

		- You can rack up some pretty huge charges on testing out Dynamo. So if you're going to test it out, either work with your local Amazon reps, or test it out at a level that you know what the cost will be. I personally suffered this one time where I did a big test, and I got unexpected costs 'cause I forgot to turn the service off. So just sharing my own pain so that you can avoid this. And they warn you here, throughput capacity for the table will cost you up to $0.59 per month if you exceeded the free tier. So don't fill in 10,000 like I did and get that big bill.

		- So I'm just going to put something really, ridiculously small just for purposes of demonstration here, and then say Continue. All right, so this is a new capability, Streams. So Streams allows you to set up what are called triggers, when certain activities occur. And this is designed to work with a new type of compute called AWS Lambda. So I'm actually going to turn this on so we can look at it, and the view type is Keys Only, or New Image, Old Image, or New and Old Image.

		- And, again, if you're not sure what this is, what will be written to the stream when items in the table are modified. So it's pretty clear then, Keys only, new image, old image, new and old. So, Dynamo Streams provide a stream of all the changes made to a table in the last 24 hours. You can access the stream with an API call and use it to keep other data stores up to date. So this is used in pipelining kind of scenarios, where Dynamo's sitting on the front and it's very scalable and very fast, but yet, for some other business requirement you need to keep this information in a persistent store; it could be a relational database, could be Hadoop, could be Redshift, could be any other type of service.

		- And then here's the alarms, so notify me when the table's request rate exceeds whatever percent, then send the notification to, and you can put the notification email address here, and then notice additional charges may apply if you exceed the free tier for cloud watch or simple notification. And then we have advanced alarm settings in cloud watch, and we saw that in the previous movie looking at ElastiCache, so you can configure advanced alarms. Why we have so many choices around alarms is because Dynamo is used for workloads that are spiky and inconsistent, so it is a very common situation that you will want to have multiple notifications when the Dynamo capacities are being hit so that you can make sure that your application, as a whole, is responding.

		- Dynamo will respond well, but sometimes there is things you need to do. For example, the web serving compute on the front, you might need to spin out more VMs or something like that. So I'm going to actually just say Continue, I may need to put an email address here. Yeah, so I'm going to turn this off which is not what you normally do. Say Continue. And then it gives me my information. And, again, very unusual not to have any indexes. Just click Create, just for demo. What's interesting about this is the new world of NoSQL. You're not thinking in terms of servers and databases, just literally tables, which is really more application-centric way to think about a database, and reflects the world of NoSQL, which is entering more and more of my customer workloads.

		- Honestly, it's more often something I see with startups, but enterprises are starting to think about it as they're collecting what I call behavioral data. And what that means is data that is in addition to their transactional, or line of business data. The reason for this is it's generally cheaper to store behavioral data, which may or may not give meaning to your business metrics in NoSQL solutions. As opposed to relational solutions, particularly if you're using licensed software, like SQL Server or Oracle.

		- So this can be a cheaper way to store, and I'll give you a very specific example while this is creating, around retail. So what you can now do, and I've had customers that have done this, is you can purchase external data. It's often from the cell phone providers, if you are a brick and mortar store, about what your customers are doing on their phone and where they are when they are near or in your store or in your mall. And you can get that information in aggregate without personally identifying information so that you can understand and create groups of customers, and then provide additional services in your physical store based on what the customers are doing.

		- For example, a customer might be in the shoe department at Nordstrom and might be looking for where to get a cup of coffee. So maybe you can put a sign about the coffee shop in the shoe department which will then keep the customer in your store longer and creates the likelihood they'll buy shoes. So those are the kind of scenarios that behavioral data and NoSQL databases have supported. So this is still creating, so let me go ahead and see if this is there. This is pretty fast, the spin ups, so now it's active. So I can explore the table, of course it's going to be empty, and then I can create an item.

		- So if I want to create an item here, I can insert a string, binary, number, so on and so forth, or I can remove, and if I want to put a value, I can just say one, location would be California, and then I save it. And so now I have my item and I can browse item. Now, normally you don't use this console to enter, this is just for testing. You would connect to this programatically and then we could start a scan, and we have our item here, or we could do a query.

		- This is, really, just a test interface more than anything. Normally this would be integrated as part of an application, and you would work with this programatically with the SDK. Now just to show you some additional information about this. We can go and click here, and we'll go over to CloudWatch. Do you remember this from the previous movie? So here we are, and we are in CloudWatch, and you can see that we have metrics from some of our other services here. We can look across our different services to see our metrics and see how our application is doing.

		- And this really starts to get into full application management, and this idea of partitioning your data workload. So you may have some data in ElastiCache, some in RDS, and some in files in S3, and some data in DynamoDB. So, really, it's the new world of implementing data solutions using the various Amazon services. Now we're not quite done yet, we have, still, one more service to take a look at, and we'll look at that in a subsequent movie.

	## Understanding Kinesis
		- In the previous movie we were talking about the fact that DynamoDB can produce streams that can record information from the incoming data. You might be wondering, "well, is there a service "on Amazon that allows you to consume streams?" And yes, there is. This is called Kinesis. This service is a consumer of, I say here, big data streams but really it's any type of streaming data. So it's used in scenarios that need real-time processing of incoming data.

		- And it creates one or more streams that are then accessed programmatically, generally, although we'll look at the console, just as we've done with the other services. And you can write data to the stream or read data from stream windows at defined streaming points. Now this is an architecture in a workload that I am finding that I'm working with more and more, as I talked about in the previous movie, as companies are asking to incorporate behavioral data which may or may not be relevant to their business case, they are looking at volumes of data and speed of incoming data that is necessitating the use of these new type of products.

		- Before we go over to the console, I want to actually show you an architectural diagram that I think is really helpful in understanding streaming, so we're going to jump to the Amazon docs and show you what that looks like. I really like this diagram because it allows you to see the idea behind Kinesis. So you can see the concept is that you're going to have some producers of streaming data. Typically it is most commonly from mobile clients, to be honest with you. But there are other types of streams, and something that isn't listed here that I'm actually working with is sensor data.

		- For example I just did one with a manufacturing line where these sensors were placed on the manufacturing line devices, and they were sending streaming data in. So it's interesting because in the old world of data, data was very static and it got sent at the close of a transaction, or even at the end of a time period, a day or an hour or whatever. And then it was usually processed overnight, or sometimes for a week or even a month, and aggregated and cleaned, and then the results were returned.

		- The new world of big data solutions is, what is data itself, is defined differently, rather than just the end result or the transaction, data is considered to be every activity. And let me make a very specific example around mobile gaming. It's very common in mobile gaming to collect every activity that a user does while they're in-game. So this could be even activities that they don't complete, even touches or the beginning of an entry into a text box that is then erased, because that data is considered to be relevant and used in making the game more playable, for example, more fun, finding out where people drop off, where they purchase, so on and so forth.

		- So the volume of data is increasing and the velocity coming in, or the speed and complexity, because of these kind of scenarios, and the data itself is called "event data", rather than "transaction data". So every single event generates a row that gets entered into the stream. So the way that AWS supports these kind of scenarios is with their Kinesis streaming technology, and you'll see when we go to the console that you create streams and then you work with the streams. Now you will most commonly do this programmatically through the API.

		- You'll hook your application to the streams. The other thing that I wanted to show here is very commonly as it's shown in the architecture, you'll then have consumers which will be virtual machines, EC2 instances, or actually, in the newer architecture, something smaller called a "container", or an application virtualization or "docker" that is more lightweight than a full VM. That's actually also supported. The processing is not the focus of this course, it's the data. And then once the processing is done by the middle layer there, shown as the consumers, then note the destinations are various Amazon persistence stores.

		- So everything from files, S3, to DynamoDB, very fast, SSD-based semi-structured database, to other services we haven't yet covered which allow for large or huge workloads Redshift and MapReduce. So it's interesting to see this flow. The other thing I wanted to point out is DynamoDB within the implement of streams, actually you could have an arrow going back around from DynamoDB into Kinesis, because one of the architectures that's been enabled now with DynamoDB streams is the ability to capture a stream of activity coming into Dynamo, send it to Kinesis, and then send it somewhere else for processing.

		- So it really starts to become about the data pipeline and the aggregation of the services when you add something like Kinesis. So let's see from a practical point how you actually do that. So we'll go over to the console, and you'll notice that Kinesis is not under Database. It's under Analytics. So real-time processing of streaming big data. And as is always, I recommend you read this when you're first starting. We're going to click Create Stream.

		- "mycoolstream". And we can have a shard calculator to estimate the number of shards for the stream, or we can specify. Let's actually look at that. So the shard calculator is going to tell you the volume of data, the average record size. The record size can be an integer here between 1 and 1024. So we'll just make it 10. And the maximum records written per second, we're going to do something ridiculously small again just so that we don't incur a bunch of charges. Notice if we put 10, it says that we should use one shard.

		- The default shard limit for the account is 50. So just to show you what this looks like, if I put 100, or 1000, then we get up to 10 shards. If I put 10,000, we're above the limit. So just to show you how that works. I'm just going to put 10. And then the number of shards. We're just going to say, let's put two so that we can see what it looks like. And then here is the values calculated. We're going to have 4 MB of Read, 2 MB of Write, transactions per second for Read are 10, and Write are 2000.

		- This is really tiny and you wouldn't use this for anything other than just looking at it, not even a POC, but just to give you an idea of how this service looks in the console. And then click Create. And here we're creating the stream. Very simple console here because, as I said you normally are going to interact with this programmatically. And really you can just work with the stream create and delete. Just for fun, while this is creating, let's go and look at our cloud trail and see what activity's being generated.

		- There's our Create Table from the previous movie, which was about DynamoDB. So it looks like we've got a little bit of lag here. Let's go back. Let's go to Kinesis. And our stream is active. And we just have some metrics that we can look at in here. Again, this is kind of a view-only console. You don't really do much other than look at the current overhead on your stream. So you have number of open shards and you can close shards.

		- You can delete the stream, and this just shows you over time the read/write latency, so on and so forth. So you can see if you have set up enough infrastructure to stream the amount of data that you're looking to work with. So Amazon Kinesis is a set of tools and utilities that allow you to do real-time processing of streaming big data.

	## Exploring Document and Graph DBs
		- Now in addition to the NoSQL databases that we looked at that are core Amazon services, I will sometimes get requests from my customers to use their preferred NoSQL database, which would be created by a NoSQL database vendor. So for example, like a MongoDB or an Aerospike. So you have a couple of options if you want to do that using the Amazon infrastructure. Of course you can always create an EC2 virtual machine and just manage it yourself and install the NoSQL database of your choice.

		- And some customers I work with really prefer to do that. That's usually when they have a developer or a DBA talent in-house who knows that particular database. Another one I've seen done this way is Cassandra, which is a wide column database. You can also use an Amazon machine image, which is a pre-built, pre-optimized EC2 image done usually by both the vendor and Amazon to make sure that the instance is optimized. And one that I've worked with this way is Aerospike, which is a key value in memory, very very fast database often used with ad serving.

		- There are also some offers in the Amazon Marketplace and what this is is a website that has a number of different prebuilt solutions that are quick and easy to setup, and I'll show you this in just a second. In addition to this there are 3rd party vendors such as MongoLabs and Redis Labs that have their managed set of solutions that run on top of either the Amazon or other clouds. And I'll also show you this, because this really comes into the set of choices around NoSQL databases.

		- It really comes down to do you want to use a very managed solution, usually DynamoDB on Amazon, or do you want to use a database that you're familiar with and run it yourself, like a Cassandra or a Mongo, or do you want to work with the 3rd party vendor who manages let's say a Redis or a set of Redis instances in a cluster for you on top of Amazon? So there's a number of different choices and really it depends on what your preference is, pricing, what your needs are, you know, the various decision factors.

		- It's a lot of different considerations and choices. Good! So in the console, notice you do not see the words NoSQL other than under "Dynamo." So that's Amazon's core offering and that works for a lot of my customers. However, in addition to that you can go over to the AWS Marketplace as one option. So let's go over there. And inside of here if I just type "redis" for example, you can see that we have a number of choices for hosted Redis.

		- And in fact we also have competitors. That's kind of interesting. So if I wanted to work with Redis To Go, I would just click here, and then I could go over to the site and work with it, or in some cases, and this is one I just actually happen to work with, "aerospike". If I go to the Aerospike instance here and I take a look at it, I can click "Continue" and I actually have the ability to do a 1-Click Launch. And what that will do is that will launch an EC2 instance of the size that I select, and here's the selection criteria the region, the version, the VPC settings.

		- Something I will point, because this is again real world, when you do this you really want to associate a key pair or you won't be able to log into it. So you want to fill this out and make sure you have a key pair and then associate it to a key pair. And in this case it runs on Linux, so you'd be SSH-ing in. And this will tell you how much it would cost based on your selection. And then to launch it you would just accept terms and click with 1-Click Launch or you could do the Manual Launch. So this can get you up and going. This is particularly helpful for us architects.

		- When you have some customer requirements and rather than manually installing I will always check in the marketplace to see if there is an image. Let me go over to the console and let me create a key pair. And I'll call this "aerospikemarketplace" so I remember what it is, click "Create." And now I will go back over here. That downloaded my private key. And I would be here and there's my associated key pair.

		- And then I will go ahead and launch and it will be deployed on EC2, and then I'll get an email when it's ready, and shows me my information, and now I can go back over here, back over here, go to EC2. Notice I have the private key download, which I would need. And I would go into my running instance. It actually takes a couple of minutes to show up in here, so I probably won't wait, but it would show up inside here. This is the default instance, and it would be called Aerospike.

		- And then I would say "Connect," and then what I would do is SSH in and then I would have full admin control. So that kind of shows you an alternative method to working with NoSQL databases on the Amazon Cloud, which obviously you could just spin up an EC2 instance and install something, but using the marketplace is a time saver, and I just wanted to include it in my discussion of NoSQL solutions available on the Amazon Cloud.

	## Updates to Compute/Containers and NoSQL
		- As we continue learning about NoSQL services and architectures on the Adobe US Cloud, there are a couple of changes to AWS Compute Services that are impacting database implementations in general and NoSQL implementations in particular. This is around changes to virtualization. As I showed in a previous movie, you can just spin-up your NoSQL database of choice on an EC2 virtual machine. If you need more than one instance, you can just spin-up more virtual machines. But there is a sea of change in the virtualization world that's impacting several different aspects of Compute and in particular, NoSQL architectures.

		- This is called application virtualization and it's done via containers, Linux containers specifically, as of this recording. And it's being led by a group called Docker. There is also some changes because of the AWS Lambda architecture. Let's first take a little closer look at Docker and see how this impacts NoSQL implementations. If you're unfamiliar, I'm at www.docker.com/whatisdocker and there's a couple of good diagrams and pieces of explanation.

		- In a nutshell, it's application virtualization. This is a pretty good diagram but the one that I really like is further down the page which shows very succinctly why it's so important to consider these architectures in enterprise and a startup solutions. The traditional virtualization requires that each time you scale out or add a new instance, in our case, of a NoSQL database, that would be the App here, you would have to reinstall a Guest Operating System on top of the Hypervisor and the Host Operating System.

		- The Docker architecture is much more lightweight and it's very well-represented by this drawing. The Hypervisor is replaced by the Docker Engine and there's no need to reinstall the Guest OSs. If you'd be thinking that this lightweight architecture is not only faster but also cheaper, you'd be right. Because of this, it is being adopted on various cloud architectures incredibly rapidly. Just to underscore that, I'll actually go to the Docker documentation and show you that if I go down to the section here where it says "Applied Docker," you can see that all the applications, with the exception of one which is Node, are implementations of NoSQL databases or caching.

		- Since we looked at Redis, I thought it would be interesting to just look through the Dockerizing a Redis service example. You might remember when we used ElastiCache in AWS, what we did is we setup a number of Redis clusters. This is that same process using the Docker service rather than using traditional virtualization. Again, if you go down you can see we've got the Dockerfile and then we're going to build the Dockerfile. We're going to create a container which is basically just a description of our application called redis.

		- Then the key command is docker run and then redis which runs that command. Then we create a container, we install redis, we have some environmental variables and that's it. Not only is it lightweight in terms of overheads, it's also lightweight in terms of scripting. Just to show you one other thing that's interesting, if I go to Docker Hub which is a public registry for Docker containers, notice the first one listed is redis and when I click on it, it has 5,900,000 downloads as of the time of this recording.

		- This is really upending the way NoSQL implementations are scaled on the cloud. Your question might be, well, how does Amazon support this? And does it support it? The answer is yes it does. In the Amazon console, under Compute, we have some new services and addition to EC2. We have the EC2 Container Service which allows you to deploy, manage and scale Docker containers. One architecture that I encourage you to take a look at when you're moving from test to production is rather than traditional virtualization to scale, at least evaluate the use of Docker containers or sets of managed Docker containers, since this is now supported in the Amazon Cloud.

		- Now, just to be complete, I did mention another Compute change that is very significant. We talked about this in the movie about DynamoDB, in terms of setting up triggers on DynamoDB, and that's Lambda. Lambda is a very unique service that I find myself investigating for more and more of my big data customer pipelines. It's a very efficient type of Compute. EC2 is the chunkiest, if you will, because you've got a virtual machine per instance. Containers are more lightweight, you have the Docker implementation and then you have the application run over and over and over, not the operating system.

		- Lambda is the most lightweight. So what is this? If we click on it, basically, it is a Compute service that runs developers' code in response to events and automatically manages the compute resources. Literally, what it is is it's a call, a method call. Every time that you run that call, you get charged for that call. It is much more lightweight and significantly cheaper to run in conjunction with these data pipelines than traditional virtualization.

		- And just for completeness, I'll actually just spin-up a really quick example. We just create a function and here we have blueprints for various functions. Notice, one of the first ones that we have is DynamoDB trigger that logs the update made to a table. We'll click on that one. You can see here is the Event source and the DynamoDB table that we used earlier. Here's the Batch size and the Starting position. This is the position in the stream where the Amazon Lambda should start reading.

		- We can have Trim horizon, Latest, those are our two options. Then we'll click Next. We're going to call this mycoolfunction. This is an Amazon DynamoDB trigger that logs updates made to the table, it's running in Node. Here's our sample code. It's very simple code, we can see that we have our function, the event and the context. Then we just forEach and then we log to the console and then we show the successful logging.

		- We have to associate a role, this is the permissions, so * DynamoDB events stream role, and we'll click Allow. What that did is that created a permission set for this code to run. Then we can set the Memory and the Timeout. Let's click Next. We can enable this now, and it warns you "You might want to test this "before you do that." Then let's click Create function.

		- You can see that we have this Lambda function. This is the actual Amazon Resource Name which is helpful when you're accessing these kind of resources and really any resource in the Amazon Cloud programmatically. You can see here is my Lambda. I can go ahead and click Test and I can do a DynamoDB Update. You can see that here's my output and here's my log. So in some ways, what's old is new again. This is a kind of a transaction log that we know for years and years in relational data but we've got a totally different kind of database here, we have a NoSQL database and we're using a streaming capability.

		- You can see inside of here, we can work with the function code, we can delete it, we can configure a sample event. Here's our Code, here's our Configuration, our Event source, our API endpoints, and here's our Monitoring, you can see we have one invocation. It's interesting in this new world of database choices that we not only have NoSQL databases to work with, we have new implementing architectures. We can go beyond traditional virtualization and we can look at containers and we can look at Lambdas, and it gives us a lot of flexibility.

		- In some ways it's a little bit overwhelming. The process that I use, as I mentioned in the start of this course, is I use a lean startup or a minimum viable process where I will work with one small set of data and try out different technologies to see which seems like it's going to be the best fit both technically and also from a knowledge in my team standpoint. But we really do have a lot of choices in the NoSQL world and the Compute choices are intertwined with the database choices.

	## Using MongoLab
		
		- Another option that I've used in setting up NoSQL solutions on the Amazon Cloud is a partner, and this partner is MongoLab. So, they offer MongoDB-managed. It's very easy to use, and they have sophisticated synchronization and backup. Of course, you're paying for the fact that they are doing this management for you. So, I use this in situations with startups, where there's no DBA resources on their team. It's all developers, usually just a couple of people, and they really want the database to be transparent.

		- Like they just want to program against it and not have to think about it, have somebody else back it up, so on and so forth. So, now I'm going to navigate to the MongoLab website. To work with MongoLab, you would need to create an account, which is free, and sign up. And that's what I've done already. So, I'm on the Homepage here, at mongolab.com and you can see that I have no Deployments and I could Create one from a backup, Clone existing or Create a new one. So, I'm going to click, Create new. I can Create a new subscription and notice I can select my Cloud provider.

		- Of course, we're going to talk about Amazon for the purposes of this presentation, but notice, they also support deploying on Google Cloud or Azure. And within Amazon, you want to then select your Region. And I'll select US West, 'cause that's where I am recording today. And then I'll scroll down and I can see that I have a Replica set cluster and this is how much it costs. And I have a Standard Line of Storage. And I have a High Line of Storage, and I have a High Performance Line. This is my version of MongoDB, that's very important.

		- And then I have Security, SSL. And then an Initial database name. Now, if I want to Create this new deployment, this is a little bit bigger than what I want to use for testing, so I'm going to go ahead and click on, Plans + Features, and I'm going to use the Sandbox version, which is great for just getting your feet wet here. Like I said, I normally would set my developer team up with production deployment, so I started there, but to just try it out, you probably don't want to pay 720 bucks a month.

		- So, we're going to click on the Sandbox and notice it's really tiny and you don't get all the features. But you can see how it works, so we'll click Free, and we'll click, Free. And here we are on a Single node. And Sandbox is free, same choices again. I'll just go with the default, East, it doesn't really matter 'cause it's just going to be quick. Then I'll say, mycoolmongo and then I'll Create a deployment. Now, what's happening here is technically, you've got a set of EC2 images that are being deployed on Amazon, but you don't manage them through the Amazon console.

		- You manage them through this interface and other tools that MongoLab provides. So, once you have this, then you can go ahead and click into it. And here it shows you how to connect. If you want to use the Mongo shell, or you can use a URI, if you have some kind of GUI tool or something. So, you first need a database user so again, it's very guided, so another reason I'll use this is if I have some people who have a relational background and they want to try out Mongo without installing anything 'cause this is so easy to use and it just takes you through the process of getting up and running with Mongo, takes literally like five minutes.

		- So, I'll call this, coolmongouser. And I'll put in my password. Notice I can set this for read-only if I want to just store my behavioural data and do reports off of it. And there I have my User. And then, inside of Mongo, you have the concept of Collections. So, if you want to Add a collection. I'm not going to do the advanced options, we'll just get started with the basic thing here. Click Create.

		- And we have one System Collection and then, inside of our Collection, we are going to Add a document. So, if we wanted to add a document. Now, the default format here is JSON, so I'm going to just be really, really lazy and copy this. Actually, this is not exactly correct because we'll see that we'll get an error when we paste this in. So, we'll say, Create and go back. So, we want to use double quotes around field names and values.

		- Again, what's really great is this is such a friendly interface, so even if you've never seen Mongo before you can get up and going super fast. So, Create. So now we have our one record and notice, it added a unique identifier here, which is the default behavior of Mongo and something that you really want to make sure you understand if you're using this in production. A little bit beyond the scope of what I'm showing you here. And then you have Indexes. So, by default, you have an Indexed Field on the ID and if you want to Add an index, it shows you how to do that.

		- And again, very, very good documentation here. And then we have Statistics. So, it talks about your different metrics. And Tools, so, you have Import/Export Helper. Tells you how to import the different kinds of files that are supported. And then you go back to Home and you can work with your MongoDB instance. Again, literally, it takes five minutes to get set up here and if you've never worked with a NoSQL Database, this alone, is kind of worth it. But, in addition, from a more I guess, useful use-case, I will often get developer teams, who are new to NoSQL, set up on Mongo, through MongoLab, because they can just focus on putting the data in and querying it out and not really worry about any of the management aspects.

		- However, to contrast, if you have knowledge in MongoDB infrastructure, you may want to set up on EC2 or some other method. But, this is just one of many solution providers. I've done this also with Redis. RedisLab has a similar kind of implementation. Just when you're working with NoSQL, you want to go through this range of processes when you're deciding how to implement on the Amazon Cloud. You don't just have to install it on EC2, or you don't just have to pick an Amazon Service. You have really a lot of choice around NoSQL.

	## Summarizing NoSQL and Prices
	- So we covered a lot in this NoSQL section. Let's summarize. So I use most often DynamoDB because it's native to Amazon and it supports not only Key-value, but now JSON data, runs on SSDs, it has integration with so many of the other services like CloudWatch and the integration now with Kinesis for logging as I demonstrated with Lambdas in a previous movie is really powerful. But that's not right for every customer, so, of course I should mention Kinesis. Kinesis is something that I'm using more and more, particularly with IoT scenarios, with streaming, in any type of scenario, and it integrates with, not only NoSQL, but Relational Databases, and some of the Hadoop ecosystem offers as well.

	- I sometimes will use EC2 with core databases. I'll have customers that'll want to use Cassandra, or Neo4j, or they'll have some database that they're comfortable with, and I'll either use a plain vanilla EC2, or I'll use something from the marketplace, or something that's managed as I showed in the, "Partner highlight," video. Of course, caching is something that more and more applications use and we can use ElastiCache, which offers Memcache, or Redis, but there are alternatives as I showed when I talked about the partner marketplace and I showed Aerospike which is a very, very, high volume database that's often used as a front-end caching in workloads, particularly those that are spiky and very high volume, such as ad serving.

	- Now as I did in previous sections, we're going to talk a little bit about pricing, so I'm going to go over to the pricing calculator. So here we are in the monthly pricing calculator, and we have some of the NoSQL services available to price calculate on, not all of them. I'll just pick a couple, just so that we can get a sense of this. So we worked with DynamoDB quite a lot, so DynamoDB, first they talk about the free tier, and then we're going to talk about the Get Dataset size. So let's talk about a terabyte, as we've been comparing, as we've gone across different services, and this is 268 a month, but with Dynamo, of course, transfer in, transfer out, is going to be a big part of this, so let's put in some more values here.

	- See how that goes up. There we go. And then let's put some Read Requests. How are you going to get these values? As we talked about, you're going to load test. Load testing on big data solutions is done for price estimation, not so much for performance. This is something that is new thing for most of my customers to think about, but you don't want to get surprised when you go to production, so you do actual price forecasting with your application. Of course there's some other considerations here, but you can see how the pricing is really very, very reasonable.

	- Now let's take it up a zero and see what happens. Does it go exponentially? It does, so that's kind of useful. That's, again, another pricing tip from the real world. Does the price scale linearly, or how does the price scale? Now let's go ahead and clear this out, and look at ElastiCache. This, again, will be a decision factor. Let's call this, "test," and let's make this three nodes like we had in our demo, and we'll say we're 100% utilized. We don't want to use Micro instances.

	- Let's use m3 mediums, those are memory-optimized, and you can see this is 200 bucks a month, and then we can have a, "Reserved." Do you remember from our discussion of RDS, or Relational Database, the difference between On-Demand and reserve pricing? Let's try it out here. So we're going to say, "testreserved," and let's say three nodes. So we had 200 bucks before we added, and we're going to have 100% utilized, and we're going to use the same cache.m3.

	- So we added about 150 bucks, so we didn't have a huge price differentiation here. We saved 25%, and we're doing a three year term. Of course, we most commonly recommend a one year term, and now let's take this off for a true comparison, and see this is 65 bucks. So you might remember using this On-Demand was 200 bucks. That's a significant price difference, so once again, lesson from the real world, I'll tell you that you want to purchase the services in the way that you're going to use them to get the best value.

	- Now we've got other services on here that you could work through, but I just wanted to pick the core ones, Dynamo and ElastiCache, but we have some of the other cloud services here listed, and you could go ahead and price those as well. Very important to do price estimations, and then validate that with load testing, particularly with NoSQL, because for most of my customers, this is a newer paradigm, and they don't really have a sense of the throughput, and some of the other aspects of working with the NoSQL services until they test it out.

# Data Warehouse and Pipelines

	## Exploring AWS Data Warehouse options
		
		- In this next section of the course, we're again going to consider our different workloads, Small or Medium, Large or Huge, and Complexity. Complexity will be defined on type of operations for this particular section. So in this case, we're going to look at AWS Data Service Choices and partners around Relational Database Systems that are designed for OLAP, or Online Analytical Processing or Reporting or Read workloads. Not Online Transactional Processing or read/write or operational workloads.

		- And AWS has a set of services that serve well Medium to Large and even extra large size workloads. The core product offering here is something called AWS Redshift. Redshift is a cheap, scalable, highly performant data warehouse. It's designed with an underlying data store that's based on an implementation of postgres. It's a wide-column store. So you can think of it as the best of both relational in terms of postgres which means that you can use ANSI style SQL to query the information stored in your data warehouse which is very very important and can really impact your ability to quickly query and get meaningful information out of your data.

		- And a wide-column store, which is a no SQL type of a database which allows scalability in a much more cost-effective manner. And as I mentioned, it can be queried with an ANSI style of SQL, it's not exactly ANSI, but ANSI-style. So you can leverage the knowledge of your existing DBA teams in working with this product. And, the thing that's most stunning to me is if you buy in a one year allotment, it's $1000 a terabyte a year. I have had so much success with this product with my customers, I'm really excited to talk to you about it and one of the things I'm just going to be honest about is I'm often called for big data architect consults and the inclination is to look at the hadoop ecosystem or some other type of no SQL solution.

		- And, particularly when working with the Amazon cloud, I would say over 50% of the time, we take those scenarios and we use Redshift and we get really good success. I've actually also had situations where some of those other database technologies were attempted and didn't succeed and the customer was really in kind of a state of panic trying to figure out what to do with their large volume of data that they really didn't want to put in a regular relational store and they wanted to optimize it for read-only. So, very excited about the product offerings here and I'm going to show you the mechanics of how to work with it and then also talk about the partner ecosystem which is a huge part of the usability and the success of Redshift.

		- So, this partner ecosystem consists of tens, maybe even hundreds of partners and is really indicative of the strength of the product of all of the data solutions that I'm talking about in this entire course for Amazon. Redshift itself has its strongest partner ecosystem. It's kind of stunning how many partners have built on top of Redshift. So one of the real world tips that I would give to you is when you're thinking of implementing Redshift, take some time to investigate the partner solutions, figure out which, if any, are appropriate for augmenting your solution before you go and build something on your own.

		- Now, let's first just look at the mechanics of working with Redshift through the console. Because this is most often used, actually relatively quickly into production, you will commonly not use the console after you do your POC you'll use either the command line tools or programmatically work with it so you can script the implementation. Or you'll make it part of a pipeline which I'll be showing in a subsequent movie. Which is another service that's available in the AWS suite of services. But anyway, let's just start with the basics before we get into the complexity.

		- So, Redshift is Managed Petabye-Scale Data Warehouse Service. So, what you create here is a cluster. And hopefully by now, if you've been watching this course from the beginning will see a familiarity to working with the console which is not to be understated. You know, simple is hard and simple is important in terms of usability and I have to really congratulate Amazon as I've worked through the various data services, clearly there's been some attention paid to usability in the console which, frankly is not as well-implemented in some of the other vendors and it affects usability.

		- So anyway, so I'm going to click on Launch Cluster and I'm going to give a Cluster Identifier. I'm going to call this mycoolcluster. And a Database Name, mycooldatabase. And here's our Port number. And the Master User Name, my super secure name. And then I'll use my common password here. All the worst practices, right? But just for setting up a test, this is good enough. So once you do that, you click continue. And now I can select the Node Type. So I have the option from Large to Extra Large and this is going to impact my pricing so I'm going to use the smallest one for this demo.

		- And, then I can use a Single Node but I'm going to start with Multi-Node. And just put in two to start and click Continue. And then I can specify whether or not I want to have Database Encryption. Here's my Networking Options, my VPC, Subnet, if it's Publicly Accessible or not, and if I wanted to have a Public IP Address, I would have to create one and I can either use an Elastic IP, or, if I had configured a static IP address, I could set it for that.

		- Elastic will be good enough for our demo. For the demo we will just select the No Preference for Availability Zone for Production Implementation you would obviously care about that. And then here's our Security Group and we can specify CloudWatch Alarms. Do you remember CloudWatch from previous videos? This will specify if we hit a certain threshold and give us an alarm. And nice again to see how the Amazon products are so well integrated. I'm going to go ahead and click Continue. I'm actually going to change the Public IP back to No since we didn't have one set up.

		- Click Continue. And here it gives us a big warning that we are eligible for the free trial but it's very quick and easy to accrue charges here so just a tip from the real world, make sure and turn this off as soon as your done testing it out so you don't get surprised by a testing bill. So we're going to go ahead and click Launch Cluster. And we're going to view it on the dashboard. And if you're thinking that this looks similar to the other data services that we've spun up, you'd be correct. So, it's going to create the cluster and while it's creating, I'm going to refer to another aspect of working with Redshift that I found to be very useful.

		- That would be the documentation. Because it's so easy to set this up, what I have found is actually people set it up incorrectly. So, as with some of the other Amazon services, if you just take a short amount of time and read the documentation, you're going to get a much better result. You'll notice that there's three core topics, and I recommend that you read all of them. Best Practices for Designing Tables, Best Practices for Loading Data, and for Designing Queries. Although this is based on a SQL data store, it is optimized for massively parallel processing and under the hood it is a column store it is not a relational database.

		- So if you just directly load your relational data, you will not get the query performance that you are expecting. From the real world, I've had this situation happen several times where I've had customers load data directly without reading any of these guidelines here and without considering any of the best practices and then be not satisified with the perforrmance that they're receiving. So, just take the time to do that and you're going to get a much better result. You'll notice I'm in the docs here and there's tutorials as well that are really really useful when you're working with Redshift.

		- So I'm going to click Refresh and as this is creating, I will point out as with the other database services, you can take Snapshots, set Security, set the Parameter Groups, work with Reserved Nodes. And, do you remember the difference between on demand and reserved? It's basically how much you pay for it. Reserved is bought in time increments and is cheaper than buying it on demand, And then here's your Events and Connect Client. So, I'll pause this video at this point to let this cluster finish creating and then we'll continue on and take a look at what we built.

	## Working With RedShift

		- Here we are inside of the AWS console and we see our Redshift cluster is now available and healthy so let's click into it and see what we can do with it. So the first thing that we'll notice is that we have No Inbound Permissions. This is our, hopefully by now, familiar endpoint that we would use in our client to connect. But in this case, we didn't set up a security group that had permissions so no client can connect. Again, this is a great usability feature of the console I've worked with other vendors' software and foolishly forgot to open up a port and wondered why I couldn't connect.

		- I just really like this, it's just very usable. So here it tells us exactly what we need to do. "The VPC security group does not have inbound permissions "to allow connections. "You can't connect until I edit the Security Group." So, there tells us the information and we need to edit this Security Group. And this takes us to that location and then we can edit the rules and we can Add a Rule. Now if you're not remembering what port we need to add we can go back and it's 5439.

		- And we're just going to open it to the world, which again is not best practice but it allows us to connect with our client. And now if we click Refresh, we can see that now we can connect to this. But the question is, how would we connect? Now before we connect, let's just look at what metadata and what activities we can do through the console. So inside of here we can actually Modify our cluster. We can Resize it, Shut it Down or Reboot it. We can Configure Audit Logging on our database.

		- We can take a Snapshot or Configure Cross Region Snapshots. So, quite a lot of activities just inside of here. If we go over to Status, we can see the status of our cluster. And if we had CloudWatch alarms configured, we could see if there were any performance thresholds that had been hit and those would show up as Alarms Triggered. On the Performance tab, we can see Query activity, of course we don't have any, because we just set this up, CPU Utilization, Network Throughput, Write IOPS, Write Throughput, Write Latency, and a number of other metrics.

		- These are around Read. Database Connections, Health Status, so on and so forth. Of course, any of this information can be dumped out to logs or can be associated to notifications as well. In Queries, what we'd be able to see is Query History and this would be Load History. Of course, we haven't done either of these activities yet. Let's go back to Configuration and let's Modify our Cluster. So you can see by Default we have a Snapshot retention period of one day. Let's set that to two days.

		- And click Modify. So we see that this console is primarily for snapshotting, for looking at performance. So we do want to connect to this and conveniently, we have a link over to the side, here which we'll be exploring in our next movie when we'll be connecting to our Redshift Cluster.
		- Here we are in the Amazon console in our Redshift cluster and we're on the Connect Client section. So we can use JDBC or ODBC in the driver specific to Redshift. So we can use a Partner Tool or we can use SQL WorkbenchJ. So what we're going to do is install SQL WorkbenchJ after we install our drivers. So the first thing that we're going to do is we're going to select our driver and download it. And then we're going to select our cluster and you can see there is our cluster connection string, our information.

		- So now I'm going to go ahead and install SQL WorkbenchJ and I found that this link is a little bit simpler so let's use this link; www.sql-workbench.net. And then I would click on build 117. I'm working with a Mac so you're going to want to do the install for your particular operating system. Now I recommend that you follow the instructions at this location. So you can see in the instructions here the first thing that you need to do is downloading and starting the workbench which we did.

		- And now in the workbench we need to open the Connect window, create a new connection profile and manage our drivers. We're doing this to associate the JDBC driver that is specific to Redshift to our connection. And this is a tip from the real world; I have found that if you try to use a generic JDBC driver it won't work properly. So it's important you follow these instructions exactly. So in the SQL Workbench I'm going to create a new profile. I'm going to call it Demo.

		- And then Im going to click Manage Drivers. I'm going to add a new one. I'm going to call it AWSRedshift and this is going to allow us to associate to the downloaded driver. And I'm going to click Navigate and I'm going to click on the JDBC Redshift driver and click Choose. Now I'm going to go back to the documentation and notice step 7 we need to copy the JDBC url from the console. So I'll go over to the console, copy this into the sample url text box, click Okay, go back to the documentation.

		- We want to type the name of the master user and the password and select Autocommit and Save. We want to select for the driver AWSRedshift and that will populate the url. Then click Save and click Okay. Now we're connected to our cluster using the SQL Workbench. So the first thing I'm going to do is create the table structures.

		- And I'll execute this and this creates my tables. So now following along in the tutorial I had to do a little bit of preparation. So I'm going to use the sample data that's stored in Amazon S3. So what you need to do is you need to figure out which location your cluster is hosted and how to do that is to click on Redshift and you'll notice it's in Oregon. So what you'll do is you'll replace the bucket name with the location of your cluster and that's going to load the sample data.

		- And then you need to replace the key id and the secret key with values from your encryption key, and I'll show you where to get that. So it's out here and under Security Credentials. And then if you've used your key you can generate a new one and just copy and paste those values in. And now I'm going to load the data with my script. And now that my script is complete I'm going to add a new tab and I'm going to run some queries.

		- You'll notice that I start with a simple select query then an aggregate, then a sub-select that has group by and then a complicated nested select. Here's my results. First result, second result, third result and fourth result. Now I'm going to navigate back to the Amazon console. Click on Redshift, click on my cluster, click on to my cluster, go over to the Queries tab and you'll see here's my queries.

		- You can see the run time. If I then click on the query because we just ran this there's a little bit of latency but what you'll be able to see is the SQL that was generated, the query plan execution and the resource overhead. This is fantastic functionality to tune your queries and to setup your cluster appropriately.

	## Using Snowflake
		- So in this next section, I'm going to highlight a partner of Amazon, and that's Snowflake DB. Now even though I would say they're a partner, they're also a competitor. They have an OLAP or online analytical processing data warehouse offering that is really a competitor to Redshift. It runs on the Amazon Cloud, but it's Software as a Service, so you're buying it from Snowflake. They're reselling Amazon Services, if you will. It uses relational SQL-like querries, and has native support for JSON data. So it's quite a strong offering, and I'm going to pop over to their website and show you little bit about their architecture.

		- The reason I'm highlighting this is as a software architect, I'm often asked who are the competitors to Redshift, and snowflake is relatively new on the scene as of the time of this recording, and it's really been taking the market by storm because although Redshift is a very strong product, almost any product would benefit from competition. So the idea is the data warehouse as a service for the cloud and it is a collection of services that has database storage, virtual warehouse processing and cloud services.

		- And you can see, this is an accurate architecture diagram. You connect with JDBC, so similar architecturally to what we saw with Redshift. We have cloud services around optimization, management, security and metadata. We have virtual warehouse processing for different workloads, marketing, finance and loading, and we have database storage. Now in order to access this, you would have to contact Snowflake and then they would set you up with a trial. So the reason I'm talking about this offering is because it's great to try out alternatives for workloads as mission-critical as data warehouses.

		- Now in this next section, I'm going to show you what their online management tool looks like. In order to get access to a trial edition, you would have to contact Snowflake through their website and then they would give you access based on your workload. So the concepts that you can work with in this interface are databases, warehouses, worksheets, query histories and accounts. So in terms of databases, I have some samples loaded up here. And if I click Create, you can see I just have the name and a comment. Very simple implementation.

		- And they show you the SQL behind this. Now it's really interesting, the CEO of this company for years and years worked previously at Microsoft and worked with SQL Server and you can see the ease of use and simplicity in this product. It's really quite elegant. So you create your database, and then I'll just open one of my databases so you can look at it here. And you can see that I have, in this case, a 16 gigabyte line item table. So this is a sort of classic data warehouse in that you have your fact table, or the center of your star in a star schema kind of a format, and then you have your dimensions or your aspect tables, which allow you to have meaning around your associated tables.

		- So I'm going to click on line item, and you can see that this is my information from that table, and I can create views. I will have schemas associated, stages, and you can see that the staging area is in S3 in AWS. File formats and sequences. So let me click back on database, and the objects that I can do with a database are clone, drop or transfer ownership. And then I'll move into warehouses and inside of here, I can create a warehouse out of the database.

		- So we can think of this mapping to a traditional OLTP or operational database into an online analytical database, but the difference is this is all happening at cloud scale. So in here, you can first click Create and notice, you say the name. And I'll just put a name in here. And then we have Standard or Enterprise, different features, and then we have different sizes. And this goes directly to the size comparison that I've been talking about through this entire course.

		- So I think the Snowflake team and I seem to look at data workloads in a similar fashion. So they have Small, 500GB to 1TB; X-Small, under 500GB; Medium, 1TB to 3TB, Large, up to 6TB, and X-Large. So we start with Small. And interestingly, taking use of cloud capabilities Auto Suspend. So this capability would suspend the warehouse and it would save compute time, and of course subsequent charges to you.

		- And you can specify the interval for suspending, or say never. And then of course you get the SQL that's associated. Very straightforward interface. So I do have some data warehouses suspended, so I can resume or transfer ownership. So let me go ahead and resume. And what this will do this will take the snapshotted copy and this'll spin it up on this particular instance size. So what we basically have here is we have services running on top of Amazon core services that provide data warehouse services.

		- And so now this is started. And then once this starts, then I could go ahead and I could work with this in terms of queries. So I would want to select this. And then I would want to select the database and then schema, if I had a particular schema, and then I would want to work with the sales table. So I might not want to do an all. Let me just go back to look at my schema here. And sample warehouse, go back into my database and look at my tables.

		- And something that's not very big. Maybe the product table. Yeah, and so I don't have the proper object here. So in terms of queries, I have something in the query history, so let me go ahead and load this into the editor. So I'm going to copy this query, and I had a failed query earlier because I had the incorrect schema. So I'll go ahead and put something that will work in here, and I'll click Execute. And you can see, here's my result. And inside of here, I have my duration and my bytes scanned.

		- So this is a relatively new product up on the market, and it is something that I know I'm going to be looking at as an architect, and I wanted to just share with you. One other point before I leave Snowflake, notice that you can resize the warehouse really just very, very simply, and this is analogous to a resize in Redshift, where you can resize by adding nodes. In this case, you add your size. But it's a similar type of a concept if your warehouse needs to be sized up or down.

		- So this is a preview of Snowflake DB, which runs on the Amazon Cloud and is a competitor to Amazon RedShift.

	## Summarizing data warehousing
	- Summarizing data warehouse offerings on the Amazon cloud, really only have two things to talk about here. I wish that there would be more products and more competition, and maybe this call out will encourage more developers out there to enter this playing field because although I truly love Redshift and customers are very happy when I put them on it, I think competition is good for all of us. So the clear leader in this market is Redshift. It's been out for quite a while and many times, as I said earlier in this section, I have customers who think they need Hadoop or think they need some NoSQL implementation, and they're really very happy with the SQL-like OLAP implementation that Redshift offers.

	- It's a great value at a thousand bucks a terabyte when you buy it in advance for a year. Has SQL-like queries and a very, very strong partner ecosystem. There's probably in the 10s and maybe even 50 to 100 partners. I commonly use partners for activities around Redshift like loading data, visualizing data, cleaning data, and more and more partners seem to be jumping on this bandwagon. New on the block but warranting notice is Snowflake DB. This is a database or data warehouse as a service which runs on the Amazon cloud, and you purchase from Snowflake DB.

	- They have feature similarity, and they have at the time of this recording just come into general availability. You'll need to contact them to try out their offering because they are a vendor that sits on top of Amazon. They're not an Amazon product per se. But their offering is very interesting in that it has native support for JSON and it has query capabilities that are SQL-like, and it has parallelism as well in its OLAP implementation.

	- To summarize, many architects these days like to talk about big data and the dupe, but my practical reality is that in the majority of cases, my enterprise customers are more satisfied and happy with cloud scale traditional kind of technologies. Now this may change as more and more data gets collected and huge volumes become the norm. But for the day in, day out, I would say that any big data solution that is using an OLAP or a read-only kind of store, Redshift is something that you should consider.

	## Visualizing Data with BIME
	- I want to take a minute to talk about Redshift Partners. I've been mentioning it throughout this section, but I want to direct you to the Partner directory, because this a great place for you to find Partners, and investigate them when you're working with Redshift, so aws.amazon.com/redshift/partners, and you can see that Partners are grouped by functionality, so, Business Intelligence, Data Integration, so on and so forth. When I was looking at this page, I found an interesting link on the Amazon Big Data Blog about Partners and about working with Redshift.

	- So this is an interesting capability that uses the new type of compute that we've been talking about in this course, which is AWS Lambda, so a very common, from the real world, scenario that I need to address, is how to get data into Redshift, and you can see this is relatively recent. This was created this year and this is a great sample that allows you to use a Zero-Administration Redshift Database loader with Lambdas, and you can see the architecture here, and there is a repository on GitHub that you would have the sample code where you have the input from S3.

	- You use the Lambda functionality in conjunction with DynamoDB, which you may remember from an earlier movie is a key value store, and then you load into Redshift. So, this starts to show us the complexity of the architectures that I'm building as I partition my data workloads across the various cloud services. It's just a really well-written example, and it's really perform-it and cost-effective using Lambdas, which is a more light-weight way to implement compute than the traditional virtual machines.

	- So I wanted to call that out in terms of Partners as well. Also, in the Big Data Blog, there are commonly blog posts written by Partners with sample code, so, I really use these two pages, this Partner page and this page in conjunction to find out what's going on with Partners. Now, I wanted to also highlight a Partner in addition to kind of talking about Partners in general, and that Partner is Bime. So, Bime is cloud-based data visualization. They allow you to make dashboards. And what I love about Bime is you can try it for free, so, if you want to try it out, you just sign up, and it's free to try, and you can connect to a number of data sources and pull the data together and make a dashboard out of it.

	- Great for just a variety of business scenarios and it connects natively to Redshift, so, once you have an account, then you will log in to your unique account, and then you click Connect a data source now! And you can see all the different data sources, and here's Redshift, and click Next, and you fill in your Username and Password, and your Host name, and then your Database name.

	- Do you remember how to get the Host name? We'll go over to Redshift, we'll click on our Cluster, and here's our Endpoint. We'll go into Bime Studio, paste this in, remove the port, and our Database name is mycooldatabase. We'll select our table. Let's pick our sales table, and click OK. And click Next. So, Bime is doing a query into Redshift, and providing you with a list of attributes and potential measures, and of course, you can adjust this, so, we could say that the buyerid, we could move by just dragging it over here, and then we can say Next, and we can store our data on Bime or just keep it in Redshift, and now we have our Query Builder, and in our Query Builder, we get a list of Measures, so we can say the quantity sold, and we can say the sum of quantity sold, and Apply, and there's the sum of quantity sold, and then if we want to slice and dice, we could do this by buyerid.

	- This might be kind of not visually appealing because we probably have a lot of buyers here, but, just to show you how it works, and we can see we ran a live query here against Redshift and we visualized it using Bime. Now, Bime has a lot of great visualization capabilities. It's a whole lot more than tabular data, and I encourage you to explore the configurations if you try out Bime, and they certainly aren't the only Partner that integrates natively with Redshift. There are many other visualization tools out there that I commonly use, QlikView, Tableau, others as well, but I really like Bime because it's quick and has a great look and feel to it, and integrates very seamlessly with Redshift.

	## Using DataPipelines
	- I'm going to take a minute now to talk about another service on Amazon that I use with data projects and that's called the Data Pipeline. And I use it particularly with data warehousing, that's why I put it in this section. Although, you can use it with really almost all of the services that we are covering in this course. So, it's a cloud-based data workflow and another way to think of this is ETL tool or Extract, Transform, and Load. And this ships or is available with components for nearly all the AWS data services. And also, the components are extensible.

	- So, an architecture example that I've used with AWS data pipelines and data warehousing is the OLTP to OLAP Cloud scale solution. So what does this mean? OLTP is your working or transactional database, so this is usually something like a MySQL, a SQL server or an oracle relational set of databases that the data is in send on a regular basis to a data store that's optimized for reading or OLAP, Online Analytical Processing, and this is often Amazon Redshift, that's why, again, I put it in this section.

	- But it can be an OLAP implementation that's built, for example, on SQL server like an analysis services cube that's maybe running on an EC2 instance. So there's a number of different combinations and permutations here but that's a common pattern. So a working or transactional set of data that is shipped via data pipeline to a analytical or a read-only store. A common way to talk about this is to say that you're using a pipeline to populate a data warehouse. Another aspect of the architecture around these data pipelines is the impact of AWS Lambda services, which you may remember from previous movies is a new light-weight type of functionality for compute where you pay by the function call rather than paying by the virtual machine.

	- So, I'm seeing more and more patterns with the integration of Lambda's in data warehousing pipelines. And it's a great way to be efficient and to save both time and money. Now here we are in the AWS management console and under the analytic section, you'll see that we see the data pipeline service, that is named as orchestration for data driven workflows. So as with the other services, there is a brief introduction which tells you that you define data nodes and here we should be familiar if you have been watching this course sequentially.

	- Select input and output data out of the S3 service, the file service DynamoDB, the NoSQL store Redshift, RDS which would be the relational service and JDBC sources. Schedule compute activities, so processes, and activate and monitor. So we're going to click on the blue get started now button and we're going to give this a name. And we can build using template, import a defintion, or build using the architect capability.

	- So, we're going to use a template, and we have some common patterns in our templates, so you can see that we have just a shell command, we have a CLI command, this is a Command Line Interface, DynamoDB template cross-regional table copy, Export into S3 or Import from S3, this is Hadoop which we haven't covered yet, then we have RDS templates, copy of RDS to S3, so on and so forth, and here we have Redshift, Full copy of RDS MySQL to Redshift, Incremental copy, load billing information, or load S3 into Redshift.

	- And we did this manually in a previous movie in this section using the client tool with a copy script, so I'll show you what this looks like in a pipeline. And you can see I have to put in my Redshift password, I have to put my security group, my database name, my username, and let me fill in some of these values here. I can optionally specify a Create SQL table query, and I specify the Insert mode OVERWRITE_EXISTING, KEEP_EXISTING, or TRUNCATE, then I specify the Redshift table name, the Input S3 folder and I can browse here.

	- And I have a very small file here. And then I can have my Redshift JDBC connection string, and my copy options and primary keys. And then I would run a pipeline or specify a schedule, and then I would set up logging, security, with IAM roles, and tags. Now, if I click Edit in Architect, I need to fill in my value for my database.

	- And now I'm going to click Edit in Architect. I'm going to disable my logging, and now I'm going to click Edit in Architect. And this shows me two data nodes and then a RedshiftLoadActivity, so you can see if I click on this data node, I can configure the properties of the data node. So you can see this is a S3 data node. Here's my type and then I can put in my path, so on and so forth. If I click on my RedshiftLoadActivity, I can again, fill in the properties.

	- If I want to add an activity, I've added an activity here, and now, I can select from the various types. And notice, it's copy, this is Elastic MapReduce, which is the Amazon managed Hadoop service, Hadoop, Hive, a lot of Hadoop ecosystem type activities, but here's the RedshiftCopyActivity that I wanted to point out. In addition to the activities that are available, this is programmatically extensible. So, once you are finished working with this in Architect, then you would list this as a completed schedule.

	- So let me go back to pipelines, and you can see here is my pipeline, granted there were some path errors there, I would have to fix before I would run it. I can edit it, clone it, delete it, export it, or activate it. And once I activate it, then it will run on the schedule and then it's integrated with all of the other Amazon services. So Data Pipeline is a service that I am finding myself starting to use more and more as I work with complex data flows and it has very good built-in integration with Redshift and S3, which is a common design pattern for data warehousing.

	## Using Xplenty for ETL
	- To finish out this section on data warehousing, I want to talk about a partner highlight called Xplenty. So Xplenty provides data pipelining, Data extract, transform, and load, and runs on the vendor cloud. So it's Very easy to use, and it Natively connects to most AWS Data Services. It Provides customizable load processes and it can get up and running really, really quick. So let me go over and show you a little bit more about their product. I'm at the Xplenty website, www.xplenty.com.

	- In order to try out their product, you have to contact them for a trial, so I'll just show you a bit about it and you can see this is a sample package and it looks like most of these eTail products where it has a square that represents a process or a data source and you have a number of transformations. It kind of reminds me of SQL Server SSIS in the way that it looks and feels, which is one of the reasons that I like it, because I'm familiar with the technology again. So notice, No Coding. No Deployment. because it's running on the cloud. So, let's look at the integrations that relate to the Amazon Data Services that we've talked about in this course so far.

	- So it integrates natively, sort of File Storage with the Hadoop File System, and in terms of Cloud Storage, it integrates with Amazon S3, some other clouds as well, and then it terms of Data Stores, it integrates with Amazon RDS, so if you're using any of those managed relational database systems like MySQL, SQL Server, it integrates there, and it also integrates natively with Redshift and other systems as well. So it's a really powerful tool.

	- I'm just looking for the other systems here. Here's another one that I have shown in this series. This is MongoLab, which is the managed MongoDB. Again, it's just very well-designed to build the cloud pipelines on the Amazon Cloud and if you're looking at using the Amazon Data Pipelining or writing your own pipeline, you might want to look at partners such as Xplenty. There are other ones as well, but this is just one that I have familiarity with, and in order to try them out, you would have to contact them to get a trial and then they would allow you to test out their functionality.

	- For the purposes of this course, I contacted Xplenty and they were courteous enough to give me access to their services to include in this recording. So, this is a demo of the Xplenty ETL service. So what we'd want to do is we want to go over to my packages and you can see that they've provided some sample packages here but we'll just build one up so you can kind of see how it works, so I'll click on new package, and then, I'll click on new source, and the sources that are available are Cloud Storage, Database, Redshift, MongoDB, and then some other sources.

	- So, let's work with Redshift, and then we can just open this up and say AWSRedshift and then we can specify our redshift connection settings here. So, what we need to do is we need to click create new, and then we'd have to create a connection to redshift, filling in the information here, the Name and the Type of Connection, a Direct or a Tunnel, the Hostname, so on and so forth, and then we could test the information. Notice, once I make the connection, then I set up a column mapping, source table, a where clause, and then a null string.

	- And I'm actually just going to get rid of this and just do a simpler demonstration, and I'm going to take a look at Cloud Storage, and then I'm going to call this AWSS3. You can see there's the S3 icon there, and so this is an Xplenty connection. Now if I wanted a new connection, we'd just go out here, and then I could connect to the various types of cloud storage, whether it would be Amazon or other clouds, and I could also connect to Hadoop as well, being a different kind of a file service.

	- So I select S3, then I need to specify the Name and I need to specify the Access Keys, and then I can test the connection. Once I have that, I specify the bucket or container, the path, pre-process, source file, and field aliases. Now, for the purposes of time, I'm actually just going to jump back to pre-created package. This is in their templates for Xplenty, this is AWS CloudFront Log Analysis. So, inside of here, you can see that each node represents an activity or a connection.

	- So if I click into this cloudfront_logs node, you can see this is Xplenty connection, here's the container, here's the fields, and this first activity that I'm going to do is remove a header, so it's a filter activity. So I have a Match condition, and then I have Match parameters. So really simple data pipelining without programming, and then here's another activity where I'm doing a clone, and then I'm performing some aggregation, so let me just drill in here.

	- You can see I'm aggregating by the date, and then I'm passing into another bucket, so this is an S3 S3, basically. So I'm aggregating_by_date, aggregating_by_ui, here, I am doing a projection of selecting some of the fields, and then I'm doing an aggregation. Here I'm doing a join, so you can see I get to specify the type of join, and then I have advanced options, optimization, whether it's Replicated, Skewed or Merge, and you can see, this is my running package, and then when I'm done, click save, and inside of my dashboard, if I wanted to run one of my packages as a job, I would schedule it as a job and I would click run now, and you can see I can also have schedules inside of here.

	- So, I could create a new schedule, very simple, and then, I can work with my cluster. So the point of this is that it's GUI-based pipelining that's native to not only the Amazon Cloud but other clouds, and very, very easy to use, so very quick to work with. To be able to try this out, you'd have to contact Xplenty so that they could give you access to sample functionality, but I wanted to include it just to show you what it looked like in this course.

# Graph DB and Machine Learning

	## Understanding AWS EC2 AMIs
		- Now we've been looking at core date services on the Amazon Cloud, but there are other types of services that we haven't yet covered that we're going to talk about in this section. They're provided by Amazon and they include data services on pre-built machine images, images from the Amazon Marketplace, or resources that are managed by Amazon partners. Now this is different than resources that are provided by partners. So, the differences managed is you're paying the partner provided by partner is you are buying software from the partner, so it's a subtle difference that we'll be looking at here.

		- Now I often use services that are provided by partners for data loading, data cleaning, and data visualization. Now we're going to use an example of a type of database that we haven't covered, that I'm having more and more of my clients, have a need to put some data in, and that's a graph database. So this is in the NoSQL world, and it really falls under, it's not really the size of the workload but it's the complexity, and this the noun-verb database, where you store both the entities, shown here by names of people and cars and companies, and verbs, which is the relationships between the entities.

		- And a database of this type allows for a faster complex query path, so is becoming increasingly used in the various workloads that my customers are bringing to me. So there are many graph databases out there, but the most prevalent in the industry is something called Neo4j. So we're going to look at how to implement Neo4j across the various possible solution vectors on the Amazon Cloud.

	## Installing a GraphDB using an AMI
		- In figuring out how to implement Neo4j on the Amazon Cloud, which isn't natively offered by Amazon, the first place we're going to look is EC2 Amazon Machine Images. So these are virtual machine images that enable infinite customization. There are 1,000s of AMIs available. And when possible, you want to use vendor-created AMIs. At the very minimum you want to use AMIs that are created by Amazon or the vendor themselves. You do not want to use public AMIs, they are a security risk.

		- So we're inside the Amazon console in the EC2 dashboard. And I've clicked on AMIs, and then I've done a search for Neo4j. And here we see that we have one image, Neo4j, and we can see that Neo4j is installed on this image. And if we wanted to work with this, we could go ahead and launch it. We could spot request, which would mean that this image would only spin up if there was compute available for the level that we're looking at.

		- So what that means is, for the particular size of the machine. Now, the reason you do a spot is because it's substantially cheaper. We could take this as a base AMI and register this. We could modify and edit tags. We could modify the Boot Volume Setting. Now to launch it, we would just click Launch and then we would select our Instance size, and I'll just go with micro for this. And we would click Review and Launch. It asks us what kind of storage we want, and it's recommending SSD.

		- And it's warning us that our security is open to the world, which is fine for a testing. And we're going to click Launch. We have to associate this with a key pair so that we can access it through SSH, and click Launch. So we can see under an Instance here, it's spinning up, and just like the Instance that we created from an earlier movie, once it's available, we can just click on Actions and we can click on Connect, and then we could SSH into it.

		- Then we would have full control. So that's one way for us to work with Instances. Of course, you could also have, and I'll go back over here to AMI, so where this is spinning up. You could also have images owned by you. So you could have your own Instance here, and you could then just pull this up through the console. Now, if this is a production situation, you'll usually use a command line interface, rather than clicking in the console. And as with any other virtual machines, as you're working with your VMs you can turn them on, turn them off.

		- You can create snapshots of them and then restore from snapshot, so on and so forth. So this can be a really quick way to test out a graph database.

	## Using AWS EC2 AMI marketplace for GraphDB
		- Another way to spin up Neo4j in the Amazon Cloud would be to use the Marketplace. You might remember from earlier movies that the Marketplace is most often EC2 instances plus services and configurations, and created by vendors. It's great for quick prototyping and also see if there will be any alternatives, in this case, to Neo4j. So in the main console, if we just click AWS Marketplace, that will take us over to the Marketplace and I've done a search on graph database, and you can see the result of this is three alternatives to Neo4j.

		- I often use this as an architect to see if there are alternatives to software that I'm looking at that are already optimized to run on the Amazon Cloud. And to see more about one of these offers, we just click on it, so GraphDB Cloud v6.3, for example. We can see that we have a trial available and it tells us a little bit more about the database and the instances and the product details. It tells us how to use it. Over here, we can use Reserved Instances for lower prices, or Spot.

		- And Reserved Instances of course are buy in advance and for at least a one-year period and Spot is bid on unused compute, so name the price. So let's go ahead and click Continue and we have the option to launch from here. We can do a 1-Click Launch and notice here is our Instance Type, VPC Settings, and Key Pair. You might remember this from an earlier movie where we were working with a database called Aerospike where we associated this with a key pair so that we could log in.

		- It gives us our cost estimated charges and we could also do a Manual Launch. So, this can get me up and running much faster than using an EC2 instance. It's a little bit easier to read in here, a little bit easier to select the regions and the areas that I want to work with. There's vendor release notes, so on and so forth. So if I say 1-Click Launch and I say Accept Terms and Launch with 1-Click, then I have to just confirm with email and then it will launch. It tells me how to SSH in, and other configuration details and then I'll be able to work with an alternative to Neo4j.

		- So it's a tip, really, that I would give architects out there who are trying to figure out which software to use. I see here this is from an earlier movie, which is another 1-Click piece of software that I launched that I could then work with as well. This isn't a graph database. This is Aerospike, which is a high-volume in-memory database but again, working with the Marketplace in addition to searching the AMIs is a real-world tip that I often use when looking at data storage but also other types of data services.

		- You can see in the Marketplace, there are a number of other data services. For example, here is Business Intelligence, where I can try out different software, just a lot faster to spin up an instance than install. So, it's something that I use quite frequently and I wanted to share this tip not only for spinning up a graph database but also for working with big data services in general.

	## Exploring GrapheneDB, hosted DB
		- Now yet another way that I will spin up specialty databases on the Amazon Cloud is through specialty hosting companies that provide management because they have expertise in this database, and this is something I'd particularly do with an instance of Neo4j. So a company that I've worked with and had good success is called GrapheneDB. And this is very similar to MongoLab's, which is managed MongoDB on the AWS cloud in look and feel, and in capabilities. So you can see here I am at GrapheneDB, and they talk about their services here.

		- It's just very similar conceptually to MongoLab's. You would just sign up for an account, and then you would have a free account, and then you could explore working with Neo4j on the Amazon Cloud by just clicking through their browser based interface, and they would guide you through setting up your Neo4j database on a AWS instance. And then if you found that you wanted to move from testing into production, you can easily move into managed plans that are standard and performance.

		- And you can see that they have native support for the features that you'd really want in a graph database, features like spatial and custom plugins and extensions. And you can see, again, differences between standard and performance have to do also with the configuration here. They're making use of SSD storage on the AWS cloud, and other features that will be important to you when you go to productions, such as priority support. So, I have a number of different ways that I'll try out specialty databases, and I wanted to illustrate them throughout this section talking about Neo4j, because I am getting more and more customers who want to try a graph database.

		- And as of this recording, there is no core Amazon service that's provided that is native graph, so you're always going to have to go with a third-party graph database. Currently Neo4j is the market leader, but I anticipate there will be new graph databases coming out, because this a really hot area, so it'll be really interesting to see when and if Amazon comes out with a native graph database service to compete with Neo4j, or how they enhance their implementation of Neo4j. At this time, my methods of using Neo4j are basically to run the AMI, that's the most simple.

		- To use a Marketplace alternative, which may or may not be Neo4j, or to look at managed Neo4j running on the Amazon Cloud, and my preferred provider for that is GrapheneDB.

	## Introducing AWS Machine Learning
		- [Voiceover] In this movie, I'm going to introduce to you a new service in the suite of Amazon Data Services and that's Amazon Machine Learning. Machine Learning as it's supported by Amazon is new and really interesting in machine learning as a service on their cloud. It's designed for ease of use which in the world of machine learning cloud-based services is relatively unique. There are other vendors out there, notably Azure, who have taken a different approach. Their approach, in Azure, for example, is to support working data scientists.

		- Amazon is taking the approach of democratizing machine learning. The idea is that even for those of us who are newer to the world of different types of machine learning algorithm and models, we can get some value out of this service. To that end, their implementation supports only a subset of the commonly used machine learning algorithms. I'll explain this in plain English in just a minute. In case you do have a background, it supports regression or classification only. It doesn't support clustering, and it requires training data which is called supervised machine learning.

		- Now, from a practical point, a little bit about Amazon Machine Learning before I go and show you the sample. Just a couple of question categories to get us thinking about in which cases we would use our data and associate it with machine learning. The first is something called binary classification. The idea with this is to predict a yes or no value. Sample question is, "Is this particular email spam, yes or no?" As with all machine learning algorithms, you get a result that is probabilistic. A percent likelihood rather than a yes or no Boolean answer.

		- The idea is you're going to get a probability of a binary classification. In this case, a yes or a no. Another type of problem that is often presented to machine learning is called multiclass classification. An example question is, "Which category sells best to a particular customer?" If you're buying books, for example, which category of books has the highest likelihood of selling best? You would get a category and you would get a likelihood. The results of machine learning algorithms are always the value and then the likelihood or the probability.

		- The third type of question that's supported in the Amazon Machine Learning implementation is regression. Regression is designed to answer questions such as, "What price would this house likely sell for?" It would be a price, a value, a number, and it would be a probability. These are the classes of questions that AWS Machine Learning is currently designed to answer. If you're coming to this with a background in classical data science, this is a constrain subset of the possible algorithms that are available, so you may not find this to a fit.

		- This is more an every man's ML. Now, to learn more about concepts, if you're new to this, there is a very, very well-written tutorial. I'm actually going to jump over there and show it to you right now. I would highly recommend if you've never done any ML or data mining, to read this because it will introduce you to the concepts but it will also, more importantly, help you to use the Amazon ML service in the ways that it was designed. One of the things that I find implementing "ML Lite" as I call it with my clients, is one of the most difficult things is matching the types of questions that are going to provide value to the business to the capabilities of the particular service.

		- I have found this tutorial to be particularly well-written. This is a conceptual tutorial so that you can get the question formulation portion of an ML project in a usable state. Now, in the next movie, we're going to look at the mechanics which are actually, incredible easy once you formulate your question, and all importantly get that source data in a state that you need it to be which there is a lot of assumptions around loading up an ML model. We're going to us the sample data, which of course, is pristine and clean.

		- In the real world, the process of preparing data for any type of analysis, but particularly machine learning, I have found to consume the majority of the time in actually using the service. In other words, I can run a model in an hour, but it might take me eight hours to figure out, find, formulate, and clean the data. That's the practical reality of this type of technology.

	## Using AWS Machine Learning
		- Here we are in the AWS console and we're going to take a look at Amazon Machine learning. It's under "Analytics", and "Machine Learning". Now you notice we're in US West and Machine Learning is not supported here so we need to switch to US East where it is supported. And as with all the services we've been looking at when you don't have any implementation Amazon provides you with a quick overview and nice blue button to get started. So we'll click on "Get Started". So we can work through either the standard setup with a sample data set or we can go right to the dashboard.

		- So we're going to click on the blue "Launch" button. And there are five steps in the data source creation phase of Amazon Machine Learning. The five steps are: Input the data, work with the schema, the target, the row ID and review. So as it says here again very well explained, first step here is to show Amazon ML your historical data, and as I mentioned in the introduction the previous movie, the types of Machine learning algorithms that are supported in this implementation require historical data called Supervised Machine Learning.

		- So if you do not have any data ready you can use their banking sample. So notice it talks about the "dataset contains information about customers as well as descriptions of their behavior". So you can preview the file here. So we'll just work with it here. So in this case we can import the data either from S3 or from Redshift. And we may need to upload this data into S3. So let's download this data. So here I've downloaded the data, and I'm going to go back over to S3, and I'm going to "Create a Bucket", and I'm going to call it "mldemoaug4".

		- So it has a unique name. And then inside of the Bucket I will upload my CSV. And once that's complete I'll remember the name "mldemoaug4", go back to "Machine Learning", "Get Started", "Launch" and enter in my Bucket name and my file name.

		- And I'll call this one "BankingDemo". This'll be my data source and I'll verify it. And now I'm getting a dialogue that says, "Amazon ML needs read permission to read input from my S3 location". Yes I'd like to grant permission. The schema is being determined, this as I said in the introduction to this is artificially easy in the example because the sample data is in the format that Amazon ML can infer the schema and I find in the real world, you have to have some iteration to prepare your data so that either you can provide a schema description file, which is an option, or that it can be inferred correctly.

		- So it's artificially easy here is really what I want to point out. And we're going to click Continue and notice the data was scanned. It inferred the column names and the data type. And does the first line contain column names, if I say yes you can see then the column names are promoted. And then the types are inferred. Now the types in Machine learning are different than if you've been working with relational data. We have types that are obvious like Numeric and Binary.

		- But Categorical might not be as obvious, so what is a Categorical type? A Categorical type would be used in the execution of an algorithm that creates categories. So do you remember from the previous movie that when we work with the Categorical algorithm type which is Multiclass Classification, we need Categorical data. So in other words we have to provide the appropriate source data. If nothing is categorized we then we don't have any source data that shows proper categories.

		- It's pretty simple if you have no Categorical data then implementation of a Multiclass Classification algorithm isn't really going to do much for you. Let me put this back to Numeric. We have Numeric as "age". We have "campaign" as, I think that was Categorical. Conference ID is Numeric so on and so forth. "day_of_week" is Categorical. And these are the inferred types. So now we're going to go ahead and continue. And it says, "Because you have picked the combined create ML and Datasource wizard, you must specify a target".

		- So what's a "target"? It actually defines it here: a "Machine learning works by finding patterns that connect your data to the value to be predicted. To create a model, Amazon ML analyzes examples of the data records with correct values. The column that contains these values in the training dataset is called the target." So it's the vernacular of ML. So the column that contains these values is the target. So which of these has a target value in it? And we'll say that the campaign is correct for this, like we've cleaned our data.

		- "You have selected campaign as a target. ML models trained from the datasource will use Multiclass Classification." because we're trying to figure out in this case which campaign is going to have the highest likelihood of association here. So this would be a type of marketing campaign. And click "Continue". And a Row identifier, this is "used to let Amazon ML know which other column to include in the prediction output file. This column makes it easier to follow which predicition corresponds with which observation." So if we wanted to select this, if we had some sort of identifier.

		- Let's see do we have something in here? "cons_price_idx", maybe this, I'm not as familiar with the data as I should be to do this. Record identifier, this may or may not be correct but you're looking for a unique identifier here. Alright, and we can make any changes. "Annotation fields must be Categorical". It's going to be converted. So this is an annotation and so it was set as Numeric. So this is an example of Machine learning for every man, because when you make invalid selections, it's possible the wizard will correct it for you.

		- However, being a wizard you need to understand why these corrections are occurring, that's why you didn't want to read the docs. So click "Continue". And now it is creating a model, so you can see in the model here, it's loading the input data and it's suggesting we use Multiclass classification or we can customize if we want something else. And we have some other settings, that we can customize. The default is that we split the data 70 percent for training and 30 percent for evaluating.

		- And this is a common technique, not only for Machine learning, but from data mining. Where you're splitting you data so you can evaluate the effectiveness of the model. We're going to click "Review". And notice we didn't provide a recipe for this model, so Amazon is going to generate one for us. We'll click "Finish". So now model report is being generated, and we can then perform an evaluation of the effectiveness of this model. We haven't done any evaluation yet, or it's not complete, I'm not sure which I have to take a look.

		- Let's actually go to the Dashboard and look. So yea it's in progress. So we have, here's our source data and then here's the split. So remember it's a 70/30 split, and what Amazon ML is doing is it's splitting the data and it's processing the model, and then I'll provide an evaluation of the effectiveness of the model. And what that is, is the result of the model, so the prediction in this case of the category. What is the percentage of likelihood based on the data, based on the use of the algorithm? And compares that to the value of random guessing.

		- And then gives you a score for this particular model, to see if the use of this data with this algorithm scores much more highly than random guessing. So that when you're using this as a basis for business decisions, you can look not only at the output of the model, but more importantly at the quality of the model. Again working in the Machine learning field or predictive analytics is really completely different than traditional reporting, in that it's probabilistic outcomes, and you iterate on the input data, the fields you select, the way you categorize them even actually down to the row level you might remove outliers.

		- You iterate on the algorithms selected, the tuning of the algorithms, and even the split of the data. So there's so many different variables that you iterate back and forth in order to produce a model that is scored highly and has a very useful output. So it's really beyond the scope of this course to teach you Machine learning. I really want to just focus on the implementation of Amazon's cloud-based service and to talk about as an Architect, how this implementation can be associated to big data pipelines, which is an architecture that I'm starting to build since the commercial availability of Amazon Machine Learning cloud-based service earlier this year.

		- Now I'm going to just pause this and wait for the process to complete and then I'll show you when the model is built and the evaluation is done. So I've waited a couple of minutes and now our processing is complete for our Machine learning example. So if we go to our dashboard we can see that we have a number of objects. We have datasources, we have a model and evaluation. So let's drill into them. So for the banking demo, this is our source data, you can see that we have information about the datasource here, the number of attributes, the type of data so on and so forth.

		- And we can click into the Target distributions. And we can see from the campaign, which you will remember from the previous movie is the value that we're focusing on here. We have distribution information, and sample data. And we also have missing values. So no missing values so we have a perfect data source unlike the real world. Then we can look at the different attributes of our datasets. We have Binary and we can preview and see what that looks like, with the Binary attributes.

		- And why we have this sort of visualizations is so that we can iterate as I was talking about in the previous movie because the process of developing useful Machine learning models is iterative and what you end up doing is playing with the data. So you take a look at what you've got and again all perfect data here no missing values, no invalids and you look at, well are there some outliers, something I need to remove because it's just statistically insignificant. Again, you have your, your standard statistical visualizations.

		- You have the box plot, and the min and the mean and the max and all that stuff. And you can select your bin width here. So it's iterative tools for working with the data. And we have no text data sources here. So that's to look at you data sources to say, well do I need to make change that? Then you have your model itself. And the only thing you really have in the model is you have the ability to work with the model, we have our evaluation which we'll look at in a minute. And then for the Usage, this is really interesting and reflects the practicality of this service.

		- We can generate a batch prediction or we can generate a real time prediction. Which can then integrate with some of the data services we've been talking about in this course such as Kinesis the stream service, which is a lovely integration and enables more senarios with this product so it's a nice functionality. So the next thing we want to look at is the evaluation of the model. So in terms of the evaluation, of course, this model's going to score well because it's the sample from Amazon. In the real world you'll have to iterate around with it a bit to generate a model that's useful for your particular business scenario.

		- So again you'll probably have to read the docs to really understand what they're showing you here because it's a lot of statistical numbers although they've got some highlights in here. And we can go and explore the model performance. And that's going to show the F1 scores and prediction distributions of our model. And we can download the complete matrix if you have the stats background. And we can see here that we have some correct values, correct predictions, and incorrect predictions. But, unless you have a stats background, you probably are going to need to download the model and read the docs so that you can really understand what it is you're seeing here.

		- And you'll see different kinds of visualizations depending on the algorithm that you've run. Remember that we're running here a classifier to see which category is most associated with the value that we're trying to predict. So this is a really quick introduction to Amazon Machine Learning. And I encourage you to read all the documentation if you're new to it. But in terms of an entry into Machine learning, couple reasons I wanted to show it. First, yes it's light weight, but it is more assesible then some of the other cloud-based products out there.

		- And second, it integrates really well with the data products we've been looking at in this course. So S3, Red shift, and optionally Kinesis if you enable streams. So this is an introduction to Amazon Machine Learning.

# Hadoop

	## Explorting Hadoop
		- In this section, we're going to look at workloads that are Large or Huge and have varying levels of Complexity. Now, they're going to interact with Small or Medium, that's how they become Large or Huge. And on AWS Data Services, your usual choice for workloads that are large or huge is the Hadoop ecosystem. And some people would just say Hadoop, but Hadoop in the wild, is really not usable. So, it's generally Hadoop plus a number of other libraries, partner tools and other services.

		- So, let's first think about core Hadoop in case it's unfamiliar to you or just to define terminology. Core Hadoop, I define, as two parts. Files, which are shown to the right here, and files can either be stored in the Hadoop Distributed File System (HDFS) or in the Amazon Implementation in S3. And processing on top of those files. And the processing that is core to Hadoop is called MapReduce, and it's a distributed processing that works against commodity hardware.

		- The Open Source, in commercial implementations of Hadoop are based on technology that was originated at Google over 10 years ago, to solve the business problem of indexing and returning useful information about the public internet. Now, on top of core Hadoop, there are a number of libraries that make the core implementation applicable and useful for a broader set of business problems than indexing the Web. And therein lies the complexity, and the rub of working with Hadoop.

		- As a working Cloud and big data architect, the hype around Hadoop, particularly where I live and work, the West Coast of the United States is in some level, extreme, in that there are entire conferences devoted, not only to Hadoop, but even to libraries that can be associated with Hadoop, such as Spark, which allows for streaming data into Hadoop. Now, my practice as an architect, does include working with Hadoop however, one of the reasons that I chose to make this course about Amazon Data Service Choices is, even though I'm most frequently called with the intent to evaluate Hadoop, the reality of implementing it with the majority of my clients is far from the hype.

		- And what I mean by that is I implement Hadoop in five to 10 percent of my client situations, so what do I use in the other 90 percent? I use the other solutions I talked about in this course. Everything from S3 to Relational Data at scale to NoSQL. Now, that being said, there is a place for Hadoop. Where am I implementing this and how do I see this playing out and providing value for my customers? The biggest use case I see is innovative things. And what I mean by that is behavioural data at scale.

		- This is most often being driven by censor data, but I've also seen use cases where it's driven by very large amounts of behavioural data. Social gaming is the vertical that I've done the most work in. And I've mentioned this throughout this course but it really comes to play here, where every activity, every action the user takes, when they're interacting with their game whether on a phone or tablet or some other form factor, is recorded, saved and analyzed. And that can result in a huge amount of data, when you have a very popular game running with a world-wide user base.

		- So, what choices do you have if you want to work with the Hadoop ecosystem on Amazon Cloud Services? Their managed service is called EMR or Elastic MapReduce, and it has a number of features, starting with, you can choose the distribution of Hadoop that you want to work with. You can choose the plain, vanilla Open Source or you can choose a commercial version. And we'll see know, when we get into the demo, in addition to being able to choose a distribution, there are other features. So, as of the time of this recording, you could work with Apache Hadoop or MapR, which is commercial distribution, through EMR and you could also choose associated libraries.

		- And those would be libraries that have these funny names such as Pig or Hive, and they provide abstraction over the top of the HDFS file storage and allow you to do specific types of query in certain types of languages. And again, I'll show you that as we get into the demo. And you can add pre- and post-scripts, and the management of your Hadoop cluster is partially handled by Amazon. Now, in addition to using EMR, you can also choose to set up your own Hadoop cluster and some of my customers do choose to do this because the leading vendor of commercial Hadoop distributions is Cloudera.

		- There's also Hortonworks, but my customers tend to choose Cloudera and, in that case, we have set up on EC2 Virtual Machines, Cloudera. I will also include, in this discussion of how to set up Hadoop, the impact of application virtualization and Docker Containers which we've talked about earlier in this course, because that is certainly also impacting architectures and implementations of Hadoop. So, you basically have choices when you're working with Hadoop on AWS. You can go with EMR which is managed.

		- You can go with plain vanilla EC2, which then you manage. You can go with a Docker. Or you can go into the Amazon marketplace and you can look at the distributions that are available through the marketplace, which are configured EC2 instances, as you might remember from discussions when we were talking about NoSQL Databases there.

	## Understanding Hadoop jobs and libraries
		- As we think more about Hadoop, core concept is something called a job, and what that is, is a processing task that runs on top of the underlying file storage. Now again, if you're totally new to the Hadoop ecosystem, I've recorded a entire lynda.com course called Hadoop Fundamentals that you might want to check out before you run the sample so you understand a bit more about Hadoop capabilities. But we'll just continue on here. So, a Hadoop job includes tools to monitor job execution overhead, and console-based tools for MapReduce tasks, and the EMR implementation on Amazon includes alarms and logs.

		- So, this is a partially managed implementation of the Hadoop ecosystem and it's similar conceptually to some of the other partially managed data solutions that we've looked at in this course such as RDS for relational data and DynamoDB for NoSQL. So, you are paying for Amazon to do some of the management here. Now that being said, as mentioned, you could use a plain vanilla implementation and then you would probably use some vendors tools to get the alarms and logs and so and so forth.

		- I generally tend to use EMR, because it's the simplest to set up and monitor, but there's a number of choices in the Amazon ecosystem. Now, in addition to the core set of libraries which are on MapReduce, you usually will have libraries that have higher levels of abstraction. And that's because to use MapReduce, you have to be able to write your queries, per say, in a programming language, in an OOP programming language, specifically like Java or C++ or something. And, it's often the case that the consumers of the Hadoop data are analysts and they are not programmers so they don't have that, kind of, knowledge.

		- So the Hadoop ecosystem includes these libraries such as Hive, Pig, Spark, and many others that provide higher level language abstractions that are more SQL-like usually, although not all of them are, so that the analysts can leverage their knowledge of more traditional database query languages. And then these higher level libraries actually translate to the lower level implementation of MapReduce set of jobs usually, although not always. And, then these jobs are run on the cluster, and the results are produced.

		- So it is possible to install other libraries on the running cluster in EMR. And, you will be using S3 file storage for HDFS, so it's an abstraction on top of S3, so it's integrating with that. Now Hadoop summarized on Amazon, you really have, in my opinion, two choices for production. You have EMR for large or huge Use Cases. Alternatively, you could use Marketplace Amazon Machine Images for large or huge. I tend to prefer the former here.

		- Now, a couple of notes about EMR. You want to set up per your requirements and this is a place where I've used spot pricing. I really want to call this out because it's a tip from the real world. You will probably remember from listening to previous movies that there's three general pricing philosophies on Amazon. One is on-demand instances, which cost the standard price, reserved instances which are reduced substantially, and you buy in one or three year increments, and the most common way to do it is one year because of price reductions that occur over time.

		- And then the third method of pricing can be dramatically cheaper, it's called spot pricing. And basically you bid on unused compute, so you say I want to pay X amount per hour and if the machines that you're trying to use are not being used, then in this case, your cluster will spin up and your job will run. I've done a lot of data experiments super cheap using spot pricing. The business case was with my genomics customer, where they had data sets coming in from the different scientific community and they weren't sure if they were going to be useful or not and they had a data scientist on their team who was an expert in Hadoop query technologies as well, so that was a good fit.

		- And we used Spot and we were able to run these processes in a very, very cheap price. Now, of course, this is not for mission critical, because you're not guaranteed when you use spot pricing that the job will actually run. This is truly for experiments. So, with EMR, you want to have the expertise in house. That's another tip from the real world, or you want to plan for training. My best success with training has been to do some sort of formal training to bring the Hadoop course skills to the people on your team that'll be working with them.

		- When using Hadoop on Amazon, I will generally use Elastic MapReduce, which is their manage service or Marketplace AMI's.

	## Exploring AWS Elastic MapReduce
		- Here we are in the Amazon console. We're going to take a look at Amazon Hadoop or EMR, Elastic MapReduce, Managed Hadoop Framework. It's under the Analytics section. As with all of our services, when we're just starting, we have some introductory information on the first page, and we're going to create our cluster. Notice we would upload our data and processing application to S3, create our cluster, and monitor the health and progress of our cluster and retrieve the output in S3. So it's very integrated with S3.

		- So we're going to call this mycoolcluster. By default, we are going to enable logging to S3 and we can launch this as a cluster or step execution. So cluster, EMR creates a cluster with a specified set of applications. With step, EMR will create a cluster, execute added steps and terminate when done. So two different ways to launch. So we have two different configurations we can choose from, Amazon, which is the EMR-4 and it includes these libraries, the core Hadoop, Hive, Mahout or Mahoot which is machine learning, Pig, and Spark which is streaming.

		- We can just select Core Hadoop, or we can select Spark which is for streaming. Alternatively, we can select MapR and then this is the libraries that we'll get. So we'll just go with the default here. The Hardware configuration, this is the machine type and the recommended default is m3.xlarge. Notice we have some really powerful choices here because the idea is we're going to be working with large or huge data sets so we have Compute Optimized all the way up to 8xlarge, GPU Instances, Memory, and Storage Optimized.

		- So lot of choices here and when you're moving from a proof of concept to production as I've been mentioning with other data services, but in particular with EMR, you're going to want to do some load forecasting and load tested and select your appropriate instance type. It really will impact the speed with which your jobs will process. Here are the number of instances, note it's just one master and two cores. And of course we need a key pair so that we can SSH into the master node of the cluster.

		- It also tells us here we should use the IAM, the Identity and Access Management roles, the permissions, so it's more granular, and for our key pair, we're just going to use one that we created in a earlier movie, and then we're going to click Create cluster. So as the cluster is provisioning here, you can see information about the provisioning and you can see the configuration details. We could add steps into the process, we could clone and terminate.

		- If we expand some of these other areas, there's not going to be really too much here until our cluster is available, but we have Monitoring, we have Hardware, and here we have two Instance Groups, we have the Master and the Core, and you'll remember that it's one master and two worker nodes. That's another terminology. Here we have our Steps and basically we're setting up Hadoop debugging which is part of the managed service that we get by default.

		- We could add a step if we wanted to or several steps. Bootstrap Actions allow us to perform these types of actions on our cluster. So I'm going to go ahead and go back to the console, back to EMR, and you see that this is starting, and I will pause the video because it takes a couple of minutes then when it comes back, I'll show you the metrics and some of the things you can do in the console with your cluster. So you can see, my Hadoop cluster is now available.

		- And I just want to point something out to you. If you ever try to do a local install, if you can do it in 9 minutes, I would pay you money. Most of the time I see local installs, in the first time especially, of Hadoop and the Hadoop libraries, taking hours or even days, so that alone justifies using Amazon to at least try out Hadoop. What I find with my customers is that when I show them how quickly you can spin up a cluster that you can use, they are astounded.

		- So all kind of kidding aside, it's really important to understand the overhead associated with managing a local Hadoop cluster, which is something that I see a lot of my customers underestimate. So in any case, our cluster is available. Let's drill in and take a look at it. It's waiting because there's no jobs running on it. So inside of here, we could SSH in, using this DNS address, and this is information about the cluster, and now if I open up Monitoring you can see this looks kind of similar to when we were looking at some of the other managed database services like Redshift, for example, where we could see the overhead to the processes running.

		- And why this is so important and why you'd want to pay for this, is because, of course, the whole idea with Hadoop is you've got some huge amount of data and you're running distributed jobs across the nodes. So you want to first make sure that they actually complete without error, and second, you want to have some idea of what the overhead is on these jobs. So you have the Cluster Status, the Node Status, and then the IO. And this is working with S3 and HDFS, as I talked about in the introduction. And then, underneath here, you have the Hardware and you can scale this cluster if need be, add more hardware, because of course this is cloud, so it's very simple to scale.

		- In fact, if I click on Resize and I click on Add task nodes, I can add additional nodes or if I click right here and resize, I can just say, okay, I want to have three nodes, and then click OK and it will automatically resize itself. So this is the beauty of working with cloud scale storage, that when business needs necessitate scaling out or up, you can very, very easily do that just through the console, or obviously you could do this programmatically or through command line. Now if I wanted to add task nodes, I would click here and then I would just say the name of the task, the size, and the count, and here, notice this Request spot.

		- This is a tip from the real world again, if you're running some jobs and they're timing out, you don't have enough hardware, you can add spot and then if these instances become available, remember how spot works, it's buying unused compute at the price that you're setting so it's usually really a lot cheaper than on-demand, then your job will run faster or your set of jobs will run faster. So this is a great tip when you're working with EMR and you have variable sized workloads or variable overhead.

		- Maybe the processing pipeline gets clogged because there's processes that are dependent on one another and if you don't have enough hardware, if you have more data spiking in, the pipeline gets clogged. So it's just a tip, and something that I've used so I wanted to point it out here. So when you're working with this, you would then add jobs to this and you'd run your jobs and then you would look at the overhead of the job on MapReduce. Now, one of the other things I would call out is that you do want to watch your costs here because when you are testing, particularly if you're doing load testing as I've been advocating to figure out the overhead and the cost, you can run up some pretty big costs if you're using big workloads.

		- So use the pricing calculator that I talked about and showed in numerous previous movies to do pricing estimations when you're doing load testing. So this is Elastic MapReduce which is Managed Hadoop on Amazon Cloud Services.

# Data Scenarios and Architectures

## Intro to AWS Data Architectures
	- In this section, we're going to combine our knowledge of the various Amazon data services, and I'm going to share with you common architectural patterns that I use with my clients, and my clients tend to be either enterprise, small to middle level enterprise, although I have worked with some Fortune 500s and 100s, so really large as well. And the other types of clients that I work with tend to be startups. The industries that I work in I'll mention. I'm not going to mention specific clients, but I've worked in manufacturing, IoT, streaming type of scenarios.

	- I've worked with education, I've worked with social gaming and social networks, so lots of behavioral data. I've worked with government, I've worked with consumer devices. So I'm going to apply my real-world knowledge over the years working as a cloud and data architect, and I'm going to take the AWS data services that I use and show you patterns. Now what I've done is I focus on the data services, and although I'll mention some of the other services that are not data services, I really want to just focus on that portion because architectures can get really complex really fast, and I think it's useful to focus on the data services as a starting point, and then fill in the complementary services such as IAM permissions and CloudWatch for alarms and CloudTrail for compliance, so on and so forth, things that I mention throughout this course but are really secondary to the core data services which form the backbone of so many of the architectures that I build for my clients.

	- So we're going to get started going through the most common architectures in this section starting next.

## Exploring a file storage scenario
	- Our first architectural scenario is file storage, and I put this one first on purpose. I most generally get called as a Big Data and cloud architect to talk about moving relational workloads, data warehouses, or working with behavioral, greenfield, Big Data projects that the enterprise thinks should be based on Hadoop to the cloud. And I generally advise against moving any of these mission critical scenarios as a first step to cloud-based data storage.

	- The reason is complexity. It's really important when your teams are getting used to working with cloud-based solutions that you start with something simple, so that you can learn the vernacular, the tools and the processes of the cloud, and you can have an early success. And the simplest possible scenario is file storage. So I always start customers with file storage, and it's been a key to my success in helping customers to implement subsequent and continually complex data projects to the cloud.

	- So, let's look at this architecture. On the left, you can see the gray represents source data. It's on-premise, or could be located somewhere else, but it's not in the Amazon Cloud. So, tape storage in this case. So you've got mobile client-server users, clients' tape storage, and then, within the Amazon Cloud, you have a number of services, and the core services for file storage are S3, which is warm storage, and Glacier. What I commonly see is that my customers are using S3, but may be not aware of the S3 properties as we discussed in the movie about S3, in terms of the bucket properties in particular.

	- And, also, my customers are not using Glacier, really, to the extent I think that they should be, because you may remember, when we looked at the pricing of S3 versus Glacier, Glacier is exponentially cheaper, because it's designed for archival storage. And as customers move more and more data up into the cloud, although storage is really cheap, it will actually become a cost factor. So, Glacier uses a concept of a vault. And, again, I'm sharing the vernacular for you to use as well.

	- Now in addition, I commonly will use either the Storage Gateway, which is a service to connect your on-prem file sources to the Amazon Cloud directly for ongoing transfer of information, and/or other tools, some of which are provided by Amazon, like the Import/Export tool, and some of which I use commercial tools, such as tools from companies like CloudBerry Lab, which I showed in the partner highlight movie for file storage.

	- I have had really good success with partner tools that are GUI-based, and look like Explorer or a file management system from the OS that the end-users are comfortable with. It really is important to consider tooling and processes for accessing the cloud services. There's no problem in using the console and clicking through the console when you're first starting. Now eventually, you'll probably want to automate with tools or scripts, but I have many a customer that's been using the cloud for one or more years that still works with the console for some situations, and given the wealth of features that are shown through the S3 console, that's a very common situation.

	- Now, in addition to using regular file storage, I'll also remind you that there is a cheaper version of S3, which is reduced redundancy. So when I've worked with clients who have a huge amount of data, social gaming was an example, and storage costs were actually a concern, we partitioned the file data by usage, so we had the warm storage in S3, the standard redundancy, and then we had some that was reduced redundancy, which was used less often, and then the archival data we moved using policies over to Glacier.

	- So we actually followed the processes that I talked about in the movie about file storage. That's just one case. For other customers, they simply just use S3 and they're done with it. But it is the basis for bringing data into other data services in the Amazon Cloud as well, so it's a great way to get started, and it's an architecture that I use with nearly every customer who moves data to the Amazon Cloud.

## Moving relational work to AWS
	- In this scenario, I'll talk about moving relation work to the Amazon cloud. On the left we have source data and I'm highlighting your corporate data center because the idea is that your data center is overworked, over stressed, you don't want to buy more servers. You don't want to upgrade the licenses. You want to move those costs to the cloud. So, over on the Amazon cloud you can see I have two different types of implementation and this is common in the enterprise customers that I work with. So, one implementation is we have VPC Subnet's, and on those Subnets we have EC2 instances with databases on the instance.

	- So in other words, you're just moving your virtual machines to the cloud. It's very simple and when you have a data center that's being managed by on premise personnel, it's a really easy way to reduce costs and take advantage of cloud scale. So what you're doing is you're using EC2 instances, you could be using Amazon machine images or you could make your own. And I've done this very frequently with SQL Server, for example and SQL Server instance are just then run on beefy machines up on the Amazon cloud.

	- And they're isolated using best security practices that start with network isolation, shown here with VPC Subnet. Also best security practices such as IAm Users, groups, roles, and permissions. Now, in addition to that, you might try out some partially manage instances of relational databases and I'm showing on the bottom two instances of MySQL. This is a common small to middle size enterprise architecture when customers are wanting to start to store and collect behavioral data.

	- Now, we've talked about this in previous movies, but I'll give you a real world example. I worked with an education customer and their product was online educational materials and what they wanted to do is they wanted to store additional information about how students access those materials and worked with them. So in other words, not just the fact that a student completed their homework, but how many times they accessed the homework site, what activities they did on the homework site, and other types of behavioral information that was stored in logs but not being analyzed.

	- Or stored in a relational database prior to the project that I worked on with them. Now, they didn't really see the business case for buying more commercial licenses of SQL Server because it's such a large cost and we were able to store the data in a method that their analysts could use their current analytic skills using Antsy SQL Query capabilities against the much more inexpensively prices MySQL managed instances up on the Amazon cloud.

	- Another reason to use this architecture is the company had no inclination to hire additional personnel to administer additional relational database instances. So using the RDS service with MySQL was a great solution for this behavioral data because the company could store the data in a way that could be query'd and they could have only a small amount of cost added. Rather than if they had purchased hardware, bought commercial RDBMS licenses, in this case SQL Server and hired more DBA's.

	- So it allowed them to get business use in a very cost efficient manner of some of their additional data and the business result for this was they were able to increase retention of their students going through their online program. So it's a solution that I'm really proud of and it's one that I'm replicating with a lot of the small to middle size customers for whom a NoSQL or a Hadoop solution wouldn't make any sense. So I call it small to medium relational.

	- You notice we also incorporated the AWS direct connect and the Customer Gateway services to facilitate data transfer back and forth between the on premise and the cloud location.

## Building a Data Warehouse in the cloud
	- In this next architectural scenario I'm going to talk about how I work with AWS Data Services to help my customers build data warehouses. So once again I'm often called for a NoSQL database, or Hadoop installation. And again, I don't mean to be negative about those data technologies. For certain situations they work great. However, I want to call out the number of times I've been asked to do that, that the correct solution is actually to use Amazon Redshift. I am such a huge fan of this technology, particularly for shops that I work with where all of the technical talent onboard has only worked with relational databases.

	- And that's the majority, because NoSQL and Hadoop is new. Now that being said, I do live and work in the West Coast of the US and when I go, for example, to Silicon Valley, Hadoop is actually as prevalent as relational databases in that region of the world. So in some ways that is maybe a precursor of what's to come. But when I'm working in other parts of my client base, in other geographies, I find that the most prevalent database that's understood by the staff that will be working with my solution is relational.

	- So that being said, if somebody asks me for a reporting solution, this is a very common architecture. And you can see that this is building on the architecture from the previous discussion. In this case I've helped my customer to move some of their on-premise relational workload up into the cloud and the reason for that is then we have a faster transfer to Redshift. And I've also used, partially managed, relational databases using MySQL to capture behavioral data.

	- And then we can aggregate those two using processing into Redshift. Now one thing that I haven't shown on here is I'm starting to use Data Pipeline service, which I talked about in the Redshift section of this course to build ETL or Extract, Transform, and Load pipelines. For those of us who have a background in SQL Server that's an SSIS-like pipeline. That stands for Integration Services on SQL Server, which is an ETL technology. You'll notice also in the services that are used to provide the glue or connection between the "on-prem" data and the AWS data, I have Direct Connect gateway and then I've added Kinsesis.

	- Because for some of my customers, they want to use the streaming capability that is part of Kinesis to get some nearer to real time results into their Redshift cluster. One of the things that I haven't shown here again because I'm focusing on the data implementation and I'm not focusing so much on the parts and pieces around the data, is what tools I advise my customers to use to visualize and query the data that's in Redshift. The tools that I have good success with are generally provided rather than built because there is such a strong partner ecosystem and it just makes sense to buy rather than build in many cases.

	- And the buy can include customization. The partner that I showed in terms of visualization was BIME. And they're kind of a new guy on the block, but I really like their cloud-based offering. Very very beautiful and easy to use. In addition to BIME I've had some great success with the market leader Tableau and then I've also done some work with the very solid QlikView, which includes now Qlik Sense. So there's a large set of visualization vendors who integrate directly into Redshift to pick from.

	- And if you are going down the relational route and you wish to put a technology that is relational-like on top of your OLTP system using Redshift along with some of the integration partners, not only for visualization, but also for loading. There's a whole set of partners that facilitate ETL if you don't want to build your own pipeline for example using the AWS service. The partner ecosystem around Redshift is elegant and makes setting up a data warehouse using this set of services a several week or maybe month long project, rather than a year long project.

	- Of course the biggest cost associated with the initial setup is often cleaning up the data. So some data problems never go away. They just scale I guess, if you will. So in addition to the practicalities of setting up the pipeline to move your source data into Redshift for a data warehouse, another lesson from the real world is to do data sampling and data cleaning to get a sense of how much resources you're going to need to put on that when you're setting up your data warehouse.

	- Because from a practical point, you could probably physically load a warehouse with most data loads in less than a week, even a couple of days. But in order to get value out of that data it has to be in a state and at a quality level that is usable by the end consumers of the data, and therein usually lies the challenge. So this is a typical architecture that I've had really just consistent success with quickly and easily building data warehouses that provide value to my customers using Redshift.

## Working with caching and real-time data
	- The next architectural scenario is around caching, and this is partitioning of data in an application in order to put a subset of data into a very fast usually in-memory cache, and this is often used in situations where you're going to have some kind of lookup, either in your mobile app or website or whatever application you're using. So here on the Amazon side, we have the Amazon cloud, we have the subnet, we have our data, and we're building in our scenario.

	- So you can see we've got our database instance running on EC2. We've got two of them there. We have managed MySQL on RDS, two behavioral instances, we have an S3 bucket storing some file data. We have now added several other services because we are partitioning our data coming into our application. We're using ElastiCache with Memcached to give us a big in-memory lookup, and an example of this would be to have customer information, so an ID and a customer name, so that in the application that information is always available.

	- We're also using Kinesis here so that we can stream the information into our data sources, and we're using DynamoDB because we want the super fast SSDs to store information. I'll just take the gaming example here about what activities we're doing in our game, so where we are in our land and what our resources are and what resources we're acquiring. Notice we also have a data pipeline associated with this, and the pipeline will have various stages where it will take the data from one service and put it into another service, either direct transfer or some kind of transformation.

	- Outside of our AWS cloud, we have an edge location, and this represents our content distribution. So we're streaming some of our content in locations that are closest to our users using our CDN, which is a service that is available to stream files based on the user's location, and this is commonly used in gaming type of applications. And our application is Kinesis-enabled, as represented by the client with a Kinesis-enabled app icon next to it, so that means that streaming is enabled inside of our application.

	- So what we're doing is we're moving from the more traditional world of transactional data into the in-memory slash streaming slash near real time world that is driven by a lot of consumer applications and probably in the front of those consumer applications is games. Social games really is driving a lot of technology change around the implementations of data architectures. This might transfer into something if you're not a social gaming company. I worked with a medical records company, and it was really interesting to try to take this kind of an architecture and apply it to the aggregation of behavioral data coming out of medical devices that people were wearing on their person, so streaming device behavioral data, everything from the Misfit Shine that's around my neck as I record this, which records my steps and my sleep information, to more sophisticated medical monitoring information that comes from devices that is associated to certain conditions.

	- So this medical record provider was working in conjunction with patients and hospitals and wanted to improve their architecture so that they could aggregate this information coming from these patient devices in near real time and provide that information as part of their medical records. So it's really an interesting use case as these consumer technologies move into commercial applications and taking the architectures that I've seen and designed and worked with in, gaming is a good example, and moving them over to domains like medicine and education and manufacturing, and the caching is really the first step in this.

	- It's enabling the flow of eventing data rather than transaction data using a variety of the Amazon data services as shown in this architecture.

## Building for IoT with Hadoop
	- In this scenario we're going to look at Internet of Things with Hadoop. Now, if you're looking closely at the architectural diagrams, you'll see that there's very little that is different from the previous one where we talked about caching architecture. We still have our on-premise sources and our Kinesis-Enabled Application, and in the Amazon Cloud, we still have relational instances that we manage, the DB On Instance, and relational instances that are partially managed, the MySQL (DB) Instances for behavioral data, we have the caching capability with ElastiCache, we have S3 Buckets, we have DynamoDB, we have Kinesis for streaming, we have a Pipeline.

	- So what's new? We added in this scenario, an HDFS Cluster and that is our Hadoop implementation, otherwise known as EMR or Elastic Map Reduce, and we've also added Machine Learning because we now have enough data that we want to try out using predictive analytics, in addition to traditional analytics to see what kind of insights we can get since we're streaming behavioral data in through our AWS data service objects. This is a very complicated architecture, and interestingly, this is the architecture that I'm most often called to implement initially when customers talk to me about big data projects on the cloud.

	- This kind of an architecture will often take an enterprise a long period of time to implement, not because the technologies are new or unusable, but because the technologies and the partitioning of data across the various data services is an entirely new set of concepts to the team on premise who is implementing and creating the enterprises application. This is why I've introduced the architectures of data services in the cloud in the order that I've shown in this section.

	- It's really important to do this in a phased and stepped process so that you can have success. It's very interesting in the world of big data. I've been working with data projects for more than 15 years and in the old days it used to be called, "Data warehousing and OLAP," and in those days, the projects that we worked with globally had a very high failure rate because the new technology at that time, OLAP, was so unfamiliar to so many of the enterprise customers.

	- If you contrast the amount of technology that had to be learned then with the amount of technology that has to be learned now, it's exponentially greater now because you have not only the difference between an OLTP and an OLAP store, you have a menu of data service choices. That includes file services, relational services, No SQL services, data warehousing, and Hadoop. It's really complex and it's really easy to get lost in complexity and to have a failure in your implementation.

	- The real world experience that I've gained in 15 years of working with big data projects bears out in the process that I'm sharing with you here. it really does work if you start first by moving files to the cloud, then moving some relational workloads, then creating a data warehouse, then adding streaming, and then, eventually, working up to this complex scenario of IoT with Hadoop. I am also called when companies have a complete and utter failure, starting with complex technologies like Hadoop or No SQL and end up with products that either don't work consistently or don't work at all.

	- Again, one of the reasons that I decided to make this course was to help to share the process that I've developed over time in working with hundreds of different customers and guiding them to getting success in moving these complex workloads to the cloud. The bottom line is, you can't skip steps. It's a process and moving through one phase at a time, having success with each level with your minimum viable outcome and your minimum viable report and solution is critical to the success of the project.

	- I see that as companies collect more and more data, they will get to the point where they will need Hadoop and need machine learning, and it's really an exciting time, having the possibility to have all these various data services. But I cannot caution you strongly enough that the practices I'm talking about here are proven and they work, so don't skip steps. When you're ready to move to Hadoop, it's a great situation if you've got the underlying infrastructure as shown here. Now, in some cases you won't need relational databases.

	- The example where I see this is in startups where they have very little need for transactional consistency and they really just are focused on streaming data. They can sometimes get by with a No SQL solution, although at some point a startup needs to monetize, and I will often say that adding a small relational instance for the small amount of transactional data is a good architectural pattern. So very few clients that I work with need no relational databases at all.

	- In the new world of cloud-based data service choices, it becomes an add-on menu. I kind of think of it like eating at a buffet, where you take a little bit of salad, then you add maybe an appetizer, then you have a first main course, maybe a second main course if you're really hungry, and then you finish with some dessert. Now, not everybody who eats at a buffet is going to be hungry enough to eat all the courses and that really, I think, is a useful analogy when you think about the data services available on Amazon. In this scenario, the complexity added by the HDFS Cluster and machine learning needs to be business-justified, also at this point you may choose to work with the Amazon Data Pipeline or you may choose to work with a commercial product, or a combination because the data movement, when you have this large number of partitioned services is quite complex, and building on some product that is designed to manage the data movement becomes an increasingly important part of these types of solution scenarios.

## Exploring more resources
	- Well, we've covered a lot of material in this course and I want to talk about some resources for further learning. The first one is the AWS blogs in general, and the Big Data Blog in partiicular. The second is GitHub: Code samples on AWS - Labs, the next is Partners, and the next is Community: User groups and community heroes, so I'm going to show you a couple of sites. So I get a lot of usable information out of the AWS Big Data Blog at blogs.aws.amazon.com/bigdata Inside of this blog, you get information from Amazon Big Data team members, but you also get useful information from partners who create partner solutions along with partners who create commercial products.

	- So for example, just some recent entries to show you some highlights: "Connecting R with Amazon Redshift", "Running R on AWS", and of course R is a machine learning language, so there's a lot of interest in machine learning as more customers put more data onto the Amazon Cloud using predictive analytics. It's a hot topic. And this shows an architecture here with using S3, RDS and Redshift, and then using an Amazon Linux AMI and R library and RStudio and Shiny, which is the visualization tool.

	- So I just find a lot of useful information on this blog, I'll always check when I'm designing an architecture in a scenario if I have some questions, and notice there's lots of links and filters here too. So I RSS this blog. The second resource that I use really frequently is Community Heroes. Community Heroes is the community, not employees, of Amazon, but community of Amazon experts who are out there implementing complex solutions, and in particular, if I want to highlight one community hero, notice I'm one of them too which is kind of cool.

	- Adrian Cockcroft who was one of the architects of the Netflix architecture and he's now working at a venture capital firm and he's out lecturing and talking about best practices on Amazon and I've learned a lot listening to his talks and looking at resources that he's recommended. It's well-known that Netflix's implementation on Amazon is one of the most scaled and most complex and most replicatable in terms of their best practices in the world, so when I have a very high volume implementation, I will very often look to the work of the Netflix team that Adrian led for much of his career.

	- In addition to that, there are other heroes who I've learned a lot from, they're great folks. This guy here, Eric Hammond, has been really instrumental in sharing a lot of information with the community as well. So this is a new program for Amazon and I'm sure they'll be adding more heroes as they recognize and find people who have done interesting solutions and share best practices for working in the Amazon infrastructure. And it's just something that has been really fun for me. In addition to this, there are local user groups.

	- I know I am an attendee of my user groups in Southern California, and I've also attended some Amazon user groups worldwide, and there are events that Amazon puts on both regionally and in their annual conference. re:Invent is relatively new and really worthwhile. You get to talk to not only a lot of Amazon people, but also customers worldwide. So the community around the implementation of Amazon Cloud is growing and I find it to be very useful as I'm building up my increasingly complex, data-driven architectures on the Amazon Cloud.

	