# AWS Enterprise Security

## AWS Security Foundations

	### Shared Responsibility Model
		- [Instructor] You've decided to adopt AWS as your enterprise Infrastructure as a Service cloud provider. Before we dive into the AWS Console and start configuring things, it's important to understand the AWS Shared Responsibility Model. You want to be confident you understand what AWS is responsible for and where that responsibility ends. AWS provides infrastructure services on a global scale. AWS subdivides the world into Regions. A Region is a completely independent physical location, such as ap-northeast-1, located in Tokyo, Japan and us-west-2, located in Oregon in the United States.

		- Each Region contains at least two Availability Zones, or AZs. Each AZ is a completely independent data center, connected through low-latency, high-speed network links. Independent of Region or Availability Zone, AWS also has edge locations throughout the world, like places in Osaka, Japan, Milan, Italy, Sao Paulo, Brazil, and South Bend, Indiana. These edge locations power CloudFront, the AWS content delivery network.

		- The combination of Regions, Availability Zones, and edge locations represent the AWS Global Infrastructure. AWS is 100% responsible for all security controls of these layers. Riding on top of these layers are AWS's four core service offerings. These offerings consist of Compute, Database, Networking, and Storage services. For example, Compute includes Elastic Compute Cloud, or EC2, which are virtual servers, the EC2 Container Service for container workloads, Lambda for purely event-driven programming, and Elastic Beanstalk for running and managing web applications.

		- Storage services include the Simple Storage Service, or S3, for object storage, EC2 Block Storage, or EBS, for block storage, which you can attach to EC2 instances, Elastic File System Storage, or EFS, for file systems, and Glacier for archival storage. Database services include Relational Database Services, or RDS, for managed relational databases like MySQL, SQL Server, and Oracle, DynamoDB for a manage NoSQL database, ElastiCache for in-memory caching, and Redshift for data warehousing.

		- Networking services include Virtual Private Cloud, which lets you create independent isolated virtual networks, think of it as a private data center in the cloud, and Route 53 for domain name services. AWS is responsible for the security of the cloud. That means, the responsibility of securing all the Global Infrastructure and core services I just described, lies on the shoulders of AWS. This is liberating for you since physical security controls and their audits can be added to your list of things to not worry about.

		- That said, what you decide to put in the cloud is another matter entirely. Most enterprises provide services where application and platform level controls are implemented using Identity and Access Management. These systems rely on appropriate network, firewall, and operating system configurations. The data within these systems can be of varied data classification status, some of which require encryption. And, of course, all enterprises posses valuable data.

		- The security of what you place and operate within AWS is your responsibility. Let's explore an example. Suppose one of your services is a typical, three-tiered application. Looking at the AWS tools available, you decide to use Route 53 for DNS, CloudFront as your content delivery network, and S3 for storing static resources. You also decide to go with a load-balanced web tier, a load-balanced application tier, and RDS as your database tier.

		- You draw a pretty diagram and can breathe easy, knowing that the security for the infrastructure of each of these components is the responsibility of AWS. So, what's on your plate? Your responsibility lies in the configuration and patching of the operating systems and software packages you put on the EC2 servers. You're also responsible for security controls for each of the Compute, Database, and Networking components you use. For example, if you use the S3 storage offering, you are responsible for its access controls.

		- You are also responsible for configuring Identity and Access Management. This is true for administrative access for your SysOps personnel, as well as application access for your end users. Looking ahead, you can focus your attention on securing what you put in the cloud while you can rest easy knowing that Amazon Web Services is maintaining the security of the components that you use.

	### Security Landscape
		- [Narrator] Let's take a quick look at the palette of security related tools that are available within AWS. Identity and Access Management serves as one of the core tools within the security tool belt. It allows you to configure users, groups, and permissions. AWS also has a Directory Service. This allows you to provide all of the services that Microsoft Active Directory offers, without having to run the associated infrastructure. Suppose you decide to use CloudFront, Amazon's global content delivery network, in front of your web applications.

		- In order to provide an additional layer of security, you may be interested in the Web Application Firewall, or WAF. WAF allows you to protect your web applications against hackers, by defining rules and filtering malicious traffic. Amazon Certificate Manager takes the pain out of managing SSL/TLS certificates. At this point, Certificate Manager only works with CloudFront and elastic load balancers. With Certificate Manager, you can request and deploy a certificate quite easily, while being comforted by the fact that renewals are automated. Amazon Inspector is a vulnerability scanning tool that you can use to identify potential security issues with applications you operated within a AWS.

		- Using software agents installed on the EC2 instances you want to scan, Inspector has access to network, operating system, file system, and application processes. Let's look a bit deeper into how Inspector works. Consider this architecture, where you have users hitting route 53 for DNS, directed to an application load balancer with an auto-scaled web tier, which in turn is linked to a load balanced application tier, and is using RDS for storage.

		- If you want to assess the vulnerabilities of your web and application tiers, you would have to install an inspector agent on each EC2 instance. Since the agent is installed on the instance, it can see above the hypervisor, into the operated system and what's running on it. Inspector uses rule sets to scope the type of scanning it performs. You can use Inspector to scan for common vulnerabilities and exposures, or CVE's, as cataloged by the Mitre Corporation. Another rule set is based on the Center for Internet Security's Operating System Security Configuration Benchmarks.

		- For things like Windows domain controllers and member servers. A third rule set checks for Security Best Practices. For example, on a Linux system, this rule set will report a finding if any operated system user other than root, has right permissions to system directories. A fourth rule set is called Runtime Behavior Analysis. For example, on both Linux and Windows, Inspector will let you know if any insecure protocols are being used for login.

		- Inspector can collect data in as short as 15 minutes, or you can let it run for a full day, depending on how much data you want to collect. After an Inspector Run, you can view its assessment in the Web Console. AWS also provides Network Security Tools. When you create virtual private clouds, or VPC's, within AWS, you'll have the ability to apply Network Access Control Lists, or NACL's. Acting as a firewall, NACL's allow you to control inbound and outbound network traffic.

		- Security Group check controls that apply primarily to EC2 instances. Security Groups act as a virtual firewall, which you can configure to allow network traffic on ports you specify.

	### Importance of Separation of Duties
		- [Instructor] Now that we understand the AWS shared responsibility model and breath of AWS security services, let's explore how to configure your AWS account in accordance with best practices. What better place to start than by Implementing Separation of Duties and multi-factor authentication. Separation of Duties is a concept that requires more than one person to perform an action. Implemented properly, Separation of Duties reduces the chances of a security compromise. Auditors love Separation of Duties and for a good reason.

		- Implementing this best practice within your AWS account will definitely get you style points when it comes time for your annual IT audit. In this video let's explore how to protect your account by implementing Separation of Duties in concert with multi-factor authentication. When you sign up for an AWS account and log in for the first time you enter your email address and password. These two items combine to make up your root account credentials. As the name implies, these root credentials are all powerful.

		- They can be used to do anything in your account up to and including deleting it. The combination of an email address and password simply do not offer enough protection. You wouldn't want to have just an email and password standing between you and the destruction of your on privacy data center, would you? Besides, it is an excepted security best practice to implement multi-factor authentication for all privileged accounts. You probably already have MFA configured for systems you currently have access to.

		- Personal examples include your LinkedIn accout Lugo account, Apple ID, and ATM card. Let's dive in and get that root account protected. The very first thing you'll want to do is procure a physical multi-factor authentication or MFA device. Why a physical device? Because it's something you are going to put in a safe place and keep track of every time it gets used. While you are waiting for the MFA device to arrive identify two groups in your organization which will be responsible for implementing Separation of Duties.

		- Task the first team, say Engineering, with knowing, maintaining, and rotating the password for the root account. Task the second team, say Information Security, with maintaining, logging, and controlling access to the physical MFA device. After your MFA device arrives it's time to enable MFA on your account. Let's visualize what this looks like. The Engineering team has the password. The Information Security team has the MFA device.

		- Since the Engineering team can't access the MFA device without Information Security and since Information Security doesn't have the root password it is not possible for each team to gain independent access to the AWS account. Fully having these teams work together with Engineering providing the password while Information Security provides the MFA token is access to the root account possible.

	### CloudTrail
		- [Narrator] CloudTrail is a service within AWS that provides comprehensive API login capabilities. Let's explore what CloudTrail is, what it can do for you, and how to get it configured. AWS famously uses application programming interfaces, or APIs as the foundational internal communication protocol. APIs provide a consistent way to interact with AWS services from a variety of different sources. For instance, suppose you want to interact programmatically with Elastic Compute Cloud or EC2 instances.

		- EC2 instances are virtual servers running in AWS. AWS provides software development kits, or SDKs, for a variety of popular programming languages including Java, Microsoft .NET, Ruby, Python, and PHP to name a few. Under the covers, all of these SDKs use the same back-end API to interact with EC2. Even better, AWS's user facing tools including the AWS command line interface, or CLI, and the Amazon web service's web Console, both use the same back-end API to communicate with EC2.

		- This is where CloudTrail comes in. Once configured, CloudTrail records API interactions and stores those interactions off in a simple storage service, or S3 bucket, in your account. Included in what CloudTrail stores, are details about the API caller. This includes the caller's identity, source IP address, and the time the API call was placed. As you can appreciate, CloudTrail represents a very powerful ally when it comes time to audit activity in your AWS account.

		- Let's dive into the console and see how to configure CloudTrail. Here I am logged into the main AWS web console. I look in the central column under management tools and find CloudTrail. Clicking on that link brings up the CloudTrail wizard, and I continue by clicking the blue Get Started Now button. The first thing I specify is the name of the trail. I'm just going to use my initials followed by L test followed by CloudTrail.

		- I definitely want this trail to apply across all AWS regions, so I will leave that radio button set to yes. I also want to create a new S3 bucket. In this case, I'm going to specify the same name as my trail name. This is very important because S3 has a global name space, meaning they are unique across all AWS users across the world. In this case, since the name is the same, I'm just going to highlight my trail name, right click, and select copy, click into the S3 bucket name, right click, and select paste.

		- I am interested in advanced options. Clicking on the blue Advanced link shows these options. The first is to enable log file validation. Choosing log file validation allows you to ensure that you can detect whether a log file was changed in any way after CloudTrail delivered it. This is a good idea, and I am going to enable it. The second option, is whether or not I want to configure a notification when a log is delivered. This is not something I'm interested in right now, so I will just leave that radio button set to no.

		- Okay, everything looks good, so I'm going to turn on CloudTrail by clicking the blue Turn On button. That's all there is to it. Now that CloudTrail is active, the next time I go into CloudTrail in the web console, I will see a running history of API activity. I can also rest easy, knowing that these log files are being stored in a forensically verifiable manner. Since we just turned this on in a new account, there's nothing interesting to see yet. Now I'd like to show you the type of events CloudTrail records by looking at activity in another one of my AWS accounts.

		- Since I configured log file validation when setting up CloudTrail, it is possible to empirically prove that the root account credentials were used to delete the Luca IAM user on March 7th, 2016 just after 10 a.m. This can be most useful if an audit or legal need arises. Clicking the arrow next to the event shows additional details including the AWS access key and source IP address. Clicking on the view event button brings up the JSON representation of this event.

		- Programmatically friendly, this is what actually gets stored in the S3 bucket, configured for CloudTrail. As you can appreciate, CloudTrail is a critical security ally. It is so important to get CloudTrail set-up when initially configuring an AWS account.

## IAM Concepts in AWS

	### IAM Overview
	
	- [Instructor] Understanding Identity and Access Management is a crucial concept to the secure administration of your AWS account. Going forward, I'll refer to Identity and Access Management, simply as IAM. What is IAM, and what is it used for? Generally speaking, IAM has two primary functions. The first function of IAM is to authenticate users. By validating the combination of a user name and password, the authentication function of IAM is complete. For example, let's say you have an online subscription to the New York Times.

	- In order to log in to your account, you have to supply your user name and password. In the context of IAM and AWS, authentication is the combination of an IAM user, and the user's security credential. The second function of IAM is the authorization of users. For example, suppose you have a valid user name and password for the New York Times, however, due to an account issue, you are not able to access the articles, that is, you are not authorized to access content, despite the fact that you are authenticated successfully.

	- In the context of IAM and AWS, authorization specifies the services that an IAM user has access to. In AWS, the creation of IAM users allows you to give more than one person individualized, auditable access to services within your account. Suppose you have 100 people in your organization, who need access to your AWS account. Instead of sharing the root account credentials, each user would have an individual IAM user. Attempting to manage authorization for 100 people independently, is a nightmare for any system administrator.

	- Fortunately, IAM and AWS supports the concept of groups. Remember the 100 people who need access to your AWS account? Chances are, they already fall into existing organizational groups. These groups can include DevOps, Engineering, Information Security, and Finance. Once you have your users mapped into groups, you can assign permissions to individual IAM policies. Think of a policy, as a grouping of permissions you want people to have. Once you have IAM policies in place, you can assign them to groups of IAM users.

	- As is natural with group membership, users within a group will have the permissions per the policies assigned to that group. In AWS, policies allow you to manage access to resources in a very granular fashion. For instance, you might give your DevOps the ability to create, stop, and terminate EC2 instances. On the other hand, you'll likely want to prevent the Finance department from taking any action in your AWS account, that is not billing related. One thing that AWS IAM is built on, is the principle of least privilege.

	- This means that by default, access is denied. Let's step through the policy evaluation process, to see what happens when a user tries to access a resource. By default, the access request is presumed to be denied. Then the IAM policies are evaluated. If there is an explicit denial of that action in the policy, access is denied, and the evaluation workflow stops. Any explicit deny, trumps everything, period. If there isn't an explicit deny in the policy, the evaluation process looks for an explicit allow.

	- If there is an explicit allow, the action is permitted, otherwise, access is denied. Remember, if you see an explicit deny statement in a policy, the associated actions are denied. Only by having an explicit allow, and the absence of an explicit deny, is an action permitted. Integrated across many AWS services, IAM has some nifty additional features, that are an asset when controlling access. You have the ability to apply IAM policies to a role.

	- Roles can be assigned to a specific resource. For example, you can create a policy that allows read/write access to an S3 bucket, assign that policy to a role, and grant that role to an EC2 instance. Any instance that has an IAM role applied to it when launched, will have the ability to read and write to the specified S3 bucket. You can grant temporary access to resources using the Secure Token Service. This is useful when you want someone to have access to resources on a time-limited basis.

	- AWS IAM supports identity federation, so, if you already manage users external to AWS, using something like an on-premises Active Directory configuration, you can map Active Directory identities to IAM users.

	### IAM Policies
	- [Tutor] IAM policies are a critical tool for effective granular permissions management within AWS. IAM policies are used to control what happens when a given operation is executed against a resource within AWS. This control can be as granular as your organization needs. For example, suppose a company wants to give its newly hired DevOps engineers access to start, run, and stop EC2 instances. However, in order to prevent accidental loss of work, the company wants to inhibit the ability to take any other action, especially instance termination.

	- That is easily accomplished with an IAM policy. Policies can be assigned to IAM users individually, to IAM groups, or to IAM roles. This allows for the separation of the authoring of policies and their association with people or resources. To assist with managing common authorization use cases, AWS provides a large number of managed policies. A good example is the read-only access policy. Since it is maintained by AWS, as new AWS services are rolled out, you can be assured that read-only access to those services will be added to the policy without any action on your part.

	- It is also possible to create completely custom policies. This is useful as it is likely that organizations have groupings of permissions that don't fit neatly within an AWS managed policy. Policies are written in JavaScript Object Notation or JSON. Easy for humans to read and for machines to process, it is necessary to become familiar with JSON in order to understand AWS managed policies or to create policies on your own. Finally, it is important to note that authorization to a service is denied by default.

	- If you have a complicated policy with both access and deny statements on the same resource, access to that resource will be denied. Bottom line, an explicit deny always wins in IAM policies. Let's go into the AWS console to create a custom policy that satisfies the use case of allowing newly hired DevOps to run, start, and stop EC2 instances. Here I am in the main AWS web console. In order to create a policy, I need to get to the IAM dashboard. Since we've been there recently, a link for IAM is in my shortcuts and recently viewed services, near the top of the screen.

	- From the IAM dashboard, I click the policies link in the left-hand navigation to get to the policies configuration screen. Clicking the blue create policy button takes me through the three-step create policy wizard. At this point I have three options. I can use an existing AWS managed policy as the foundation for my custom policy. I can use the policy generator to help create my custom policy. Or if I'm very familiar with JSON and IAM policy syntax, I can write my own custom policy from scratch.

	- In this case, I will use the policy generator, so I click the blue select button in that section on the screen. Since the default behavior is to deny actions and I want to allow my DevOps to have full access to EC2 except for terminate, I leave the effect radial button on allow. From the AWS service dropdown, I select Amazon EC2. You can see by the sheer number of services, how complicated policies can get if you want them to. Finally, I am going to allow the run, start, and stop actions.

	- In the actions dropdown list, I scroll to the bottom and check run, start, and stop instances. Since I want this permission to exist across all EC2 instances, I put an asterisk or star in the Amazon Resource Name field. Finally, following best practices, I want to guarantee that users have MFA enabled in order to be able to take any action. In order to do that, I click on the add conditions link. In the conditions dropdown, I pick Bool for a Boolean operator.

	- In the key dropdown, I select AWS MultiFactorAuthPresent. And in the value field, I type the word true. In order to add the condition, I click the intuitively named add condition button. And as we can see, the condition reads that multi-factor authentication has to be on. Comfortable that this statement reflects what I want to do, I click the add statement button. Now that I've constructed my policy statement, I click the blue next step button to review the policy.

	- The first thing I want to do in the review policy screen is specify a policy name. I like to use a prefix, so that all my custom policies are grouped together. I go ahead and intuitively name this policy, sbn-ec2-run-start-stop. In the description field, I describe what the policy does. The description reads, this policy is intended to be used on new hires in the DevOps department. It only allows the ability to run, start, and stop EC2 instances.

	- Although it is tempting to click the blue create policy button at this point, I'm going to click validate policy to make sure there are no syntax errors. Upon seeing the green, this policy is valid message. I can proceed by clicking the blue create policy button in the lower right. After a brief delay while the policy is created, I get a confirmation screen indicating that its creation is successful. If I type my prefix into the filter box, it filters in real-time to show the policy itself.

	- That's another reason why I like to use a prefix, as it is really easy to see all of the policies I've authored. Note that the number of attached entities is zero. This tells me that no groups, users, or roles are using my newly created policy at this time. IAM policies are tools that allow a remarkable degree of specificity for managing access to AWS resources. The interface I just walked through illustrates how complex policies can get. When constructing policies for the first time, remember to take your time to deeply understand what you want to do and how to best accomplish your goals as efficiently as possible.

	- Use conditions to further restrict when policies can be used. For example, specifying that MFA must be in use. You can refactor your policies as you learn more, use new services, or as new services are released by AWS. And experiment with policies in a non-production AWS account. Finally, remember when writing policies, a deny action always wins. For instance, suppose you have two policies applied to a user or a group. The first policy allows read-write to S3 and the second policy denies access to S3.

	- The result will be that user or group will not be able to write to S3. Remember, for any situation that requires access to AWS resources, if you can imagine it, you can write a policy to handle it.

	### IAM Groups
	- [Narrator] An important part in your AWS journey is the mapping out of your IAM group structure. Let's dive right in by taking an example organization chart and mapping it into IAM groups. Taking a look at this same Org chart, we see five distinct groups. Management, DevOps, Engineering, Information security, and finance. It is tempting to simply reflect the group structure as is into IAM. Before I create the IAM groups, let's think a little more deeply about each individual box. For each group on this Org chart, let's consider a few questions.

	- First of all, do all people in a given group need the same permissions? Do groups need to be split apart to accommodate different job roles? For instance, are all DevOps personnel the same? Do they all need equal access to systems? Should new hires in the organization have the same access as experienced staff? What about non-technical personnel? Do the same people who are responsible for remitting payment also analyze overall spend by service? What about individual group leaders? Do they need read-only visibility into the cost of the resources they are using? For example, the engineering group might have 10 people in it, only two of which require the ability to alter network configurations.

	- So you may wind up with both engineering and an engineering administration group in IAM. Let's go into the browser and see how this is done. In order to create IAM groups, I need to get to the IAM dashboard. To get there, I can simply click the IAM shortcut near the top of the screen. From the IAM dashboard, I click on the groups link in the left-hand navigation pane. In the screen that comes up, I can see all of the groups that exist.

	- Of course, we don't have any yet, because we haven't created any. This is a new account. So let's go ahead and get one created. I can do that by clicking the blue create new groups button. This starts the three step, create new group, wizard. In step one of the create new group wizard, I specify my group name. In this case, I type in DevOps-newhires, then click on the next step button, in the lower right hand corner of the screen. This brings me to screen two of the wizard, where I have the option of attaching a policy.

	- I see all of the policies that exist in my account. Note that currently there is a limit of 10 individual policies that can be associated with a given group. The AWS manage policies have the familiar AWS icon next to the policy name. I can filter for the policies that I've created by dropping down the policy type menu and choosing customer manage policies. The absence of the AWS icon is an immediate cue that this is a custom policy in my account. In this case, I want to use the SBN EC2 run start stop policy.

	- So I click the check box and then click the blue, next step button in the lower right hand corner. The third and final step in the create new group wizard shows me a review screen where I can reexamine my settings prior to actually creating the group. Everything looks good here, so I go ahead and click the blue, create group button at the bottom of the screen. Finishing the wizard takes me back to the group screen within the IAM dashboard where I can see my newly created group. If I have a addition groups I need to create, I can simply iterate through the create new group wizard at this point.

	### IAM Users
	
	- [Instructor] In order for your organization to operate securely and effectively in AWS, it is important to configure each person as an independent IAM user. While it is possible to federate AWS with an external identity provider like Active Directory, I am going to focus on a configuration where master user identity data is managed within AWS IAM. Most AWS services can be configured in a number of ways. When creating IAM users I can automate the creation with a script using the Command Line Interface, or CLI, from a Linux system.

	- I can automate with a Web service that can call the IAM HTTP API. I can use a script using the Tools for Windows PowerShell from a Windows system. Or I can configure manually using the Web Console. While automation is a best practice, I am going to use the Web Console to illustrate the user creation steps. A typical engineering group is composed of many people. In this example, Luca and Catherine are Linux specialists. Melinda and Tracy, in addition to having Linux expertise, are also network administrators.

	- To set them up as IAM users, I login with my root credentials and navigate to the IAM Dashboard. From the main console screen, I click on the IAM shortcut to get to the IAM Dashboard. At the IAM Dashboard, I see that I am making good progress on my security status. If I scroll down a little bit, I see that I have no root keys, and I have already set up both IAM groups and MFA on my root account. Let's look at the IAM user sign-in link here. It's composed of this unfriendly AWS account number, followed by a link.

	- I want to make that a little bit better for my users, so I click the Customize link. In this case my account alias is just going to be sbncorp. When I'm ready, I just click the blue Yes, Create button. Ah, that's much better. A much friendlier and easier thing for my people to remember. Now I want to set up a password policy. Clicking on the warning triangle next to Apply an IAM password policy and scrolling down a little bit, displays the Manage Password Policy button. I click on that, here I have the traditional password complexity controls.

	- In this case, I'm going to choose to make the password minimum length eight characters and I also click the checkbox to require at least one number. Satisfied that meets my password complexity requirements, I scroll down and click the blue Apply password policy. Scrolling back up a little bit I get the green Successfully updated password policy message. All right, now I'm in great shape and ready to create my users. So I click on the Users link in the left-hand nav. To start the user creation process, I just click the blue Create New Users button.

	- Now I can type in the identifier I want the four engineers to use when accessing AWS, luca, catherine, milind, and tracy. These are simply their account names. Note that by default the checkbox for generating an access key for each user is checked. Access keys are necessary if the user intends on interacting with AWS using the Command Line Interface or with programming languages. Since these four individuals are all part of the engineering team and will need to use the CLI, I am going to generate access keys at this point.

	- Now that I've typed in the usernames and have generated access keys, I'm going to click the blue Create button in the lower right-hand corner. The confirmation screen that follows shows that the users have been created, and emphasizes the fact that this is the last time the access key information will be available for downloading. While it is possible to view the access keys by clicking the Show User Credentials link, once I close out of this screen, the secret access key for each individual's access key ID will no longer be available.

	- For distribution to the people themselves, it is a good idea to download these credentials by clicking on the blue Download Credentials button in the lower right corner. This downloads the access key in secret for each created user into a comma-separated values file, or CSV, and places it in my downloads folder. By opening the resultant CSV, I can verify that the access key visible at the Web console matches the values in the CSV. I'm just going to expand these columns here, and then I can see that this secret access key corresponds to the secret access key shown in the console.

	- After validating the credentials, I return to my browser and click the Close link in the lower right-hand corner, which brings me back to the Users portion of the IAM Dashboard. Now I need to assign a password for each user. In this case, I want to assign a password for Catherine. So I click the checkbox next her name, drop down the User's Action button, and click Manage Password. I'm happy to let AWS generate a complex password on my behalf so I simply click the blue Apply button in the lower right-hand corner. I can see the password generated by clicking the Show User Security Credentials link.

	- Similar to how I downloaded the access key and secret access key, I can also click the blue Download Credentials button in the lower right corner. This will download Catherine's password into another credentials CSV file. Opening up the file and expanding the columns so that they're readable, I can see Catherine's username, her password, and that friendly sign-in link that we set up earlier. It is imperative to secure any file containing credential information. You should follow internal best practices for communicating the credentials to each person.

	- Clicking the Close button takes me back to the Users dashboard. Following best security practice, I also want to enable multifactor authentication for these privileged users. I'm going to work with Catherine first, so I click the User Action dropdown, and choose Manage MFA device. In this case, Catherine chooses to use her smartphone as her MFA device. That's fine by me so I just click the blue Next Step button, click the blue Next Step button again, and this brings up the familiar QR code to scan.

	- Of course, to activate the MFA device, Catherine will have to sit next to me so she can scan this QR code and enter the authentication codes. While it is possible to create users manually using the Web Console it is only possible to assign passwords one at a time. This quickly becomes tedious. In order to make the most efficient use of your time and to drive consistency, you should automate the creation of users using one of the three available methods. Keep in mind that each person will have to be physically present with you when it comes time to configure MFA.

	- Finally, avoid generating access keys unless the user needs to interact with AWS in a programmatic fashion. While this is required for engineers, it likely isn't the case for people in the finance department.

	### Create IAM Roles
	- [Instructor] Roles are a critical IAM component as they can be used to grant consistent permission sets to both people and machines. Roles apply to a variety of use cases. Roles can be used to grant EC2 instance access to resources without worrying about maintaining access keys. This is a crucial concept to understand, instead of embedding access credentials on a EC2 instance and then rotating them to follow Security Best Practices, you can simply have an instance assume an IAM role. Roles are also useful for granting existing AWS users temporary access to resources.

	- For example, a user may need a higher level of account privilege in a production emergency. For example, you can map active directory groups to IAM roles. Roles are also useful when configuring mobile applications. Instead of embedding account keys within the app itself you can manage access to AWS resources with IAM roles. Roles are also great tools for granting account access to auditors or users from other AWS accounts. Let's visualize an example.

	- Let's say you are using a S3 bucket for storage. You want EC2 instances, an external AWS account user, a Windows Powershell user, and an IAM user at the web consol all to have the same access to that bucket. By defining a role and assigning it to each of the four groups, you can grant consistent access to your continuants. To create a role I need to get to the IAM Dashboard. So I click the IAM shortcut at the top of the screen. From the IAM Dashboard I click on the Roles link in the left hand navigation which brings me to the Roles configuration screen.

	- Clicking the blue Create Role button takes me through the five step Create Role wizard. The first thing I need to do is specify a Role Name. Since it is possible for roles to be automatically generated when setting up other AWS services, I like to use a prefix to identify the fact that I created the role. In this case, I am going to call the role sbn-s3-read only and then I will proceed by clicking the blue Next Step button in the lower right. This brings me to rather complicated looking screen where I can select the Role Type.

	- This screen defaults to setting up AWS service roles which allows access to services within your account. Selecting Role for Cross-Account Access brings me to the screen where I can let other AWS accounts or third parties access services within this account. While clicking the Role for Identity Provider Access allows me to grant access to web identity providers via either OpenID Connect, or SAML, which stands for Security Assertion Markup Language. In this case, I just want to give this role read-only access to S3, so I click on AWS Service Roles.

	- Clicking the Select button next to Amazon EC2 brings me to the Attach Policy screen. This screen is populated with both AWS managed policies and user managed policies. We can easily distinguish between the two because of the familiar AWS icon which denotes that the policy is managed by AWS. Since I want to simply grant access to read publicly available static assets, I am going to go with the AWS managed policy. To search for it I simply type S3 into the filter search box.

	- This filters in real time to show me the AmazonS3FullAccess and AmazonS3ReadOnlyAccess policies. The second one is the one I want so I click the check box and the blue Next Step button in the lower right hand corner. Looking everything over everything looks good, so I go ahead and click the blue Create Role button in the lower right. After a short delay during which the role is created, I am returned to the Roles section of the IAM Dashboard where I see my newly created role. Roles are wonderful tools for managing access as they can be assigned to IAM groups, IAM users, or to EC2 instances.

	### Temporary Access
	- [Narrator] The AWS security token service is a powerful tool for extending access to resources beyond the confines of an AWS account. The security token service is useful for granting temporary access to resources within an AWS account. This temporary access can be applied to IAM users within the account itself, it can also apply to enterprise identity or web identity users that are federated within an account. It is also useful for providing access to other AWS accounts you may own. Let's explore how to set up an IAM role that allows existing IAM users a different level of access to account resources than they typically have.

	- In general, I like to make it easy for people to do the right thing while making it hard for them to do the wrong thing. In the case of my devops admins, I want to make sure that they consciously elevate their privileges before modifying anything in AWS. Let's say we have an admin named Enzo. Enzo's IAM user id is assigned to the devops-admin group, the devops-admin group has a policy attached to it. In this case, it is the AWS managed read only policy. This means that will only be able to view everything in the AWS account.

	- Let's add a new policy to the devops-admin group which let's Enzo assume a role with elevated permissions. This will allow him to make changes in the environment. Anything from spinning up new easy two servers to making use of various storage services.

	### Grant Temporary Access
	- [Teacher] Here I am, logged into my AWS account as the enzo IAM user. I can validate this by looking in the top-right corner at the username and the affiliated account, enzo @ sbncorp. Since Enzo has the global read-only access permission, he has the ability to view anything in this account. Suppose that he has been asked to start an EC2 instance in the Oregon region. The first thing he does is to make sure Oregon is selected from the list of available regions in the top-right corner of the screen.

	- Dropping that menu down, you see all of the regions that AWS offers services in. US East, US West, in this case, we're interested in Oregon. With Oregon selected, he goes to the Compute section on the left and chooses EC2. Clicking on that link brings up the EC2 dashboard. In this case, he's been tasked with starting an instance, so let's go ahead and try. In the left-hand navigation, under Instances, we click the Instances link.

	- The Instance pane that comes up shows a single server, the s3TestBox. Under the Instance State column, we see that the current state is stopped. In order to try and start this service, Enzo goes to the Actions button and clicks it. From the menu that drops down, he chooses Instance State, then Start. At the confirmation screen, he says yes, this is in fact the instance that I want to start. So he clicks the blue Yes button.

	- Immediately, he receives an error message indicating that he is not authorized to perform this operation. I'm going to use my AWS administrative privileges to fix Enzo's problem. First, I'm going to create a role and assign elevated privileges to it. Second, I'm going to need a policy which uses the security token service to assume the newly created role. Of course this policy will have to be applied to the DevOps admin group, of which Enzo is a member. Finally, Enzo will be able to start that EC2 instance.

	- Let's get that role created. Here I am logged into the console as my administrative user. I need to navigate to the IAM dashboard, so I click the shortcut link at the top of the screen. In the left-hand navigation pane, I click on the Roles link, which takes me to the Roles page. Clicking on the blue Create New Role button, I start the Create Role wizard. The first thing I do is name my role. In this case, sbn-devops-admin-elevated.

	- That looks good, so I click the blue Next Step button in the lower-right corner. In this case, I'm interested in creating a role for cross-account access. So I click the Select button on the line that says Provide access between AWS accounts you own. This is a remarkably powerful construct, as it allows you to grant temporary access across AWS accounts. Let's say you segment your development and test environments from production by using independent AWS accounts. A cross-account role let's you have the same IAM users work across those accounts.

	- Consider it a pro-tip that you can grant cross-account access to the account that you're already in. In this case, we want IAM users from the current account to access this role. So we need to get the account number. The easiest way to do that is to drop down the menu underneath my username. From the options below, I choose My Account. This opens up my account details in a separate tab. The account ID I'm looking for is the numeric value next to the Account Id at the top of my screen under Account Settings.

	- I double-click those numbers to select them, then copy them to my clipboard. Now I can go back to my original tab where I'm creating the role and paste those numbers into the Account ID box. Since I'm granting elevated privileges, I definitely want multi-factor authentication configured. So I go ahead and click the Require MFA checkbox. Okay, this screen looks good, so I click the blue Next Step button in the lower-right corner. This brings me to Step 4, the attachment of policies. For the sake of simplicity, I choose the AdministratorAccess policy.

	- We have to be very careful with this policy, as it allows every action on any AWS service in our account. That looks good, so I click the blue Next Step button in the lower-right corner. On the Review screen, I see the policy that's attached to this role, as well as a link that I can give to Enzo so he can use it. Everything looks good, so I click the blue Create Role button. This returns me to the Roles screen. If I click on the newly created role, I can see everything about it.

	- I want to call your attention to the role ARN. A role ARN, or Amazon resource name, is a globally unique ID for roles. We want to keep this role ARN in the back of our minds as we look at the policy. Clicking on the Policies link takes me back to the Policies page. I type my prefix to show only the policies that I've created. The one I'm interested in is the sbn-assume-administrative-access policy. Clicking on that policy, I can see that it is attached to the devops-admin group in the Attached Entities tab.

	- Let's examine the policy itself by clicking on the Policy Document tab. We're looking at a chunk of JSON, which is what the Create wizards generate. In this case, we see that the Effect is Allow, the Action in this case is security token service AssumeRole, and the Resource on which we're operating is the role that we just created. Remember that ARN that we just looked at when we created the role? This is where it comes into play.

	- Of course, since this is elevating access, we add a condition which requires multi-factor authentication to be present. Let's go back to the Attached Entities tab. We can see that this policy is attached to the devops-admin group. If we click on the devops-admin group, we can see all of the users who are part of that group, including Enzo. Let's go back to the role we just created and email that link to Enzo. I do so by clicking on the Roles link and then the name of the role that we just created.

	- What I'm looking for is the Give this link to users who can switch roles in the console. I copy that link to my clipboard, and I can email it over to Enzo. So here's Enzo, still facing the error message. Clicking out of that error message, he pastes in the URL that he just received in email. This brings up the Switch Role dialog. You can see that the URL pre-populates the account name as sbncorp, and the role as sbn-devops-admin-elevated.

	- Enzo chooses to supply his own display name; in this case, enzoElevated. Then he goes ahead and signs in. The biggest difference that Enzo notices in the web console is that his username looks different. Dropping down his login information, you can see he's logged in as enzo, while he is currently active as the sbn-devops-admin-elevated role. Now he can go back to EC2 using the shortcut link.

	- Click Instances in the left-hand navigation. The s3TestBox is still there, and is still in the stopped state. From the Actions menu, he chooses Instance State > Start. At the confirmation screen, he validates that this is indeed the machine he wants to start, so he clicks the blue Start button. This time, the outcome is very different. Instead of the Permission Denied message, the machine starts as intended. With his task complete, he can go back to the safety of being read-only enzo.

	- In order to return to read-only access, Enzo clicks on the enzoElevated privs label in the top-right corner and chooses Back to enzo.

	### Incorporate Least Privilege
	- [Instructor] Remember when we created the SBN devops admin elevated role? For simplicity's sake, we attached the AWS managed administrator policy. While convenient at the time, it's not the best way to go. Let's explore why. Here we are looking at the screen that defines what that role can do. On the Permissions tab, we can see that the only policy attached is the administrator access policy. I'm going to click on the blue Show Policy link to explore what that policy actually looks like.

	- Clicking on the link displays the JSON, which defines the administrator access policy. It's power lies in it's simplicity. The fact that the action and resource segments of this policy both have an asterisk indicate that this policy has the effect of allowing any action on any resource within the AWS account. If this is the only policy assigned to a group with no explicit deny actions in place, any service within AWS can be altered. This includes everything from creating virtual private clouds, to deleting a single s3 object.

	- It even includes the ability to delete IAM users. While it's true that with great power, comes great responsibility, this level of access is much too broad to be assigned to people throughout an organization. A practical alternative is the use of this policy, in concert, with a custom policy. I went ahead and created a policy called SBN Global Deny. Which I am going to attach to the SBN Devops elevated role. I'm going to click Cancel to get rid of the administrator access policy definition.

	- Then I'm going to click the blue Attach Policy button. In the policy window, I tighten my SBN prefix. The SBN Global Deny policy shows up in the list, I select it, and click the blue Attach Policy button. This returns me to the policy screen, where I can see that I now have two policies attached to this role. Let's take a peek at the JSON for the SBN Global Deny policy, to see what it does. I can do that by clicking the blue Show Policy link. The policy contains examples of what I don't want my general user population to be able to do.

	- If you look at the first section here, it shows that I don't want just anyone to be able to delete a cloud trail. And I want to retain a log of API use in my account. Scrolling down, we see another Deny statement, with a whole bunch of things in the Action section. Basically, this statement inhibits destructive IAM-related activities. If I scroll all the way down, this last statement denies access to the cloud hardware security module, or cloudhsm.

	- Cloudhsm facilitates the management as cryptographic keys to which only you have access. While this is appropriate for some companies in these cases, it is an expensive proposition. Unless you're absolutely sure that you need to use this service, it's a good idea to prohibit this access, to avoid incurring accidental expense. Again, you want to make it difficult for people to inadvertently spend a lot of money. Let's cancel out of this policy window, and use the policy simulator to illustrate how these policies work together. I can fire up the policy simulator, by clicking the blue Simulate Policy link.

	- In a new tab, the IAM Policy Simulator is launched. Think of it as a web-based debugger for IAM policies. Clicking on the Select service drop-down, shows all of the AWS services, where IAM applies. In this case, I'm interested in simulating Identity and Access Management actions. So I find the IAM icon, and click it. To see the actions that are available to test, I click on the Selection actions drop-down. The Select actions drop-down for IAM is quite comprehensive.

	- In this case, I want to test the ability to create an IAM user, and the ability to delete one. I find the CreateUser and DeleteUser statements in the drop-down, and make sure they're selected. Notice that they appear below in the Action Settings and Results section. Now I'm ready to test, so I can click the blue Run Simulation button. I see that the DeleteUser action is denied, while the CreateUser action is allowed. This illustrates the principle of least privilege.

	- Even though the administrator access policy permits any action on every resource, the explicit denial in the SBN Global Deny policy, takes precedent. By clicking on the DeleteUser denial, I can click the blue Show Statement link. This causes the precise statement, which caused this denial, to be highlighted in the left-hand pane on the screen. Scrolling up, you can see that the exact statement which caused this denial, is highlighted. So, we know this statement inhibits the DeleteUser action.

	- If we wanted this policy to be able to delete IAM users, we can test that out right away in the policy. Scrolling down, we can find the precise statement that inhibits the DeleteUser action. We can remove that line, and then scroll all the way down, and click Apply. Scrolling back up, if we re-run our simulation, we see that the DeleteUser test now passes. In this case, that's not what I want to have happen.

	- So I'm going to go back to the left-hand pane, and undo my action. Then I'm going to scroll to the bottom, and click Apply. To confirm that my undo is successful, I'm going to scroll up and click Run Simulation one last time. Perfect. The DeleteUser action is denied. I'm confident that you can appreciate how non-trivial IAM policies can be. The policy simulator is your best friend, when troubleshooting permissions in an AWS account. Each organization is different.

	- Take plenty of time to study, in-depth, the actions you want to inhibit in your account. Once you have your list, it is a great idea to institute a Global Deny policy, and apply it to your users, groups, and roles. Keep in mind, that although I am working in the policy simulator, any edits I apply to the policy in the left-hand pane, take immediate effect. In order to minimize disruptions to your users, you need to manage changes to your IAM policy, very carefully. Approaches can vary from working off hours, to having a completely separate AWS account, for policy development.

	### Exploring Federated Access in AWS
	- [Instructor] Allowing external users to access resources in your AWS account is an important concept to internalize. If you have any on-site systems today, you may want to grant people from your existing authentication store delegated access to your AWS account. Let's explore how to set up federated access. Federated, meaning externally authenticated, users can come from a variety of sources. External entities you may want to federate with include on-premises authentication sources you operate like a local Microsoft Active Directory, other AWS accounts you own, or from web identity providers, such as Facebook, Google, or any provider that supports OpenID Connect as an authentication protocol.

	- Let's look at a typical use case where federated authentication applies. Alison is a corporate domain Administrator. Everyday she authenticates to her line of business systems using Microsoft Active Directory. As part of its cloud strategy, her company has started using resources in AWS and Alison will need to be able to manage them. Instead of provisioning a new IAM user in AWS for each person from her Active Directory, Alison federates the on-premises Active Directory mapping users to AWS IAM roles.

	- Active Directory remains the source of truth for user IDs while what the users have access to in AWS is controlled by IAM roles. I'm not running a local Active Directory service. To simulate an on-site Active Directory server, I created a simple Active Directory inside AWS. Let's look at how to federate the Administrator of that Active Directory to the AWS console. To do so, I locate Directory Service in the middle column under Security and Identity and click that link.

	- This page shows the directories that I've configured within AWS. Again, use your imagination and pretend that this Active Directory service lives on-site. Clicking into the simple Active Directory configuration screen and scrolling down, I see that no access to AWS resources is enabled. Since I want to enable access to the web console, I click on the AWS Management Console link at the bottom. It's not enabled today but we're going to change that.

	- I get a pop-up screen asking to confirm that I want to enable directory users to be able to access the AWS console. Since this is exactly what I want to have happen I click the blue Enable Access button. In order to specify the permissions that Active Directory users have on resources within AWS, I have to create a role for them. I do this by clicking the blue New Role button which starts the Add Role Wizard. At this point, I can assign directory users to an existing IAM role or I can create a new one.

	- Since this Create New Role flow establishes a trust policy automatically, I am going to click the blue Create New Role button. I am presented with a series of role templates to choose from. Since I am federating an administrative user, I select the Power User role and click the blue Next Step button. The following screen indicates that a new role is being created. Clicking on View Details, I can see the role description and if I click on the View Policy Document, I see this role will have full access to all AWS resources except for IAM.

	- This looks good so I click the blue Allow button. Reading JSON really is your friend. This brings me to the Select Users screen. In this case, it's asking for users from the Active Directory. In the Search Users box, I type in the administrator username, then hit Enter. This adds the Active Directory Administrator user to the list of users to whom I will assign the Power User role. That's all I want to do at this point so I click the blue Next Step button.

	- This brings up the final User and Group Role Assignments review screen. In this case, I verify that the role being created is Power User and that I am adding the Administrator user from my directory to that role. Let's get this done. So I go ahead and click the blue Create Role Assignments button. After a brief pause, I get the confirmation message that the role assignments have been created. Clicking the triangle next to Power User shows that it has been applied to the Administrator user from my Active Directory.

	- I can see what the role looks like in IAM by clicking the View Role in IAM link. In a new tab, this opens up the roles segment of the IAM dashboard. Clicking on the Trust Relationships tab, I see that in this case the identity provider is ds.amazonaws.com and that the condition is mapping to an external ID. It is this trust relationship that you would need to establish between AWS IAM and your on-premises Active Directory.

	- Now I'm going to show you what it looks like to log in as the administrative Active Directory user. Clicking back on the original tab, I'm going to click the name of my directory in AWS to get its log in link. Scrolling to the bottom of the directory page, I copy into my clipboard the URL associated with the AWS management console. Now I'm going to show you what it looks like to log in as the Administrator Active Directory user. In a new browser, I paste in the URL from my directory server's console access.

	- As you can see, it's asking me to supply my Active Directory credentials. In this case, the name of my directory is SBN Corp. I type in the Administrator username and the Administrator's password. When I'm ready, I click Sign In. Voila! I have signed in to the Amazon web console. In order to get more detail about who I'm logged in as, I click on my user information in the upper right corner. In the menu that drops down, I see that I have a federated log in and that I am logged in to the Power User role as Administrator.

	- To validate that the role I set up is working as designed, I click on the IAM shortcut to get to the Identity and Access Management dashboard. As expected, I am not allowed to see any IAM details due to the lack of authorization. This is just as it should be and validates that the Power User role works as intended for federated users. Now that I have federated an external user to my AWS account, let's consider the next logical steps if I wanted to federate the rest of my Active Directory users.

	- The first thing I would do is to map my existing Active Directory users and groups to IAM roles within the AWS account. Then I would finish up the configuration of the IAM roles and policies. And finally, I would have my users test it out.

	### Use Case: Securing Financial Access
	- [Narrator] So far, I've spent a lot of time talking about tools you can use to control what engineers are able to do within your AWS account. That's all well and good, but what about our friends over in the finance department? The financial view of your AWS account is an important one. Just like you don't want to give every engineer in your organization the same access to AWS resources, you probably don't want to give every finance person the same level of access. Let's explore how to configure IAM users to manage the financial aspects of your AWS account.

	- Contained within the billing and cost management dashboard, AWS offers a rich set of tools to help you understand the financial health of your account. Some of the most important features of the billing and cost management dashboard include the cost explorer, an interactive dashboard that allows you to view a detailed about of your spend by each AWS service offering. Also included, is the budgets dashboard, where you can create target budgets for your AWS service costs on a monthly basis. The payments method and history screens let's you validate historical payments and specify the details of how you pay for AWS every month.

	- The credit management interface is where you can apply AWS granted credits to your account. And the consolidated billing interface is where you can link multiple independent AWS accounts together for aggregated financial management. Consolidated billing is particularly useful if you are with a large company. For example, you may wish to have separate accounts for different divisions within your organization. Let's explore two financial management-related use cases. Let's focus on two people in your company, Alex and Luca.

	- Alex is a finance power user. She needs to be able to access every financial tool AWS makes available, from remitting payment, to cost analysis, to budgeting, Alex does it all. Luca is the dev-ops engineering manager. Luca and his team rely on a number of AWS technical tools to deliver the service that they are responsible for. In addition to using AWS technical tools on a daily basis, Luca is also accountable for his team's budget. To that end, he needs visibility into his AWS costs.

	- While Luca does need the ability to explore and visual his costs, he doesn't need the same access that Alex does. For example, Luca will never be asked to remit payment against the account. As you can imagine, this is where IAM comes to the rescue with its remarkable granularity and its ability to assign permissions to people. At this point, I've logged into the AWS console with my root account credentials. Not used for day-to-day administration, the root credentials are required to enable the ability for IAM users to access the financial management tools.

	- To do so, I click on the root account name in the upper right corner, then choose my account from the menu that appears. This causes the global account settings page to appear in a new tab. Here you can update a variety of account settings including the root account password, contact information, and supplemental contact information. If I scroll down a little bit, you can see where you can specify separate contacts for billing, operations, and security. This is particularly useful for larger enterprises.

	- If you scroll all the way to the bottom of this screen, you can see that if you decide to close your AWS account, this is where you do it. You click the checkbox and click Close Account. As you can appreciate, this is a remarkably powerful thing to be able to do, which is why you want to restrict access to and refrain from using your root account unless you absolutely need to. If I scroll back up to the middle of the screen, I find the IAM user access to billing information section. In bold, it tells me that IAM users are currently not allowed to view any billing information.

	- Since that is exactly what I want to have happen, I click the small blue Edit link on the right side of the screen. This shows me the activate IAM access section. First I click the checkbox and then precede by clicking the blue Update button. After a brief pause, that bold section is updated to reflect that fact that IAM users can now access billing information. It is very important to understand the implications of activating IAM access to billing. Remember the administrative access and global deny policies? By enabling IAM access to billing information, any user with the administrative access policy can now view financial information.

	- While this may be your intention, now is the perfect time to reflect on access rights and adjust your IAM policies appropriately.

## S3 Access Management

	### S3 Management Options
	- Simple storage service, or S3, is one of the oldest service offerings in the AWS portfolio. Storing literally trillions of files as objects, it is safe to say that S3 is also remarkably widely used. People can interact with S3 directly via the web console or a variety of available third-party tools. Machines interact with S3 programmatically, using the AWS command line interface or using one of the software development kits provided by AWS. There are SDKs for a number of popular languages, including python, NoJS, Ruby, .NET, PHP, and Java.

	- Finally, machines within an AWS virtual private cloud in your account can interact directly with S3 using a VPC endpoint. It is helpful to understand how S3 is organized. Let's cement some AWS specific jargon in your mind. First off, there's the S3 bucket. It helps to think of a bucket as a folder, a container in which a number of objects can be stored. What do you put in a bucket? In S3 terms, you can put objects into a bucket. It is helpful to think of objects as files.

	- Objects can be binary files of different types, such as pictures or word processing documents. Objects can also be simple text files. Basically, any digital document that you want to store, can be considered an object. Objects are placed into buckets. This is analogous to a folder on your desktop or laptop with a variety of different files in it. This is important to keep in your head since security controls exist at both the bucket and object level. Over the coming chapters, we will explore the following four ways to control access to S3.

	- The first is virtual private cloud endpoints. VPC endpoints are useful because they keep the network traffic between S3 objects and resources within your account private. Identity and access management policies. IAM policies are constructs within an AWS account that defines what users can and cannot do. In the context of S3, IAM policies are useful since they can be applied to IAM users, groups, and roles. The third is S3 bucket policies. As the name implies, bucket policies are security controls placed at the individual bucket level.

	- Finally, we will look at S3 access control lists or ACLs. ACLs are applied to individual objects within an S3 bucket.

	### Access S3 Privately
	- [Instructor] By design, S3 is very easy to access using common HTTP URLs. Communication between a server you operate locally and S3 is routed via the Internet. The same is true for servers within AWS. Let's look at how to access S3 using a private interface from servers within your AWS account. Suppose you store a software installation package in S3, and you want to install that software on a server locally. From your on-premises server, you could use the AWS Command Line Interface, or CLI, to issue a GET Object call to S3.

	- As parameters to this call, you specify the names of both the S3 bucket and the S3 object in your AWS account. The communication is handled via HTTP and is routed over the Internet. Similarly, the software installation package is downloaded to your server over the Internet. Now suppose you want to install that same piece of software on a server within your AWS account. You issue the same GET request as you did from your local server, and the request gets routed to S3 over the Internet.

	- This diagram represents the default configuration. However, many AWS customers were clamoring for way to communicate to S3 privately, from within their AWS account, without traversing the Internet. To meet that demand, AWS came out with a Virtual Private Cloud Endpoint for S3. A Virtual Private Cloud, or VPC, is a networking construct that lets you create a logically isolated network, private to your account. Within a VPC, it is possible to create an Endpoint for S3.

	- This VPC Endpoint is a virtual device that lets your interaction between your AWS servers and S3 communicate privately, without the need to access the Internet at all. Meanwhile, your local server still uses the Internet in order to interact with S3. VPC Endpoints for S3 let you provide secure access to S3 for running servers within AWS. This is true because traffic between the two does not go over the Internet. Access to S3, through a VPC Endpoint, can be restricted to trusted buckets by implementing access policies.

	### Manage S3 with IAM
	- [Narrator] Starting off in the Web Console, let's first look at the S3 structure we're dealing with by clicking the S3 link under Storage and Content Delivery. The screen that comes up shows all of the S3 buckets that exist in this AWS account. Since I'm logged in with full administrative access, I can take any action I want to. For instance, I can click the SBN Corporate Secrets bucket to see what's inside. With my administrative access, I can add, download, or remove any item in this bucket.

	- Now let's look at the structure of the customer buckets. I click the blue All Buckets link to get back to the blue All Buckets screen. Then I look at the SBN Customer One bucket by clicking on it. Per corporate standards, each customer has its own bucket and the bucket contains two folders, Inbound and Outbound. The Outbound folder is where Julia needs to place files. However, she should not be able to delete any files after they have been placed in the Outbound folder.

	- Meanwhile, the Inbound folder is where customers drop files. Julia should not be able to alter those files, but she should be able to retrieve them. Now let's head over to the IAM Dashboard and see what kind of permissions Julia has. Dropping down the services menu, from the top left corner of the screen, I choose IAM from my history of recently accessed services. Let's start off by looking at Julia's permissions by finding her in the users screen.

	- Clicking on Julia's name brings up her user summary screen. By clicking on the Permissions tab, I see that she is a member of the Customer Analyst's Group and that that group has two policies attached to it. SBN S3 Put Outbound and SBN S3 Console Access. Clicking the Show Policy link for SBN S3 Console Access, brings up the policy document in JSON. Examining it, we see that S3 Get and List actions are allowed.

	- Cancelling out of that screen, let's take a quick look at the SBN S3 Put Outbound policy. This policy document is slightly more complicated. Let's look at it EFFECT by EFFECT. The first EFFECT Statement allows the put object action on the outbound directory in any S3 bucket. That is, if there is an S3 bucket in the account with an outbound directory, this policy allows the placement of an object inside that bucket. The second EFFECT Statement denies the ability to delete an object from the inbound directory.

	- That is, this statement doesn't allow the ability to delete an object from any S3 bucket that has an inbound directory. If we scroll to the bottom of this policy, we see that the final deny EFFECT is associated with the SBN Corporate Secrets bucket and the Cloud Trail bucket. Julia will be unable to access these buckets at all, as it should be. As an aside, the Cloud Trail bucket is where AWS API Audit Logs are placed.

	- In her role, Julia certainly should not have access to this information. While we could log in as Julia and validate all of the above, what's more interesting is when an IAM role is applied to an EC2 instance. Let's see what this looks like. I'm going to pop over to the EC2 dashboard to verify the IAM role that is applied to a server. Scrolling down, I cancel out of this policy, and go to the services menu and select EC2 from my history.

	- This brings up the EC2 dashboard. This is the screen from where you can create server instances in AWS. For this test I have one instance running. I can validate its setting by clicking on the instances link in the left hand navigation. Selecting the instance that is running, the S3 Test Box Two brings up a lot of information about the instance itself, on the lower half of the screen. I can just slide that screen up to see all of the details.

	- What I'm most interested in is the IAM role associated with this instance. We see that it is the SBN Customer Analyst's role. Clicking on that link, takes me to the role definition in IAM. You'll notice that the SBN S3 Put Outbound and SBN S3 Console Access roles are associated with this role. That means that this server has the same permissions that Julia has.

	### Solving S3 Access with IAM

	- [Narrator] Now let's take that one step further. We have a couple of policies that are applied both to an IAM group and to an IAM role. We also have an EC2 instance launched with that IAM role. Let's fire-up a terminal session on that EC2 instance and try to access S3. Here I am at the EC2 dashboard with the s3TestBox2 instance selected. I want to establish a connection to it, so I click the Connect button. The screen displays instructions on how to connect to the instance using SSH.

	- PuTTY is a common SSH client for Microsoft Windows. Since I'm on a Mac, I can access SSH directly, so I'm going to highlight the example SSH connect text and save it into my clipboard. Now let's slide over to a terminal on my Mac. First, let me verify that I have the sbn.west.pem private key file in my directory. I can do that by typing the ls command. Great, there it is. Now I will paste the SSH connect text into the Mac terminal to establish the connection.

	- Okay, here I am with a terminal connection on that EC2 instance. It is a machine running Amazon Linux, which already has the Amazon Command Line Interface, or CLI, installed. First of all, let's validate that I have no access to the sbn corporate secrets bucket with the CLI command. As expected, I get an access denied error message because the role that corresponds to this instance doesn't have access to the corporate secrets bucket. Now let's see if I can see what is in the customer one bucket.

	- Yes indeed, I can see the file called cust1.infile1.txt. I want to pull it back to the machine that I'm on right now. Before I do that, let's verify that the file doesn't exist in my current directory. I can do that by typing the ls command. The absence of output from this ls command shows me that I have no files on the EC2 instance. Now I can execute a copy command using the CLI. The format of the command is AWS S3 copy from S3 from the SBN customer one bucket, this object, the cust1.infile1.txt file.

	- The dot denotes that I want to copy that file to my current directory. The download feedback indicates that the file was copied from S3 to my local directory. I can verify that by re-executing the ls command. Now when I list the current directory, I see the file which I retrieved from S3. Now let's try and delete that file from S3. As we can see, the delete failed because I don't have the appropriate IAM privilege.

	- The permissions on the inbound directory are performing as expected. How about the outbound side? Let's first create an empty file with the touch command. Re-executing the ls command shows the file that I just generated. Now let's copy that newly created file to the outbound directory for customer one. Great, it looks like the upload command succeeded. We can verify that by listing the file in S3.

	- Finally, let's validate that this instance cannot remove the file that it just placed in the customer's directory. Perfect, once again we get the access denied message. Let's pause and consider the significance of IAM policies on S3. First of all, using an IAM policy with S3 prevents the need for embedded credentials on EC2 instances. Not having embedded credentials saves you from the operational burden of maintaining and rotating access keys.

	- Abstracting beyond S3, we know that IAM roles can include policies for any AWS service. With that kind of potential, the need to embed credentials on an EC2 instance should be very rare indeed. Secondly, policies can simultaneously be applied to both instances and people. That centralizes permissions management. If you need to change the way a group of people and a group of machines accesses a given service, you can do that by modifying the appropriate policy.

	- Again, if you think beyond S3, you realize the power this concept represents. Third, when writing IAM policies for S3, you can handle any complicated permission situation which may arise. You can write policies to manage multiple S3 buckets with diverse permission requirements. It's worth reiterating that IAM roles can encompass multiple AWS service offerings, from compute and database to storage and networking, you can design an IAM role and policy combination to fit your organizational needs.

	### S3 Bucket Policies
	- [Tutor] S3 bucket policies are security controls applied at the bucket level. Let's explore some defining features and ways in which you can use them. S3 bucket policies are useful in that they specify security controls at the individual bucket level. In addition, they are useful if you use many S3 buckets, each of which has its own security requirements. Most notably, bucket policies are an ideal mechanism to grant access to a specific bucket across AWS accounts.

	- Perhaps the defining feature of this control mechanism is that bucket policies allow you to delegate access without sacrificing control. The permissions specified in the bucket policy take precedence. Consider the following use case. A company chooses to separate its development and production systems by implementing separate AWS accounts. For its production systems, the company stores configuration files in an S3 bucket in its production AWS account. Olivia is an engineer with an IAM user in the production AWS account.

	- According to her role, she needs to be able to access and modify configuration files in the S3 bucket. Meanwhile, Madeline, using her IAM development account, is working on building out a new environment. To help her get started, Madeline wants to be able to reference production configuration files but should not be able to update them. Let's look at how things are set-up in the AWS accounts. Here I am in the web console for the production account.

	- Since we've been there recently, I have an S3 shortcut under my recently viewed services at the top of the screen. Clicking on it takes me to the S3 landing page. The bucket I'm interested in is the SBN S3 bucket policy example bucket. I'm going to right-click on that bucket name and click properties. This brings up the properties of the bucket in question. Let's look at the permissions associated with this bucket. I can do so by clicking the permissions link.

	- By default, the root account has full permissions on the bucket. What I want to look at is the bucket policy. I can do so by clicking the edit bucket policy link. Examining the policy, we see that the Olivia IAM user is coming from the production account, and that her permissions are set to allow full S3 access on the bucket in question and its contents. Scrolling down a bit, we see that Madeline's IAM user is coming from a different AWS account; the development account.

	- Per this bucket policy, she is limited to listing the bucket and its contents, and to retrieving objects from within the bucket. In a different browser, let's look at the development AWS account to see how Madeline's IAM user is setup. Here I am in the development account looking at Madeline's user summary information within the IAM dashboard. On the groups tab, I see that she is part of the production access group. Clicking that link brings me to the summary page for that group.

	- On the permissions tab, I see two policies attached to this group. The first is Amazon S3 Full Access. That will allow full access to S3 resources within the development account. Let's take a closer look at the second policy to see what it does. I can do so by clicking the show policy link. If I look at the policy, I see that it grants broader permissions than what is stated in the bucket policy; it grants full S3 access to the bucket and to objects within that bucket.

	- It is critical to understand that the bucket policy trumps all other access. The development account specifies broader access to the S3 bucket then is specified in the bucket policy. At the end of the day, the bucket policy permissions take precedence. Instead of having full S3 access, Madeline will only be able to list and get. Coming up let's take a peak at what that looks like from the command line.

	### S3 Bucket Access with CLI
	- [Narrator] Here I am with a terminal window on my local Mac, I've already installed the AWS CLI. If you need to download it, it's pretty easy to do. Just pop open a new web browser and Google search for download AWS CLI, clicking on the first link will take you to Amazon documentation with instructions on how to download and install the CLI for your specific platform. I've also set up a configuration file for the credentials I want to use with the AWS CLI.

	- I'll show it to you now, I'm going to use the VI Editor to open the credentials file and look at it. The profile I'm interested in is the Madeline.dev profile. As you can see it specifies her access key id and the associated secret access key. This combination will allow me to use the CLI as the Madeline IAM user from the development AWS account. Putting out of the editor, let's list the contents of the s3 bucket as Madeline.

	- It's kind of a long command so let's deconstruct it briefly. Using the AWS CLI, you want to use the s3 component to list the contents of this bucket. Specifying Madeline.dev to the profile option means that I'm using Madeline's IAM credentials. Great now we see what's in the bucket. In this case, Madeline wants to recreate the copy of the nginx configuration file to use as the basis for her development work.

	- Specifying the nginx.conf file in the copy command, I go ahead and hit return. The successful download message means that the command worked. Issuing the LS command I can see that the file is now in my directory. Madeline looks at the file, edits it for her purposes and wants to copy it to her development s3 account. However, being human, Madeline makes a mistake and ends up trying to copy it back to the production account.

	- Despite the fact that she was trying to copy her local file to the bucket in question, she was not permitted to do so and got the access denied message. Even though her IAM permissions would allow this to happen, the bucket policy is in place causing the upload to fail. Thank goodness for bucket policies, they allow you to delegate access without relinquishing control. In addition to restricting access by IAM user, that's only the start of what you can do with bucket policies.

	- They can be used to restrict access based on IP address, grant read access to anonymous users. And ensure that multi-factor authentication is required to interact with the bucket. If you have a diverse set of complicated access rules for your s3 buckets, you can get it done with bucket policies.

	### S3 Access Control Lists
	- [Voiceover] S3 access control lists, or ACLs, are another tool that exists for controlling access to objects stored in S3. Let's understand what they are and see how they work. The first thing to understand about ACLs is that they apply to every object you put into S3. With literally trillions of objects stored in S3, that's potentially a lot of ACLs. Imagine the chaos if each object had unique ACLs. Maintenance would be an administrative nightmare.

	- The next thing to understand is that with S3 being one of the oldest services in AWS, S3 ACLs came into being long before IAM existed as a service. Let's explore S3 ACLs to give you an appreciation why you need to be careful if you decide to use them. Here we are logged into the web console looking at the S3 landing page. Let's take a peek at the SBN S3 ACL example bucket. Right clicking on the bucket and choosing properties shows me the properties affiliated with this bucket.

	- If I click on permissions, I can see that it says add bucket policy. This means there is no bucket policy in place. I can validate that by clicking the add policy button, see the bucket policy is empty. I'm going to close that out. Drilling into the bucket itself, I see a private directory. Drilling into the private directory, I see a number of images. I click the yellow tulips JPG object, then click the properties button in the upper right corner.

	- This shows me all of the properties for this object. If I click on the permissions section, I can see that only the account owner has any permissions on this object at all. However, the thing that's unique about S3 is that S3 objects can be served via the web. See this link? That is a direct link to the object itself. Let's see what happens if we click on it. A new tab opens and we get an access denied message. This makes sense since we are going in as an unauthenticated user.

	- Now let's go back into the S3 console and see what happens if someone updated the ACLs on just that object. Let's say they added more permissions and when they chose to grant permissions, they granted it to everyone. Let's say they granted open download and view permissions to everyone. Clicking the blue save button puts that change into effect. Now if we go back over to the browser where we previously got the error message and refresh the page, we get a dramatically different result.

	- When AWS says everyone, it means everyone in the world. Could a bucket policy help us stop this? It is possible to override any ACL settings with a bucket policy. Let's see what that would look like. I'm going to go back to the S3 management console all the way back to my bucket list and then right click on the bucket in question and click properties. I want to add a bucket policy to address this issue.

	- I stored a bucket policy I call the deny hammer on my local disc. I'm going to open that up with text edit. Before we implement this policy, let's take a peek at what it does. The most interesting thing about it is that it uses the not principle element. If you read this policy, the not principle element means that unless you're the root account user, the Olivia IAM user, or the snijm.admin IAM user, you will not have any access at all to that bucket or its contents.

	- Here's the bucket and the contents, so I'm just going to select all that, copy it in, go back to my browser, and paste it in the bucket policy editor. I'm going to go ahead and save that bucket policy. Now let's go back one more time to the other tab and hit refresh. Once again, we get the access denied message. When you reflect on your strategy for securing S3, remember that simplicity is your friend.

	- You have the principle of least privilege to fall back on, so by default, access is denied, and an explicit deny always wins. This can get tricky to keep track of when security controls are spread across IAM policies, bucket policies, and S3 ACLs. You should use IAM policies and S3 bucket policies first. Only if you can't solve your access needs should you use access control lists. Unless you have a compelling need for diverse permission settings for individual objects within an S3 bucket, you're better off staying away from access control lists.

	- Start organizing and managing your S3 content with IAM policies and bucket policies. Use ACLs as a security control of last resort. Consider for a moment how complicated permissions can get within a single AWS service. Abstract that across the variety of offered services. Manual access checks are just not practical. You need to get into the habit of automating your security checks.

	### Pre-signed URLS
	- [Instructor] Pre-signed URLs are a unique way to grant temporary access to S3 objects to people who don't have an AWS account. Since pre-signed URLs expire, using them is a great way to enable time-limited access to objects in S3. Let's explore how pre-signed URLs work. Let's say a company stores marketing collateral in S3. Kevin is a developer with an IIM user account. As part of an upcoming marketing campaign, he is tasked with making some time-sensitive collateral images accessible to the world.

	- Instead of using access-control lists on the individual S3 objects, he decides to generate a pre-signed URL. Part of the marketing collateral contains coupons that expire. Because of this requirement, Kevin chooses to use pre-signed URLs, as they become invalid after a specified period of time. Using his security credentials, Kevin generates a pre-signed URL set to expire after the marketing campaign is over. Kevin can embed that pre-signed URL in web sites or marketing emails.

	- The marketing campaign goes out, and people go to the website or open the marketing emails; when they do, the marketing collateral is retrieved from S3, using Kevin's permissions. That is because Kevin is the person who generated the pre-signed URL in the first place. AWS supports the generation of pre-signed URLs in a variety of development languages. Conveniently, AWS also supplies a plugin for Microsoft Visual Studio that makes it possible to generate a pre-signed URL without having to explicitly write software.

	- Let's go into Visual Studio to see how this is done. The simplest way to demonstrate pre-signed S3 URLs is with the Toolkit for Visual Studio. Absent this toolkit, you can programmatically create a pre-signed URL using an SDK in your favorite programming language. For demonstration purposes, I've created a Windows machine in AWS on which I've installed Visual Studio and the AWS Toolkit. I go ahead and launch Visual Studio where I've already installed the AWS Toolkit for Visual Studio.

	- Here I get a warning message, saying that a new release is available and can be found on the Amazon website. I can go ahead and click the link if I'm interested, which takes me to the website, with instructions on how to download and install the new package. Conveniently, there is a download link right at the top of the webpage where I can download the toolkit if I need to. I'm going to skip installing the upgrade for now, and get on with generating the pre-signed URL.

	- I've logged into the AWS Toolkit using Kevin's security credentials. Using the AWS Explorer, I come down to Amazon S3, and click the arrow next to it to expand its contents. The collateral I'm interested in is within the sbn-s3-presigned-example bucket. Double-clicking on the bucket name allows me to see what's inside. The object I want to generate the pre-signed URL for is the RedPoppies.jpg.

	- Right-clicking on that object name, I can scroll to the bottom of the popup menu, and choose Create Pre-signed URL. It's a relatively straightforward interface. On the left-hand side is a calendar where I can choose the expiry date and time. I am going to choose just before midnight on March 23rd. Since this is collateral I want people to be able to retrieve, I'm going to choose the GET action. OK, so I'm ready to click the Generate button, and create that URL.

	- Clicking on that URL opens a new tab in my browser, and retrieves the object from S3. If we look at the URL in the browser, we see the name of the S3 bucket and the key for the S3 object. Note that as parameters to this URL there is an AWS access key ID, expiry time, and signature. If I delete those parameters, and simply try to hit the bucket directly, I get an access-denied message.

	- This access-denied message is exactly what someone will get after the 23rd.

## Security Audits in AWS

	### Prepare
	- Maintaining the security of your AWS account is crucial to effective, sustainable operations in AWS. So is maintaining security within your account. Let's explore tasks you will want to keep in mind as you audit your use of AWS. The first thing you will want to do is review the status of your root account credentials. Verify that you have organizationally separated knowledge of the root account password from the Multi-Factor Authentication device used to access the AWS console.

	- In addition, verify that the root account access keys for use with AWS APIs have been disabled. With IAM being widely available across AWS service offerings the need for programmatic root account activities should be very rare. Root account access to the web console can be devastating in the wrong hands. In 2014, a company was forced to shut down its operations due to compromised access to its AWS console. You'll want to have a comprehensive understanding of the IAM policies being used in our account.

	- Confirm that IAM policies conform to the principle of least privilege. First, understand the access use case that each policy is intended to fullfil. Then, ensure only the permissions required to meet the use case are contained in the policy. This is also a good time to review changes to any AWS-managed policies you are using. If there is a difference in policy, see what the differences in policy versions are. As your use of AWS increases in complexity, it is easy to lose track of what IAM policies can do.

	- To help, use the policy simulator to validate your access control use cases. Finally, to avoid accidental unauthorized access, pay close attention to any policy that includes the ability to create or modify policies, roles, groups, or users in IAM. The need to modify IAM itself should be restricted to your identity and access management team. Another item you will want to review is IAM groups. First on the list is to look for groups which are not being used.

	- As a follow-on activity, examine the membership in each group. If there are any group members who should no longer be in the group, remove them. While you are reviewing groups, verify that only the appropriate IAM policies are attached. It is also important to look at IAM users. In a well-thought-out group structure, IAM policies do not need to be attached directly to individual users. Check for users that have policies assigned to them individually. If any exist, validate that direct attachment is required.

	- If it isn't, detach the policy from the user, and assign it to the appropriate group. Next, check for users who have never used access keys. If any exist, remove the access keys unless they are needed. Finally, ensure that you are rotating individual credentials on a regular basis. Similar to the review of IAM users, you want to check on IAM roles. You'll want to remove any roles which are not necessary and are not being used. You will also want to consider the machines that have roles.

	- Validate that the people who can access these machines should have the permissions granted by the role. If the machines have a greater level of access than the people using them should have, you need to revoke machine access or relaunch after you've adjusted the role. A number of other AWS services have security controls in them. For instance, with S3 you will want to review policies applied to individual S3 buckets. You will also want to review access control lists applied to individual objects.

	- Of course, S3 is just an example. Other AWS services have security controls as well. Things to consider include who is receiving notifications generated by events in your AWS account, who has access to deployment tools, and the rules governing network access to EC2 instances. If you are using an external authentication store like Active Directory, you need to do all the user verifications upstream of AWS. CloudTrail provides a wonderful audit trail of activity within your AWS account.

	- Consider using a third party tool to scan for irregular access patterns in your CloudTrail logs.

### Access Key Rotation
	- [Instructor] Rotating user access keys is one of those periodic maintenance tasks that you need to be on top of. Similar to rotating passwords, key rotation is a best practice. It is almost certainly something to be examined during a security audit. The following process is a safe method for rotating access keys. Starting with an understanding of where access keys are being used, the first step is to create a new access key. This can be done via the web console, command line interface or API.

	- In the configuration file of the application, using the access key or wherever the current access keys are being used, you need to replace them with the newly generated secret and key. Then perform a regression test using the newly created access key. Once you have confirmed functional continuity, you can proceed to inactivate the old key. You may want to keep the old key around for awhile just in case you discover a mission critical use case which was not accounted for. Once you are comfortable that the old access key is no longer needed, you may delete that old access key.

	- Let's explore what this looks like from the console. Here I am at the IAM Dashboard. The task that I want to accomplish is the rotating of Madeline's access keys. To do so I click the User's link in the left hand nav pane. Clicking on Madeline's user name brings me to her summary screen. On the Security Credentials tab, I see that she has one existing access key. To create a new one, I click the blue Create Access Key button.

	- The dialog box confirms that the new key has been created. Clicking on the Show User Security Credentials link displays the keys on my browser. However, I need to transmit them to Madeline so she can rotate the keys on her end. To do so I'm going to download those credentials. They download to the credentials.csv on my local machine. Opening that file I can see that the format specifies Madeline's User Name, the Access Key Id, and the Secret Access Key associated with the Access Key Id.

	- I need to securely transmit this information to Madeline. Remember that you have to use the same judiciousness in handling these credentials as you would passwords. From a terminal let's open up the AWS credentials file and see how we can add her new access keys to a new profile. I'm going to go down and make a copy of the existing madeline.dev profile. The copy I'm going to rename from madeline.dev to madeline.dev.new.

	- Now I need to replace the access key id. So as Madeline I've securely received these credentials, I have the new access key id, and I go back into my terminal and paste that information in. Of course I have to do the same thing with the Secret Access Key. I'm going to write this file to disc and then quit out of the editor. Now let's test it out.

	- The first thing I'm going to do is list the contents of an s3 bucket using Madeline's existing profile. Great, now I need to reissue that same command with her new profile. Wonderful! The new profile is working as expected. That means that her newly created credentials work and that she's successfully changed out the configuration information locally to use them. Now let's go back into the console and disable her old credentials.

	- Going to X out of this dialog box, and look at her old secret access key. And instead of deleting it straight away, I'm just going to click the Make Inactive link. In the confirmation dialog box, I'm going to click the red Deactivate button. After a brief pause, I see that the status for her old access key has been switched to Inactive. Let's verify that from the command line. Executing the list command against s3, using her old profile generates the error InvalidAccessKeyId.

	- It is for this reason that AWS allows two access keys per person. It allows you to effectively switch them out without jeopardizing your operations. As the administrator Madeline has let me know that she has completed her testing and that she no longer needs her old access key. Let's close out the operational process by removing it. In the web console I go ahead and click delete next to her Inactive Access Key Id. At the confirmation box I simply click the red Delete button.

### Trusted Advisor
	- [Voiceover] AWS has built a tool called Trusted Advisor into the web console to help you operate your AWS account as efficiently as possible. Let's explore what Trusted Advisor can do for you and look deeply into its security recommendations. Trusted Advisor is available for free in every AWS account. With business-level paid support, Trusted Advisor offers comprehensive best practices checks against your account. Without a paid support plan, the checks are less complete. Currently, Trusted Advisor offers insights into four major categories.

	- Cost optimization, performance, fault tolerance, and security. Let's explore the cost optimization portion of Trusted Advisor. At the free tier, no cost optimization checks are included. With a paid plan, the cost optimization section looks for AWS compute and network resources in your account that aren't being heavily used. For example, suppose there is an EC2 instance in your account with consistently low CPU utilization. Trusted Advisor would flag that instance for your review as you might save money by replacing the underutilized instance with one that is less powerful and cheaper.

	- The second category is fault tolerance. As with cost optimization, no fault tolerance checks are included at the free tier. With a paid plan, the fault tolerance section looks at how AWS resources in your account are configured for resiliency. For example, one thing that fault tolerance checks is how the load balancers are configured in your account. If a load balancer exists but has no EC2 instances associated with it, the fault tolerance section of Trusted Advisor would flag that for your review.

	- The third trusted advisor category is performance. It evaluates your AWS account for things that could impact performance. At the free tier, it checks to see if you're approaching any AWS service limits in your account. For example, suppose a professor teaching a network and security class wants to provide logical network isolation for each of her 18 students. To do so, she decides to provision each student a virtual private cloud. However, the default service limit for the number of VPCs available in an AWS account is five.

	- Trusted Advisor will alert the professor once she has consumed 80% of the service limit. In this case, when she configures her fourth VPC, she will be using 80% of the available five. As with all service limit related issues, this can be resolved by opening a support case and requesting a service limit increase. Predictably, the performance checks are more comprehensive with a paid plan. A greater number of services are examined. Typically, the recommendations can be addressed by requesting a service limit increase via a support call.

	- Some recommendations, such as high CPU on EC2 instances, can cause you to consider alternate configurations. For example, suppose Trusted Advisor reported consistently high CPU on an EC2 instance. If load on the machine were to spike, the CPU could become saturated and customers would be disserviced. To address this possibility, expending the effort to configure an auto-scaling group to dynamically react to high CPU load might be worth the effort.

	- Security is the final category available within Trusted Advisor. AWS takes security very seriously. As such, the free tier includes more checks than any other Trusted Advisor category. The free tier checks that the root credentials for your account have been protected with multi-factor authentication. Trusted Advisor also checks your account to ensure IAM users have been configured. Using an IAM user as opposed to the root account credentials for daily tasks is a best practice, which is why this check exists.

	- The final check in the free tier has to do with network security. Trusted Advisor will give you a report on any network security groups in your account that allow unrestricted access to common administrative ports. For example, ports 1433 and 1434 are well-known as the ports for administrative access to the Microsoft sequel server database. Trusted Advisor will let you know if your network is configured to allow unrestricted access to these ports. As you would expect, the paid plan offers more comprehensive checks across more AWS services.

	- Checks within the paid tier include permissions on S3 buckets, CloudTrail, and more. Let's take a quick peek at the security portion of Trusted Advisor. Here I am logged into the web console of my AWS account. I can find Trusted Advisor in the center column under management tools. Clicking on the Trusted Advisor link causes the Trusted Advisor dashboard to appear. Let's take a peek at the recommendations. Since I'm not paying for support in this account, there are no cost optimization checks or checks for fault tolerance.

	- However, the red exclamation point under the security section indicates that there's something I need to take a closer look at. In the recommended actions section, I'm going to click on the first recommended action, security groups with unrestricted ports. Scrolling down, I can read a description of how this check works and what the alert criteria are. Scrolling down even further shows me the offending security groups. The first two recommendations concern the security group named Oracle 1521.

	- As you can see, it allows port 1521 and port 22 to be accessed from any IP address. Looking at the SSH plus security group, we see that it allows ports 1433, 22, and ICMP traffic from any IP address on the planet. Scrolling back up, it's in our best interest to go to the recommended action section and reconfigure the security groups to tighten them up. Unless there's a compelling business need to let these ports be accessible from anywhere in the world, it's a good idea to change the configuration of these security groups to lock them down since you wouldn't want hackers to be able to access these administrative ports.


Hammer Endurolyte
Fizz Tablets
Magnesium/Potassium
CoQ10
Nuun Electrolyte
Pickle Juice
K-Chlor
